{"pages":[{"title":"更换icarus主题记录","text":"[TOC] NexT: https://github.com/iissnan/hexo-theme-next icarus: https://github.com/ppoffice/hexo-theme-icarus 两者效果图如下: NexT icarus 相较NexT, icarus使用的人数没有很多, 想要改什么在网上搜到的基本都是关于NexT的 虽然icarus也提供了很多配置, 但还是有些地方想按照自己意思做些修改, 强迫症~ 有些无法通过配置完成的, 只能改源码了 作为后端开发前端知识匮乏, 还好icarus使用的是ejs, 一种模版语言, 类似以前用过的FreeMarker和Thymeleaf 变动日志主题配置文件_config.yml的更改不记录了, 可参考文档: Documentation 主要记录源码部分的变动如下: 修改navbar导航栏左边的logo配置方式因为不会设计Logo, 就改成”icon+文字”的方式, 并加入logo.img配置项 themes/hexo-theme-icarus/layout/common/navbar.ejs 修改navbar导航栏右边的搜索功能原版2.3.0只有一个小的搜索icon, 加入搜索输入框并嵌入搜索icon themes/hexo-theme-icarus/layout/common/navbar.ejs themes/hexo-theme-icarus/source/css/style.styl 修改个人信息页中的几个links原版是通过socialLinks动态配置的, 不支持微信, 码云, 微博这几个常用, 这里为了方便我使用&lt;a&gt;+&lt;img&gt;标签写死 themes/hexo-theme-icarus/layout/widget/profile.ejs 友情链接标题前加入icon, 为了好看 themes/hexo-theme-icarus/layout/widget/links.ejs 修改文章页(index页和post页)的文章时间加入判断, 如果是列表页显示例如几月前, 文章页显示具体日期, 例如2018-12-22 themes/hexo-theme-icarus/layout/common/article.ejs 修改文章详情页面不显示文章图片thumbnail在阅读文章时感觉有点花, 默认是index页和post页都会显示 themes/hexo-theme-icarus/layout/common/article.ejs 修改首页文章列表摘要信息不显示样式去掉Markdown生成的html标签, 类似简书上的文章排版, 整洁一点 themes/hexo-theme-icarus/layout/common/article.ejs 修改文章页面布局原版的主页和文章页都使用三栏布局, 在文章页阅读会显得内容很窄, 尤其是代码部分, 需要左右滚动, 故修改文章页为两栏布局 themes/hexo-theme-icarus/includes/helpers/layout.js themes/hexo-theme-icarus/layout/common/widget.ejs themes/hexo-theme-icarus/layout/layout.ejs themes/hexo-theme-icarus/source/css/style.styl 目录的开启方式改为默认就开启文章目录这样可以不用每个md文件都去写toc: true themes/hexo-theme-icarus/includes/helpers/config.js 修改开启目录后的显示问题默认目录在滚动文章时如果太长会显示不全, 所以增加目录粘性布局 themes/hexo-theme-icarus/layout/widget/toc.ejs 文章页增加版权声明 themes/hexo-theme-icarus/layout/common/article.ejs themes/hexo-theme-icarus/source/css/style.styl 修改底部footer的显示信息 themes/hexo-theme-icarus/layout/common/footer.ejs","link":"/README.html"},{"title":"","text":"基本资料 -姓名: ?-出生年月：1994年06月-籍贯：嘉峪关-学历：本科-求职意向: Java开发 联系方式 -邮箱：coderthank@163.com-博客：https://xiefayang.com-github：https://github.com/thank037-微博：i蝸居年華_谢谢谢 自我评价 -热爱开源，热爱分享-对学习能力充分自信-认同《代码整洁之道》与《重构》, 崇尚整洁的编码习惯-随和, 不钻牛角尖 专业技能 TODO 项目经验 TODO 教育背景及工作 2012年9月—2016年6月 甘肃农业大学 信息管理与信息系统 本科2015年12月—2016年4月 浪潮集团通信信息技术 Java研发(实习岗)2016年8月—至今 北京中盈安信 Java开发 荣誉奖项及开源贡献 2014年09月 甘肃农业大学计算机技能大赛(设计类)一等奖 校级 2014年09月 甘肃农业大学计算机技能大赛(应用类)二等奖 校级 2014年11月 甘肃农业大学计算机编程校内选拔赛Java组第一名 校级 2014年12月 第六届蓝桥杯全国软件大赛甘肃赛区Java组一等奖 国家 2015年03月 全国计算机等级四级网络工程师 教育部 2015年05月 第六届蓝桥杯全国软件大赛全国总决赛Java组优秀奖 国家 2018年09月 成为EdgeX Foundry社区子项目edgex-ui-go贡献者 2018年12月 建立国内首个EdgeX Foundry物联网框架交流社区","link":"/about/index.html"}],"posts":[{"title":"Activiti初探","text":"前言 Activiti是一个基于Java开源的工作流引擎, 实现了BPMN2.0规范. 提供了任流程的定义, 部署和流程调度. 这里很重要一个概念是工作流, 听起来很虚. 如果接触过git workflow, 会了解到它应用的本质是使用Git来进行有效的代码流程管理和搞笑的开发协同约定. 关于Activiti的由来和简介, 不在赘述. 关于理论, 可以参考: 工作流(workflow): http://blog.csdn.net/u014682573/article/details/29922093 BPMN2.0规范: http://www.mossle.com/docs/jbpm4devguide/html/bpmn2.html 准备下载Activiti 官网http://activiti.org/download.html 下载解压后的目录结构: database: 提供一些数据库脚本. libs: Activiti所需要的jar包和源码包. wars: 官方提供的一些demo, 可以部署到tomcat下看看. 安装Activiti Eclipse设计器 在Eclipse -&gt; Help -&gt; Install New Software -&gt; Add Name: Activiti BPMN 2.0 designer Location: http://activiti.org/designer/update/ 当然也支持离线安装方式. 此外还需要jdk6以上的环境, activiti支持多种主流数据库环境. 了解ProcessEngineProcessEngine: 流程引擎对象. 所有的操作都离不开它. 要了解ProcessEngine, 最好的方式是在你的项目先建立activiti project, 引入相关jar包. 然后在其中编写junit, 来了解它的工作方式. 初始化数据库整个流程引擎需要数据库表的支持, 不同版本可能数量不同, 我的是activiti-6.0.0版本, 需要28张表. 初始化数据库的方式也有多种. 执行脚本方式: 可在其中包括了建立数据库表和约束12345678910111213141516172. 代码方式: 使用ProcessEngine来创建, 代码如下: ``` java @Test public void createTable1(){ ProcessEngineConfiguration processEngineConfiguration = ProcessEngineConfiguration.createStandaloneProcessEngineConfiguration(); processEngineConfiguration.setJdbcDriver(&quot;com.mysql.jdbc.Driver&quot;); processEngineConfiguration.setJdbcUrl(&quot;jdbc:mysql://192.168.118.128:3306/activiti_test?useUnicode=true&amp;characterEncoding=utf8&quot;); processEngineConfiguration.setJdbcUsername(&quot;root&quot;); processEngineConfiguration.setJdbcPassword(&quot;root&quot;); processEngineConfiguration.setDatabaseSchemaUpdate(ProcessEngineConfiguration.DB_SCHEMA_UPDATE_TRUE); ProcessEngine processEngine = processEngineConfiguration.buildProcessEngine(); System.out.println(&quot;processEngine1: &quot; + processEngine); } 工作流引擎配置也可以通过配置文件123456789```java@Testpublic void createTable2(){ System.out.println(&quot;init table&quot;); ProcessEngineConfiguration processEngineConfiguration = ProcessEngineConfiguration.createProcessEngineConfigurationFromResource(&quot;activiti.cfg.xml&quot;); ProcessEngine processEngine = processEngineConfiguration.buildProcessEngine(); System.out.println(&quot;processEngine2: &quot; + processEngine);} 或者, 更简单, 使用默认方式获取ProcessEngine的方式: 12345/** * getDefaultProcessEngine()会调用init方法, * 自动读取classpath下的activiti.cfg.xml文件来初始化数据库 */ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); 配置文件123456789```xml&lt;bean id=&quot;processEngineConfiguration&quot; class=&quot;org.activiti.engine.impl.cfg.StandaloneProcessEngineConfiguration&quot;&gt; &lt;property name=&quot;jdbcDriver&quot; value=&quot;com.mysql.jdbc.Driver&quot; /&gt; &lt;property name=&quot;jdbcUrl&quot; value=&quot;jdbc:mysql://192.168.118.128:3306/activiti_test?useUnicode=true&amp;amp;characterEncoding=utf8&quot; /&gt; &lt;property name=&quot;jdbcUsername&quot; value=&quot;root&quot; /&gt; &lt;property name=&quot;jdbcPassword&quot; value=&quot;root&quot; /&gt; &lt;property name=&quot;databaseSchemaUpdate&quot; value=&quot;true&quot; /&gt;&lt;/bean&gt; 完成之后, 查看数据库会看到ACT_开头的表 关于表表名的开头标识了该表业务含义: RE: repository, 该类表包含流程定义和流程静态资源. RU: runtime, 该类表包含流程实例, 任务, 变量, 异步任务等运行中的数据. ID: identity, 该类表包含身份信息, 比如用户, 组. HI: history, 这些表包含历史数据, 比如历史流程实例, 变量, 任务等. GE: 表示通用数据, 用于不同场景下, 如存放资源文件. 123456789101112131415161718192021222324252627# 部署对象, 流程定义相关select * from act_re_deployment #部署对象表select * from act_re_procdef #流程定义表select * from act_ge_bytearray #资源文件表select * from act_ge_property #主键生成策略表############################################ 流程实例, 执行对象, 任务select * from act_ru_execution #正在执行的执行对象表select * from act_hi_procinst #流程实例的历史表select * from act_ru_task #正在执行的任务表(只有节点是UserTask的时候, 该表中存在数据)select * from act_hi_taskinst #任务历史表(只有节点是UserTask的时候, 该表中存在数据)select * from act_hi_actinst #所有活动节点的历史表#############################################流程变量select * from act_ru_variable #正在执行的流程变量表select * from act_hi_varinst #历史的流程变量表 关于几个对象 ProcessDefinition: 流程定义对象, 可以从这里获取到资源文件. ProcessInstance: 流程实例对象, 代表流程定义的执行实例 一个流程实例包括了所有的运行节点. 可以利用这个对象来了解当前流程实例的进度等信息. 流程实例就表示一个流程从开始到结束的最大的流程分支, 即一个流程中的流程实例只有一个 Execution: 执行对象. 按照流程定义的规则执行一次的过程. Activiti用这个对象去描述流程执行的每一个节点. 在没有并发的情况下, Execution就如同ProcessInstance. 流程按照流程定义的规则执行一次的过程, 就可以表示执行对象Execution. 对应的表: ACT_RU_EXECUTION, ACT_HI_PROCINST, ACT_HI_ACTINST ProcessInstance extends Execution. 总结: 一个流程中, 执行对象可以存在多个, 但是流程实例只能有一个. 当流程按照规则只执行一次的时候, 那么流程实例就是执行对象. Task: 任务, 执行到某任务环节时生成的任务信息. 对应的表ACT_RU_TASK, ACT_HI_TASKINST 此外还有: ACT_RU_EXECUTION, ACT_HI_PROCINST, ACT_HI_ACTINST 关于几个Service RepositoryService: 管理流程定义. RuntimeService: 执行管理, 包括启动,推进, 删除流程实例等操作. TaskService: 任务管理 HistoryService: 历史管理(执行完的数据的管理) IdentityService: 组织机构管理 FormService: 一个可选服务, 任务表单管理 ManagerService","link":"/2017/11/15/Activiti初探/"},{"title":"Centos中SVN服务器实现自动更新","text":"前言 在上一篇《Centos搭建SVN服务器》中提到用钩子实现自动更新和同步. 难受的是我们的SVN服务器上是不允许我直接登录放hooks的, 但还是不想每次别人喊我手动去执行更新 于是尝试了以下两种方案 利于钩子实现有一个前提是: SVN服务器版本库和要更新的WEB目录在同一台机器(SERVER-A)上 实际是利用/hooks/post-commit这个文件的 提交后自动触发功能. 在post-commit中编写svn up脚本来实现SERVER-B提交后, 触发SERVER-A的post-commit文件. 自动执行脚本, 更新SERVER-A中的WEB目录. 如果SVN服务器版本库在SERVER-A上, 要更新的WEB目录在SERVER-B上呢? 方案一: 循环定时更新先来说一种较”傻”方法, 事实上我刚开始也是这么做的 在WEB目录所在机器(SERVER-B)上, 写一个循环定时更新的脚本. 例如每过10秒update: 123456#!/bin/bashwhile [ 1 -gt 0 ]do sleep 10 svn update /home/www/project/ &amp;&gt;/dev/nulldone 加上日志的方式: 12345678910111213#!/bin/bashpre_update=\"$(svn log /home/www/project/ | head -2 | gawk NR==2{'print $1'})\"while [ 1 -gt 0 ]do svn update /home/www/project/ &amp;&gt;/dev/null now_update=\"$(svn log /home/www/project/ | head -2 | gawk NR==2{'print $1'})\" if [[ ${now_update} &gt; ${pre_update} ]] then echo \" update_time : $(date) \" &gt;&gt;/tmp/svn_update.log pre_update=${now_update} fi sleep 5done 方案二: 利用钩子触发更新还是利用post-commit的提交后自动触发功能. 但是这里update的是另外一台机器了, 所以就需要远程传输文件. 命令介绍两个命令: scp: 用于在Linux下进行远程拷贝文件的命令 和它类似的命令有cp，不过cp只是在本机进行拷贝不能跨服务器，而且scp传输是加密的. rsync: 是一个远程数据同步工具，可通过LAN/WAN快速同步多台主机间的文件 rsync使用所谓的“rsync算法”来使本地和远程两个主机之间的文件达到同步.这个算法只传送两个文件的不同部分，而不是每次都整份传送，因此速度相当快. 注意: 虽然 rsync比scp会快一点，但当小文件众多的情况下，rsync会导致硬盘I/O非常高，而scp基本不影响系统正常使用。 具体的语法和参数参照: rsync命令: rsync命令 scp命令: scp命令 配置ssh例如: 上传本地文件到远程机器指定目录: scp [本地路径] root@10.10.10.10:[远程路径] 之后会让你输入远程登录的密码. 因为最终是以脚本方式来拷贝到远程机器. 为了方便, 我配置了SSH免密登录 配置很简单: 命令在A上生成秘钥, 在.ssh中就能看到id_rsa和id_rsa.pub两个文件 把id_rsa.pub随便拷贝到B上, 并将其中内容放到.ssh下的authorized_keys就可以了注意权限和用户就好了.可以参考: SSH免密码远程登录Linux 配置完成后可以ssh root@[B的IP]试试, 应该不输入密码就登录了. 写脚本下面, 就开始vi xx/project/hooks/post-commit: 1234567891011121314151617181920212223242526#!/bin/shexport LANG=en_US.utf8SVN=/usr/bin/svnWEB=/home/www/project/RSYNC=/usr/bin/rsyncLOG=/tmp/uplog_xfy.logWEBIP=\"192.168.100.212\"echo \" start update ! \" &gt;&gt; $LOG#更新本机WEB目录$SVN update $WEB --username 'xiefy' --password 'xiefy'#远程传输if [ $? == 0 ]then echo `date` &gt;&gt; $LOG #chown -R xiefy:cloudlink /home/xiefy/project #1.scp方式 scp -r $WEB root@$WEBIP:/home/xiefy/ &gt;&gt; $LOG #2.rsync同步方式 #rsync -vaztpH --timeout=90 $WEB root@$WEBIP:/home/xiefy/ &gt;&gt;LOG fi 好了, 作为一名开发狗. Linux下的SVN服务器就研究到这… The End","link":"/2017/03/09/Centos中SVN服务器实现自动更新/"},{"title":"总结Eclipse设置","text":"总结一些Eclipse常用的设置, 不设置不好用 去掉红色波浪线的拼写检查: Windows-&gt;Preferences-&gt;General-&gt;Editors-&gt;Text Editors-&gt; Spelling将Enable spell checking取消即可. 去掉语法提示: General-Editors-Text Editors-Annotations-Errors将Show in的三个选项都去掉. 调整字体: Windows -&gt; Preferences -&gt; Color and Font -&gt; Basic -&gt; TextFont 调整背景色: Windows -&gt; Editors -&gt; Text Editors -&gt; 下面BackGround选择:色调85，饱和度90，亮度205.(豆沙绿) 去掉自动添括号: Windows -&gt; Java -&gt; Editor -&gt; Content Assist -&gt; 把Insert single proposals automatically前面的勾去掉 去掉自动添括号: Server Locations:第二个: User Tomcat installation(takes control of Tomcat installation)部署deploy可改为原来的: webapps, 不改的话另外创建一个wtpwebapps 修改Timeouts 默认的太小了, 如果你的项目在这个时间内还跑不起来, 就会报错, 所以seconds改大点. 后续补充 TODO","link":"/2015/06/15/Eclipse的几个常用小设置/"},{"title":"Docker镜像仓库","text":"[TOC] 这里仅以一个场景: 自己制作的镜像如何分享, 主要说明私有镜像仓库Harbor的搭建使用 Docker Hub首先介绍Docker Hub 推送&amp;拉取镜像推送自己的镜像到Docker Hub上, 因为网速… 这里用一个超小的镜像试验 123[root@bogon ~]# docker images | grep busyREPOSITORY TAG IMAGE ID CREATED SIZEbusybox latest af2f74c517aa 6 days ago 1.2MB 1. 打标签 1docker tag busybox:latest thank037/busybox:1.0 123[root@bogon ~]# docker images | grep busybusybox latest af2f74c517aa 6 days ago 1.2MBthank037/busybox latest af2f74c517aa 6 days ago 1.2MB 注意这里的thank037就是DockerHub中的username 2. 推送 1docker push thank037/busybox:1.0 去自己的镜像仓库看下吧, 有了 如果推送过程中提示: denied: requested access to the resource is denied 登录一下即可: docker login 拉取镜像很常见 1docker pull thank037/busybox:1.0 国内的镜像仓库国内也有很多, 例如网易, DaoCloud, 阿里云等, 上传镜像和拉取镜像的姿势基本和Docker Hub一样 网易的这两天个人认证无法使用, 必须要企业认证, 所以这里实验阿里云的 首先需要在阿里云-容器镜像服务中创建一个命名空间 可以设置仓库类型, 公有私有 推送和拉取省略自己制作镜像的过程, 这里以一个我已经拉取好的官方镜像redis:latest为例 首先登录1$ sudo docker login --username=xxx registry.cn-hangzhou.aliyuncs.com 12345678# tagdocker tag redis:latest registry.cn-hangzhou.aliyuncs.com/thank/redis:1.0# push docker push registry.cn-hangzhou.aliyuncs.com/thank/redis:1.0# pulldocker pull registry.cn-hangzhou.aliyuncs.com/thank/redis:1.0 去镜像仓库看看吧, 有了 基于代码源构建上面的方式是自己在docker环境中制作好镜像, 上传到仓库, 这里介绍一种基于代码源的构建方式 以GitHub为例, 代码源为: https://github.com/Thank037/spring-cloud-study2 创建镜像仓库: 写好仓库名称, 选择好命名空间即可 设置代码源: 选择github, 第一次使用需要绑定账号 构建设置 构建设置: 例如可以开启代码变更自动构建镜像 构建规则设置: 设置代码源的分支/tag, Dockerfile目录和名字, 需要构建的镜像版本 开始构建: 点击规则上的立即构建即可 每次的构建过程会输出到构建日志中, 可以进行查看 私有仓库 出于安全和速度的考虑, 很多企业会选择搭建私有镜像仓库 registry安装和使用是一个官方提供的镜像仓库, 安装极其简单! 1docker pull registry:2 拉取完成后启动registry: docker run -d -p 5000:5000 推送和拉取过程跟推送到公有仓库差不多 12345678# 打标签docker tag busybox:latest localhost:5000/busybox:1.0# 推送docker push localhost:5000/busybox:1.0# 拉取docker pull localhost:5000/busybox:1.0 需要注意的是 如果其它主机在推送到registry所在主机时可能出现安全错误, 也就是镜像仓库和docker客户端不在一台机器上 在Client端/etc/docker/daemon.json中加入: 1{ \"insecure-registries\":[\"ip:5000\"] } 然后重启docker即可 systemctl restart docker 除了推送和拉取镜像, registry还提供了一些API, 查看镜像信息, 删除等… 例如: 12[root@bogon ~]# curl localhost:5000/v2/_catalog{\"repositories\":[\"busybox\"]} 可以看到registry很轻便, 具备了基础的镜像管理功能, 但是有以下两个缺点 不具备授权认证功能, 需要自己去做一些认证方案 虽有API, 但没有一个好看的界面 harborharbor意为港湾, 很贴合它的作用 github地址: https://github.com/goharbor/harbor 记得以前是在vmware下, 现在地址转到goharbor下了 官方的定义是企业级私有Registry服务器, 实际内部也是依靠docker registry 那么它与registry相比, 一定提供了很多企业级的特性, 例如: 提供良好的Web界面 提供良好的安全机制, 包括角色, 权限控制, 日志等 支持水平扩展 传输优化, 因为镜像的分层机制, 所以每次传输并非全量, 从而提升速度 搭建环境准备 在安装harbor之前, 需要docker环境, 除此还需要docker-compose, 不说了 安装Harbor 在github release中查看需要安装的版本, 这里我用当前最新的v1.7.5 文档中有两种安装方式: offline和online, 也就是在线和离线 online: 在线方式会下载一个很小的tar包, 里面只有docker-compose.yml和启动脚本, 会在启动安装时在线拉取镜像 offline: 下载一个大概500多M的tar包, 里面包括了镜像文件, 所以安装时会快 我的浏览器设置了代理, 所以选择offline的离线包, 再上传放到docker环境的主机上 大致看下解压后的目录 123456789101112131415[root@localhost opt]# tree harbor -CL 2harbor ├── common │ ├── config │ └── templates ├── docker-compose.chartmuseum.yml ├── docker-compose.clair.yml ├── docker-compose.notary.yml ├── docker-compose.yml ├── harbor.cfg ├── harbor.v1.7.5.tar.gz ├── install.sh ├── LICENSE ├── open_source_license └── prepare 接下来对harbor.cfg几个配置做修改, 我修改了如下: 123456789101112# 注释中也说了, 不要设置成localhost和127.0.0.1阿hostname = hub.thank.com# 邮箱相关的设置email_server = smtp.163.comemail_server_port = 25email_username = coderthank@163.comemail_password = ******email_from = thank &lt;coderthank@163.com&gt;# 修改admin默认的登录密码harbor_admin_password = 123123 更多详细的设置可参考官方文档和配置文件的注释 然后执行install.sh即可 12345678910111213141516171819202122[Step 0]: checking installation environment ...Note: docker version: 1.13.1Note: docker-compose version: 1.16.1[Step 1]: loading Harbor images ......略[Step 2]: preparing environment ......略[Step 3]: checking existing instance of Harbor ...[Step 4]: starting Harbor ......略✔ ----Harbor has been installed and started successfully.----Now you should be able to visit the admin portal at http://hub.thank.com.For more details, please visit https://github.com/goharbor/harbor . 从Step可以看到这个一键脚本帮我们做了傻瓜式的安装 docker-compose帮我们完成了Harbor所需要的多个容器服务 来看一眼 镜像 12345678910111213141516[root@localhost harbor]# docker images --format \"table {{.Repository}}\\t{{.Size}}\" | grep harborgoharbor/chartmuseum-photon 113 MBgoharbor/harbor-migrator 679 MBgoharbor/redis-photon 101 MBgoharbor/clair-photon 164 MBgoharbor/notary-server-photon 135 MBgoharbor/notary-signer-photon 132 MBgoharbor/harbor-registryctl 102 MBgoharbor/registry-photon 86.7 MBgoharbor/nginx-photon 35.9 MBgoharbor/harbor-log 81.4 MBgoharbor/harbor-jobservice 84.1 MBgoharbor/harbor-core 95.6 MBgoharbor/harbor-portal 40.6 MBgoharbor/harbor-adminserver 72.3 MBgoharbor/harbor-db 138 MB 容器 1234567891011[root@localhost harbor]# docker ps --format \"table {{.ID}}\\t{{.Image}}\\t{{.Ports}}\" | grep harborc517af1cff6a goharbor/nginx-photon:v1.7.5 0.0.0.0:80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp, 0.0.0.0:4443-&gt;4443/tcp7bb6eac57291 goharbor/harbor-jobservice:v1.7.5723cb4c96a0d goharbor/harbor-portal:v1.7.5 80/tcp1425f64756d1 goharbor/harbor-core:v1.7.5f19a00ce693b goharbor/registry-photon:v2.6.2-v1.7.5 5000/tcp4d3f0f8929c7 goharbor/harbor-adminserver:v1.7.523df470d2b0d goharbor/harbor-db:v1.7.5 5432/tcpe7d6d1cc3b1b goharbor/harbor-registryctl:v1.7.56e2912ec566e goharbor/redis-photon:v1.7.5 6379/tcpee8685192041 goharbor/harbor-log:v1.7.5 127.0.0.1:1514-&gt;10514/tcp 界面使用因为刚才在配置文件赔了一个假域名, 这里我修改下hosts1192.168.118.143 hub.thank.com 访问http://hub.thank.com, 可以看到Harbor的登录页面 直接访问IP也可, 默认80端口 登录后的主页面 测试: 创建用户thank, 创建项目cloudlink-base和项目中添加用户thank, 角色为项目管理员 其它功能就不做介绍了, 中文简体点一点就好啦 运维管理从前面的一键启动脚本也能看出, 我们可以用docker-compose统一完成harbor服务的运维管理 停止: docker-comose stop etc… image管理这里测试一下镜像的拉取和上传 方法跟其它仓库的没什么不一样, 不说了 1234567891011# logindocker login hub.thank.com# image tag docker tag redis:latest hub.thank.com/cloudlink-base/thank-redis:latest# push docker push hub.thank.com/cloudlink-base/thank-redis:latest# pulldocker pull https问题刚才在配置文件harbor.cfg中可以看到默认配置了http协议访问 这里在其它主机上传镜像时, docker客户端默认都是https访问Harbor, 所以会出现connection refused的错误 有两种解决办法: 前面提到过, 在/etc/docker/daemon.json加入 123{ \"insecure-registries\" : [\"hub.thank.com\"]} 修改harbor配置文件为https方式: ui_url_protocol = https, 除此之外还需要配置证书 不是很复杂, 可以参照搭建Harbor企业级docker仓库#五、Harbor配置TLS证书","link":"/2019/04/10/Docker镜像仓库/"},{"title":"Centos搭建SVN服务器","text":"序言工作所需, 写接口很忙, 不想每次别人让我手动去svn up更新代码! 预期目的: 搭建完成SVN服务器, 创建版本库 配置用户和组, 使得可以完成基本的checkout和commit操作 实现仓库代码更新后自动同步到WEB目录下, 不用手动(SVN钩子) 安装环境并创建SVN版本库安装安装Subversion: 1yum install -y subversion 查看是否安装成功: 1svnserve --version 显示如下信息: svnserve, version 1.7.14 (r1542130)…. 创建版本库下面创建版本库 建立目录: mkdir /cloudlink/webapp ` svnadmin create /cloudlink/webapp/project ` 可以看到在project下生成如下版本文件: 1conf db format hooks locks README.txt 进入conf目录可以看到: authz: 权限配置文件 passwd: 用户名口令配置文件 svnserve.conf: 服务配置文件 配置用户和组配置authz12345678910111213141516171819[aliases]# joe = /C=XZ/ST=Dessert/L=Snake City/O=Snake Oil, Ltd./OU=Research Institute/CN=Joe Average[groups]# harry_and_sally = harry,sally# harry_sally_and_joe = harry,sally,&amp;joecloudlink = xiefayang ### 将用户加入用户组# [/foo/bar]# harry = rw# &amp;joe = r# * =###用户组cloudlink对版本库project具有读和写的权限# [repository:/baz/fuz] # @harry_and_sally = rw# * = r[project:/]@cloudlink = rw 配置passwd1234[users]# harry = harryssecret# sally = sallyssecretxiefayang = xiefayang 配置svnserve.conf找到如下配置项, 去掉”#”, 做相应配置 1234567891011121314# 匿名用户访问权限: 无anon-access = none# 普通用户访问权限: 读,写auth-access = write# 密码文件password-db = passwd#权限配置文件authz-db = authz# 版本库所在地realm = /cloudlink/webapp/project 测试启动SVN服务 1svn -d -r /cloudlink/webapp 如果不能启动成功就杀掉再重启 1ps aux | grep svn 测试checkout, 切换到/home/www目录下 1svn co svn://localhost/project 查看是否检出成功 测试commit 123vi test.pysvn add test.pysvn commit test.py -m \"test commit file\" 如果显示提交成功，则服务器搭建成功． 其他测试在另一台机器(windows)上安装SVN, 这里我安装了TortoiseSVN 点击SVN Checkout: 填入相应仓库地址, 用户名和密码, 点击OK即可检出 注意: 如果检出失败了, 就需要检查一下/etc/sysconfig下的iptables. 加入: 1-A INPUT -p tcp -m state --state NEW -m tcp --dport 3690 -j ACCEPT 钩子实现自动更新和同步这一步主要实现当更改完代码提交到SVN服务器后, WEB目录下的仓库自动同步. 不需要手动update了 在刚才版本库目录的project下新建post-commit文件(post-commit意味提交后调用此文件更新, post-commit.tmpl只是个模板, 不用管) vi post-commit 12345#!/bin/shexport LANG=en_US.utf8SVN_PATH=/usr/bin/svn WEB_PATH=/home/www #web目录$SVN_PATH update $WEB_PATH --username 'xiefayang' --password '111111' --no-auth-cache ​ 看看/home/www 的用户组和所有者(假如都是:thank) 修改post-commit用户为www的目录用户: chown thank:thank post-commit 赋予执行权限: chmod 755 post-commit 至此, 一定要区分[SVN服务器]和[WEB目录]这两个地址 如果SVN服务器和WEB目录都在同一台机器. 那么在其他地点修改代码提交后, WEB目录就会自动更新同步. but: 如果SVN服务器在远程另外一台机器. 待研究~","link":"/2017/02/28/Centos搭建SVN服务器/"},{"title":"Elasticsearch初探","text":"[TOC] 前言最近这个好火, 简单体验下 一: 安装和启动ElasticSearch下载 官方地址: https://www.elastic.co/cn/downloads/elasticsearch github: https://github.com/elastic/elasticsearch 下载或clone后解压 单实例节点启动123# cd elasticsearch目录下bin/elasticsearchbin/elasticsearch -d # 后台启动 默认端口9200, 启动完成后访问http://ip:9200 即可查看到节点信息 启动中我遇到两个错误: 错误一: 12can not run elasticsearch as root # 不能以root用户启动 123[root@01 bin]# groupadd xiefy[root@01 bin]# useradd xiefy -g xiefy -p 123123[root@01 bin]# chown -R xiefy:xiefy elasticsearch 错误二: 123456ERROR: [2] bootstrap checks failed[1]: max file descriptors [65535] for elasticsearch process is too low, increase to at least [65536][2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]-- 错误[1]: max file descriptors过小-- 错误[2]: max_map_count过小, max_map_count文件包含限制一个进程可以拥有的VMA(虚拟内存区域)的数量，系 统默认是65530，修改成262144 123456# 切换到root用户vi /etc/security/limits.conf# 添加如下* soft nofile 65536* hard nofile 65536 1234567# 切换到root用户vi /etc/sysctl.conf# 添加如下vm.max_map_count=655360# 重新加载配置文件或重启sysctl -p # 从配置文件“/etc/sysctl.conf”加载内核参数设置 elasticsearch-head插件安装 elasticsearch-head 是一个用于浏览Elastic Search集群并与之进行交互(操作和管理)的web界面 GitHub: https://github.com/mobz/elasticsearch-head 要使用elasticsearch-head, 需要具备nodejs环境: nodejs安装 下载源码: https://nodejs.org/en/download/ 安装方式有多种, 我用源码安装方式(包含npm) 下载源码: 1https://nodejs.org/dist/v8.9.4/node-v8.9.4.tar.gz 解压源码: 1tar xzvf node-v* &amp;&amp; cd node-v* 安装必要的编译软件 1sudo yum install gcc gcc-c++ 编译 12./configuremake 编译&amp;安装 1sudo make install 查看版本 12node --versionnpm -version 下载或克隆elasticsearch-head后, 进入elasticsearch-head-master目录: npm install 速度太慢可以使用淘宝镜像: npm install -g cnpm --registry=https://registry.npm.taobao.org npm run start open http://localhost:9100/ 这时可以访问到页面, 并没有监听到集群. 解决head插件和elasticsearch之间访问跨域问题. 修改elasticsearch目录下的elasticsearch.yml 123# 加入以下内容http.cors.enabled: truehttp.cors.allow-origin: \"*\" 然后: http://localhost:9100/ 即可访问到管理页面. 分布式安装启动elasticsearch的横向扩展很容易: 这里建立一个主节点(node-master), 两个随从节点(node-1, node-2) 我提前拷贝了三个es: 12345678[xiefy@01 elk]$ lltotal 33620-rw-r--r-- 1 root root 33485703 Aug 17 22:42 elasticsearch-5.5.2.tar.gzdrwxr-xr-x 7 root root 4096 Jan 8 11:17 elasticsearch-head-masterdrwxr-xr-x 9 xiefy xiefy 4096 Jan 8 10:15 elasticsearch-masterdrwxr-xr-x 9 xiefy xiefy 4096 Jan 8 14:13 elasticsearch-node1drwxr-xr-x 9 xiefy xiefy 4096 Jan 8 14:16 elasticsearch-node2-rw-r--r-- 1 root root 921421 Jan 8 11:14 master.zip 分别配置三个es目录中的config/elasticsearch.yml node-master: 123456789cluster.name: xiefy_elastic node.name: node-master node.master: true network.host: 0.0.0.0 # 除此之外, head插件需要连接到port: 9200的节点上, 还需要这个配置, # 用来允许 elasticsearch-head 运行时的跨域http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot; node-1: 12345cluster.name: xiefy_elastic node.name: node-1network.host: 0.0.0.0http.port: 9201 discovery.zen.ping.unicast.hosts: [&quot;127.0.0.1&quot;] node-2: 参考node-1 相关配置解释: cluster.name: 集群名称, 默认是elasticsearch node.name: 节点名称, 默认随机分配 node.master: 是否是主节点, 默认情况下可不写, 第一个起来的就是Master, 挂掉后会重新选举Master network.host: 默认情况下只允许本机通过localhost或127.0.0.1访问, 为了测试方便, 我需要远程访问所以配成了0.0.0.0 http.port: 默认为9200, 同一个服务器下启动多个es节点, 默认端口号会从9200默认递增1, 这里我手动指定了 discovery.zen.ping.unicast.hosts: [&quot;host1&quot;, &quot;host2&quot;] Elasticsearch默认使用服务发现(Zen discovery)作为集群节点间发现和通信的机制, 当启动新节点时，通过这个ip列表进行节点发现，组建集群. 分别启动三个es实例和head插件: 访问http://ip:9100: 二: 基础概念ElasticSearch与关系型数据库的一些术语比较 关系型数据库 Ela Database Index Table Type Row Document Column Field Schema Mapping Index Everything is indexed SQL` Query DSL select * from table… GET http://… update tables set… PUT http://… Server: Node: 一个server实例 Cluster：多个node组成cluster Shard：数据分片，一个index可能会有多个shards，不同shards可能在不同nodes Replica：shard的备份，有一个primary shard，其余的叫做replica shards Gateway：管理cluster状态信息 Shards &amp; Replicas副本很重要, 主要有两个原因: 它在分片/节点发生故障时来保障高可用性, 因此, 副本分片永远不会和主/原始分片分配在同一个节点中 它允许扩展搜索量和吞吐量, 因为可以在所有副本上并行执行搜索 可以在创建索引时为索引定义分片和副本的数量。创建索引后，可以随时动态更改副本数，但不能再更改分片数。 每个Elasticsearch分片都是Lucene索引, 在一个Lucene上的最大文档数量大约21亿(Integer.MAX_VALUE-128 ) THE REST API: elasticsearh提供了丰富的REST API来与集群交互, 这些API包括: 检查你的集群，节点和索引的健康情况，状态和统计 管理你的集群，节点，索引数据和metadata 针对索引执行CURD（创建，读取，更新，删除）和搜索操作 执行高级的搜索操作，例如分页，排序，过滤，聚合，脚本等等。 通过_cat可以查看到很多资源: 123456789101112131415161718192021222324252627282930curl -XGET &apos;localhost:9200/_cat&apos;=^.^=/_cat/allocation/_cat/shards/_cat/shards/{index}/_cat/master/_cat/nodes/_cat/tasks/_cat/indices/_cat/indices/{index}/_cat/segments/_cat/segments/{index}/_cat/count/_cat/count/{index}/_cat/recovery/_cat/recovery/{index}/_cat/health/_cat/pending_tasks/_cat/aliases/_cat/aliases/{alias}/_cat/thread_pool/_cat/thread_pool/{thread_pools}/_cat/plugins/_cat/fielddata/_cat/fielddata/{fields}/_cat/nodeattrs/_cat/repositories/_cat/snapshots/{repository}/_cat/templates 偷偷看一眼集群健康状态: curl -XGET 'localhost:9200/_cat/health?v' 节点的信息: curl -XGET 'localhost:9200/_cat/nodes?v&amp;pretty' 而关于和集群索引和文档的交互, 放在下一章. 三: 基础用法索引创建 方式一: head插件可以直接新建/删除/查询索引(Index) 方式二: 通过rest api 12345678# 新建curl -X PUT 'localhost:9200/book'# 删除curl -X DELETE 'localhost:9200/book'# 查看当前节点下所有Indexcurl -X GET 'http://localhost:9200/_cat/indices?v' 这样创建的Index是没有结构的. 可以看到索引信息的mappings:{} 下面来定义一个有结构映射的Index PUT http://ip:9200/people 123456789101112131415161718192021{ \"settings\": { \"number_of_shards\": 3, \"number_of_replicas\": 1 }, \"mappings\": { \"man\": { \"properties\": { \"name\": {\"type\": \"text\"}, \"country\": {\"type\": \"keyword\"}, \"age\": {\"type\": \"integer\"}, \"birthday\": { \"type\": \"date\", \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\" } } }, \"women\": { } }} 里面设置了分片数, 备份数, 一个Index和两个type的结构映射. 插入数据 PUT http://47.94.210.157:9200/people/man/1 指定ID为1 123456{ \"name\": \"伊布\", \"country\": \"瑞典\", \"age\": 30, \"birthday\": \"1988-12-12\"} 如果不指定ID, 会生成为随机字符串, 此时需要改为POST方式 POST http://47.94.210.157:9200/people/man 神奇的一点, es不会限制你在创建一个文档之前索引和类型必须存在, 不存在时, es会自动创建它. 更新数据 POST http://47.94.210.157:9200/people/man/1/_update – 修改ID为1的文档 12345{ \"doc\": { \"name\": \"梅西梅西很像很强\" }} 还可以通过脚本方式更新: 1{ \"script\": \"ctx._source.age += 10\" } 这里是把年龄加10, ctx.source引用了当前文档. 索引/替换文档 PUT http://47.94.210.157:9200/football/man/1 – 修改ID为1的文档 1{&quot;name&quot;: &quot;伊布asdasadasds3333333333333333sd&quot;} elasticsearch将会用一个新的文档取代（即重建索引）旧的文档 删除数据 删除文档: DELETE http://47.94.210.157:9200/people/man/1 查看数据 根据ID查询 GET http://ip:9200/people/man/1?pretty=true 12345678910111213{ \"_index\": \"people\", \"_type\": \"man\", \"_id\": \"2\", \"_version\": 1, \"found\": true, \"_source\": { \"name\": \"伊布\", \"country\": \"瑞典\", \"age\": 34, \"birthday\": \"1972-12-12\" }} found字段表示查到与否 ​ 查询全部 GET http://ip:9200/Index/Type/_search 或带排序带分页的查询: POST http://47.94.210.157:9200/people/_search ES 默认 从0开始(from), 一次返回10条(size), 并按照_score字段倒排, 也可以自己指定 12345678910# 带排序带分页的查询{ \"query\": { \"match_all\": {} }, \"sort\": [{ \"birthday\": {\"order\": \"desc\"} } ], \"from\": 0, \"size\": 5} 1234567891011121314151617181920212223242526272829{ \"took\": 5, \"timed_out\": false, \"_shards\": { \"total\": 3, \"successful\": 3, \"failed\": 0 }, \"hits\": { \"total\": 3, \"max_score\": 1, \"hits\": [ { \"_index\": \"people\", \"_type\": \"man\", \"_id\": \"2\", \"_score\": 1, \"_source\": { \"name\": \"伊布\", \"country\": \"瑞典\", \"age\": 34, \"birthday\": \"1972-12-12\" } }, .... .... ] }} 12345678# 返回结果解释- took: 耗时(单位毫秒)- timed_out: 是否超时- hits: 命中的记录数组 - total: 返回的记录数 - max_score: 最高匹配度分数 - hits: 记录数组 - _score: 匹配度 关键字查询 12345{ \"query\": { \"match\": {\"name\": \"梅西\"} }} 注意: 这里是模糊匹配查询, 例如查询的值是”西2”, 那么会查询所有记录name有”西”和name有”2”的. 关于查询多个关键字之间的逻辑运算: 如果这样写, 两个关键字会被认为是 or的关系来查询 12345{ \"query\": { \"match\": {\"name\": \"西 布\"} }} 如果是and关系来搜索, 需要布尔查询 12345678910{ \"query\": { \"bool\": { \"must\": [ {\"match\": {\"name\": \"西\"}} , {\"match\": {\"name\": \"2\"}} ] } }} 聚合查询 POST http://47.94.210.157:9200/people/_search 分组聚合 1234567{ \"aggs\": { \"group_by_age\": { \"terms\": {\"field\": \"age\"} } }} 返回结果中, 除了有hits数组, 还有聚合查询的结果: 在单个请求中就可以同时查询数据和进行多次聚合运算是非常有意义的, 他可以降低网络请求的次数 1234567891011121314151617# 聚合查询结果\"aggregations\": { \"group_by_age\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": 24, \"doc_count\": 2 }, { \"key\": 32, \"doc_count\": 1 } ] }} 支持多个聚合, 聚合结果也会返回多个: 12345678910{ \"aggs\": { \"group_by_age\": { \"terms\": {\"field\": \"age\"} }, \"group_by_age\": { \"terms\": {\"field\": \"age\"} } }} 其他功能函数 1234567{ \"aggs\": { \"tongji_age\": { \"stats\": {\"field\": \"age\"} } }} stats指定计算字段, 返回结果包括了总数, 最小值, 最大值, 平均值和求和 123456789\"aggregations\": { \"tongji_age\": { \"count\": 3, \"min\": 24, \"max\": 32, \"avg\": 26.666666666666668, \"sum\": 80 } } 也可指定某种类型的计算 1234567{ \"aggs\": { \"tongji_age\": { \"sum\": {\"field\": \"age\"} } }} 返回结果 12345\"aggregations\": { \"tongji_age\": { \"value\": 80 } } 四: 高级查询分为子条件查询和复合条件查询: 类型: 全文本查询: 针对文本类型数据 字段级别查询: 针对结构化数据, 如日期, 数字 文本查询 模糊匹配 12345{ \"query\": { \"match\": {\"name\": \"西2\"} }} 短语匹配 12345{ \"query\": { \"match_phrase\": {\"name\": \"西2\"} }} 多个字段匹配 12345678{ \"query\": { \"multi_match\": { \"query\": \"瑞典\", \"fields\": [\"name\", \"country\"] } }} 语法查询: 根据语法规则查询: 带有布尔逻辑的查询 1234567{ \"query\": { \"query_string\": { \"query\": \"(西 AND 梅) OR 布\" } }} query_string 查询多个字段 12345678{ \"query\": { \"query_string\": { \"query\": \"西梅 OR 瑞典\", \"fields\": [\"country\", \"name\"] } }} 结构化数据查询123{ \"query\": {\"term\": { \"age\": 24}}} 带范围的查询 12345678910{ \"query\": { \"range\": { \"birthday\": { \"gte\": \"1980-01-01\", \"lte\": \"now\" } } }} 子条件查询Filter Context: 用来做数据过滤, 在查询过程中, 只判断该文档是否满足条件(y or not) Filter和Query的区别? Filter要结合bool使用, 查询结果会放入缓存中, 速度较Query更快 123456789{ \"query\": { \"bool\": { \"filter\": { \"term\": { \"age\": 24 } } } }} 复合查询 固定分数查询 12345{ \"query\": { \"match\": {\"name\": \"梅西\"} }} 可以看到查询的结果, 每条数据的_score不同, 代表了与查询值的匹配程度的分数. 但查询不一定都需要产生文档得分，特别在过滤文档集合的时候。为了避免不必要的文档得分计算，Elasticsearch会检查这种情况并自动的优化这种查询。 例如在bool查询中返回年龄范围在20-50的文档, 对我来说这个范围内的文档都是等价的, 即他们的相关度都是一样的(filter子句查询，不会改变得分). 12345678910{ \"query\": { \"constant_score\": { \"filter\": { \"match\": {\"name\": \"梅西\"} }, \"boost\": 2 } }} 可以看到查询结果, 每条数据的_score都为2, 如果不指定boost则默认为1 布尔查询 12345678910{ \"query\": { \"bool\": { \"should\": [ { \"match\": {\"name\": \"梅西\"}}, { \"match\": {\"country\": \"阿\"}} ] } } } 这里两个match之间是或的逻辑关系. should 如果改为 must 代表与逻辑. 我们也可以把must，should，must_not同时组合到bool子句。此外，我们也可以组合bool 到任何一个bool子句中，实现复杂的多层bool子句嵌套逻辑。 再加一层Filter, 只有age=32的能返回 123456789101112131415{ \"query\": { \"bool\": { \"must\": [ { \"match\": {\"name\": \"梅西\"} }, { \"match\": {\"country\": \"阿根廷\"}} ], \"filter\": [{ \"term\": { \"age\": 32 } }] } } } country=阿根廷的不返回: 123456789{ \"query\": { \"bool\": { \"must_not\": { \"term\": {\"country\": \"阿根廷\"} } } }} 五: 关于中文分词为什么需要中文分词? 首先看一下默认的分词规则. 12345678910111213141516171819202122232425262728# 英文GET http://47.94.210.157:9200/_analyze?analyzer=standard&amp;pretty=true&amp;text=hello world, elasticsearch{ \"tokens\": [ { \"token\": \"hello\", \"start_offset\": 0, \"end_offset\": 5, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 0 }, { \"token\": \"world\", \"start_offset\": 6, \"end_offset\": 11, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 1 }, { \"token\": \"elasticsearch\", \"start_offset\": 13, \"end_offset\": 26, \"type\": \"&lt;ALPHANUM&gt;\", \"position\": 2 } ]} 可以看到, 英文的默认分词是根据标点符号和空格默认来分的. 再看看中文的: 123456789101112131415161718192021222324252627GET http://47.94.210.157:9200/_analyze?analyzer=standard&amp;pretty=true&amp;text=你好,啊{ \"tokens\": [ { \"token\": \"你\", \"start_offset\": 0, \"end_offset\": 1, \"type\": \"&lt;IDEOGRAPHIC&gt;\", \"position\": 0 }, { \"token\": \"好\", \"start_offset\": 1, \"end_offset\": 2, \"type\": \"&lt;IDEOGRAPHIC&gt;\", \"position\": 1 }, { \"token\": \"啊\", \"start_offset\": 3, \"end_offset\": 4, \"type\": \"&lt;IDEOGRAPHIC&gt;\", \"position\": 2 } ]} 可以看到ES对中文的分词并不智能, 是将汉字全部分开了, 所以引入中文分词. IK IK: https://github.com/medcl/elasticsearch-analysis-ik The IK Analysis plugin integrates Lucene IK analyzer into elasticsearch, support customized dictionary. 安装 1.download or compile optional 1 - download pre-build package from here: https://github.com/medcl/elasticsearch-analysis-ik/releases unzip plugin to folder your-es-root/plugins/ optional 2 - use elasticsearch-plugin to install ( version &gt; v5.5.1 ): ./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.2.1/elasticsearch-analysis-ik-6.2.1.zip 2.restart elasticsearch 两种安装方式, 任选其一, 注意版本就好 Github里有Quick Example 可以看下怎么使用 需要在建立索引时指定ik分词器, 建立索引和搜索索引字段都需要指定, 例如: &quot;analyzer&quot;: &quot;ik_max_word&quot;和&quot;search_analyzer&quot;: &quot;ik_max_word&quot; IK提供两种分词规则: ik_max_word: 会将文本做最细粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,中华人民,中华,华人,人民共和国,人民,人,民,共和国,共和,和,国国,国歌”，会穷尽各种可能的组合； ik_smart: 会做最粗粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,国歌”。 除此之外, IK也支持扩展自定义词典, 以及热更新. 123456789101112131415161718192021# TestGET http://47.94.210.157:9200/_analyze?analyzer=ik_max_word&amp;pretty=true&amp;text=你好,啊{ \"tokens\": [ { \"token\": \"你好\", \"start_offset\": 0, \"end_offset\": 2, \"type\": \"CN_WORD\", \"position\": 0 }, { \"token\": \"啊\", \"start_offset\": 3, \"end_offset\": 4, \"type\": \"CN_CHAR\", \"position\": 1 } ]} 六: Spring Boot 集成 Elastic Search版本参考 Spring Boot Version (x) Spring Data Elasticsearch Version (y) Elasticsearch Version (z) x &lt;= 1.3.5 y &lt;= 1.3.4 z &lt;= 1.7.2* x &gt;= 1.4.x 2.0.0 &lt;=y &lt; 5.0.0** 2.0.0 &lt;= z &lt; 5.0.0** 服务器集群ES版本 5.5.2 Spring boot 1.5.9.RELEASE Elastic Search 5.5.2 log4j-core 2.7 集成步骤 引入Maven依赖: 123456789101112131415161718&lt;properties&gt; &lt;log4j-core.version&gt;2.7&lt;/log4j-core.version&gt; &lt;elasticsearch-version&gt;5.5.2&lt;/elasticsearch-version&gt;&lt;/properties&gt;&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;transport&lt;/artifactId&gt; &lt;version&gt;${elasticsearch.version}&lt;/version&gt;&lt;/dependency&gt;&lt;!--&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;${elasticsearch-version}&lt;/version&gt;&lt;/dependency&gt;--&gt; 注意: transport中依赖了elasticsearch, 但默认是2.4.6版本, 需要指定下elasticsearch的版本5.5.2 也可以直接引入: 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt;&lt;/dependency&gt; 但是spring-boot-starter-data-elasticsearch只支持到2.4.x版本的es. 如果使用5.x.x版本ES, 就用上面那种方式单独引入ES依赖. ​ 添加配置类 12345678910111213141516171819202122232425@Configurationpublic class ElasticSearchConfig { /** 集群host */ @Value(\"${spring.data.elasticsearch.cluster-nodes}\") private String clusterNodes; /** 集群名称 */ @Value(\"${spring.data.elasticsearch.cluster-name}\") private String clusterName; @Bean public TransportClient client() throws UnknownHostException{ InetSocketTransportAddress node = new InetSocketTransportAddress( InetAddress.getByName(clusterNodes), 9300 ); Settings settings = Settings.builder().put(\"cluster.name\", clusterName).build(); TransportClient client = new PreBuiltTransportClient(settings); client.addTransportAddress(node); return client; }} application.properties中配置: spring.data.elasticsearch.cluster-nodes=xxx spring.data.elasticsearch.cluster-name=xxx 测试用例简单的CRUL操作: github: https://github.com/thank037/elasticsearch_demo.git @Link com.thank.elasticsearch.TestElasticSearchCRUD.java","link":"/2018/01/01/Elasticsearch初探/"},{"title":"Git初探","text":"[TOC] 前言早就听说了GitHub的强大. 一直没有机会去看, 在公司实习的几个月里也没机会接触SVN和Git ] 抱着对Linus大神的崇敬, 和开源的崇敬之情. 趁着不忙的几天来学习一下Git. 希望以后能够用到. 其实Git还是十分好学. 用不了多久, 你就能体会到它的高效简洁之美! 这里我是在本地虚拟机Centos来学习. . . 只是学习他的简单原理和操作, 并没有真正的尝试项目. 同时借鉴了网上一些有经验的前辈们的理解.. 来自己操作和学习 很多地方简单的例子我都有做图解,,做PPT真的好难阿!!! 并且花了两天时间来学习并整理这个小的文档. 希望能有所收获! 1. 介绍Git一款免费, 开源, 高效的分布式版本控制系统 https://git-scm.com 官网下载 支持命令行和GUI https://gitref.org 官方的帮助文档. SVN和Git的主要区别: SVN是集中式版本控制系统，版本库是集中放在中央服务器的，而干活的时候，用的都是自己的电脑 所以首先要从中央服务器哪里得到最新的版本，然后干活，干完后，需要把自己做完的活推送到中央服务器。 Git是分布式版本控制系统，那么它就没有中央服务器的，每个人的电脑就是一个完整的版本库，这样，工作的时候就不需要联网了，因为版本都是在自己的电脑上。 既然每个人的电脑都有一个完整的版本库，那多个人如何协作呢？比如说自己在电脑上改了文件A，其他人也在电脑上改了文件A， 这时，你们两之间只需把各自的修改推送给对方，就可以互相看到对方的修改了。 2. 全局配置git 安装 : sudo apt-get install git 查看版本: git –version 全局配置: 123&gt; git config --global user.name xxx&gt; git config --global user.email xxx&gt; git config --global color.ui true 配置用户名和邮箱是为了提交代码时的团队标识 git config –list 查看git全局配置 cat ~/.gitconfig 全局配置都保存在用户目录下这个.gitconfig隐藏文件中, 也可以vi编辑 可以通过以下命令获取git帮助 git help 可以通过以下命令获取特定某个指定的的帮助 git help 特定指令 可以查看仓库提交的历史记录 git log 3. 创建Repository初始化一个新的Git仓库 git init : 初始化git目录环境(创建出新的空的repository) 并生成隐藏文件夹.git .git(Git仓库文件, 所有相关的数据都保存在这儿) 12&gt; $ ls -A 查看到隐藏文件.git&gt; $ cd .git/ 另一种方式: 1git clone https://github.com/kennethreitz/requests.git 克隆网上的git项目,会自动创建repository 4. Git的对象类型Git中有四种基本对象类型，组成了Git更高级的数据结构： ● Blobs每个blob代表一个（版本的）文件，blob只包含文件的数据，而忽略文件的其他元数据，如名字、路径、格式等。 ● trees 每个tree代表了一个目录的信息，包含了此目录下的blobs，子目录（对应于子trees），文件名、路径等元数据。因此，对于有子目录的目录，git相当于存储了嵌套的trees。 ● commits 每个commit记录了提交一个更新的所有元数据，如指向的tree，父commit，作者、提交者、提交日期、提交日志等。每次提交都指向一个tree对象，记录了当次提交时的目录信息。一个commit可以有多个（至少一个）父commits。 ● tags tag用于给某个上述类型的对象指配一个便于开发者记忆的名字, 通常用于某次commit。 5. 提交及添加文件 git status –查看git repository的状态 Untracked files: 没有登记的文件 a).添加 git add hello.py命令进行添加 b).提交 git commit -m ‘init commit’ (不加-m会默认打开vi, ‘ ’里面是操作的描述,这里的init commit表示首次提交,) 可以直接提交到仓库(不暂存) git commit -a -m (或-am) “modify hello.txt” 注意: 1.其实不是在暂存区不存, 而是帮我们跳过(git add暂存区)这一步骤. 2.他不会自动提交 未追踪文件, 也就是新文件没有add过是不行的. 关于这三个文件状态及工作区域: ● History 也可以说 Git repository(git仓库): 最终确定的文件能保存到仓库, 成为一个新的版本, 并且对他人可见 ● Staging area (暂存区,Cache, Index): 暂存已经修改的文件 ● Working directory(工作区): 编辑, 修改文件 6. 查看git状态 git status -s 列出简要状态信息 文件前面有两个flag. 第一个是Staging area, 第二个是Working directory 再次vi hello.py后查看状态: 出现两个M, 第一个M表示Staging area里的文件发生变化, 第二个M表示Working directory发生了变化. 两个区完全一致, 标志位就是空白, 有发生变化导致不一致, 标志位就是M 再次进行commit, 三个区完全一致. 两个标志位都为空 7. 查看文件差别 git diff 是查看第二个flag(也就是Staging area和Working directory) 具体变化信息的命令 在modify操作后可以用: git diff –staged 可以查看第一个flag(也就是Staging arae和History 之间的变化), 产生相同的效果 git diff HEAD 可以看History 和 Working 之间的变化 git diff –stat 后面加stat可以简化变化信息 8. 撤销误操作 9. 移除及重命名文件删除文件 1.系统级别删除 rm filename 2.从git中删除文件 git rm filename 3.提交操作 git commit -m “delete filename” 注意: 只是删除当前版本中的文件, 文件依然被记录在Git仓库历史中 git rm –cached filename 删除Staking arae中的文件, 文件还存在Working directory中, 可以通过git add 或 git reset 恢复 重命名文件 git mv xxx new xxx git commit -m “rename xxx” 重命名相当于执行了以下三条命令: mv xxx newxxx (先系统级别上改名mv) git rm xxx git add newxxx 10. 暂存工作区 假如出现这种情况(MM), 工作区修改完(修改1)后add到暂存区, 还未commit到仓库中, 此时发现一个遗漏或紧急情况, 需要在工作区中在修改1之前做修改(修改2), 但是又不想放弃修改1, 此时可以暂存工作区后进行修改. git stash 暂存工作区 此时就回到了修改一之前的现场, 此时可以做自己的代码修复vi git commit -am ‘quick fix’ 修改完后就可以提交这个紧急修复 git stash list 查看暂存工作区的列表 可以看到一个 git stash pop 弹出这个现场 然后在进行add和commit后, 修改1和修改2的作用就同时生效了. 11. 图解commit对象为了理解HEAD,commit,和commit下对象之间的结构关系, 我们用图来理解: 下面实际来看一下 git log 后, 出现最近的操作日志如下: 每个commit后面都有一个哈希码来唯一标识这个commit. HEAD默认指向第一个commit git cat-file -t : show object type git cat-file -p: print object’s content 当然, 用commit 前的哈希值也可以看到这个对象. 这里就可以看到前面commit图解中的信息 找到这个commit对象了, 我们再来看看这个对象中的tree里面有什么. 也可以继续追踪下去. 这样你就可以自由的找到commit对象里面的所有内容. 12. 理解tree-ish表达式在.git文件中有一个HEAD cat HEADref: refs/heads/master 我们查看这儿 cat refs/heads/master git log –oneline 可以看到, 这个master分支就指向的是最新的commit git rev-parse HEAD 可以看到, 这里HEAD和master的最终指向是一致的, 都是commit. git show 和 git cat -file -p类似, 能更简便的看到content 如果要定位到commit下的tree. 可以用tree-ish表达式 HEAD ~3^{tree} 如果要定位到tree下的hello.py, 不用先定位到tree. 也可以用表达式 HEAD~3: tree-ish表达式的引入, 让我们更方便的定位到任意一个对象和文件 13. 创建及删除分支 查看分支: git branch – master 新建一个分支: git branch newBranchgit branch– masternewBranch 切换分支: git checkout newBranch创建分支和切换分支可以合并一步进行:git checkout -b newBranch 删除分支 git branch -d newBranch 注意: 切换分支实际就是把head指向了不同的branch cd heads目录, 就会发现有两个分支,master和newBranch 他们都指向一个commit 14. 合并分支(上)这里我们介绍合并分支的做法, 和两种合并机制 我们在新创建的分支newBranch中进行代码的升级(hello.py)提交后, 再删除newBranch, 会出现一个error. 这是因为newBranch原本指向的呢个新commit, 删除后, 这个新commit就没有被指向了. 如图: 所以应该需要在删除newBranch之前让master指向新commit. 也就是进行分支合并, 合并后的效果如图: 使用git merge newBranch 然后就可以删除newBranch了 这种合并机制叫Fast-forword 15. 合并分支(下)如果出现这种情况: 在两个分支都出现了代码的修改 两个分支master和bugFix 在bugFix分支上做了两次fix (fix1和fix2) 并提交 然后切换到master分支, 做一次Modify. 进行git merge bugFix后, 两个分支的操作都被合并了 但是此时的合并方式就不是Fast-forwrd那么简单了. 而是另一种机制3-way merge 它的合并原理如下: 先找到两个分支的共同分支对象 将fix code2和B进行比对, 形成对比信息. 然后将对比信息添加应用到Modify code上. 形成新的对象NEW. master重新指向NEW. 分支合并完毕! 16. Git远程仓库远程仓库的实现: 1 使用现有的Git网络仓库服务 Github: https://github.com (创建开源免费哦) Bitbucket: https://bitbucket.org 2 搭建自己的Git仓库服务器","link":"/2016/03/02/Git初探/"},{"title":"Hexo+GitHub搭建独立博客","text":"[TOC] 前言 之前, 一直看CSDN上写别人的博客, 博客园, CSDN上都有很丰富的资料. 自己在CSDN上记录学习文章过程中, 发现了一些不太喜欢的地方. 比如有时候再嵌入代码和图片时候兼容问题(可能是自己的错误) , 会有广告, 缺乏个性. 所以就想搭建一个独立的博客. 在互联网上搜集一些资料, 参考了很多别人的个人博客. 最终选择Hexo+GitHub Pages 整体难度不是很大, 但是过程还是比较繁琐的, 需要懂点Git, 很多是需要命令中的配置. 如果你有强迫症, 想要把博客定制的比较符合心意, 就更需要读更多文档来做个性化配置. 因为网上有很多文档. 包括官方文档. 其实完全够用. 本文简短描述过程. 不做累赘的配置描述. 推荐两篇比较好的博文: 如何搭建一个独立博客——简明Github Pages与Hexo教程 一步步在GitHub上创建博客主页(7个系列) 介绍Hexo关于Hexo, 文档很详细了: https://hexo.io/zh-cn/docs/ GitHub Pages我有自己的阿里云ECS, 但是不想放在上面, 因为是借同学在校证明买的优惠机, 他毕业机器就没了… 好在看到GitHub Pages github Pages 是 Github 的静态页面托管服务。它设计的初衷是为了用户能够直接通过 Github 仓库来托管用户个人、组织或是项目的专属页面。 可以帮助我们通过Git来管理的静态服务器.相比wordpress, 它的搭建部署较为复杂, 需要安装一些软件. 也会由于粗心容易出现一些bug需要解决. 比较适合于爱折腾的coder. 说明: 仓库存储的所有文件不能超过 1 GB 页面的带宽限制是低于每月 100 GB 或是每月 100,000 次请求 每小时最多只能部署 10 个静态网站 github pages有300M免费空间 依托github, 上面有很多大神. 拥抱开源, 分享知识 在搭建之前, 需要懂一点Git相关知识, 关于Git笔记. 可以参考 个人博客: git小玩 CSDN: git小玩 安装这俩是必须的 Git: http://git-scm.com/ Node.js: http://nodejs.org/ 软件较小, 近乎傻瓜式安装. 注册GitHub搭建博客之前, 需要先到github上做一些事情. 进入 http://www.github.com/ sign up for GitHub 即可. 牢记自己注册信息, 尤其email. 需要到邮箱自己验证一下. 登录之后 点击头像下的Settings选项. 标注的几个就是需要自己设置的, 比如SSH和仓库. 配置SSH keys前面我们下载好了git, 而这个秘钥, 就是让本地和远程GitHub建立联系. 打开前面下载好的GitBash, 或者右击某个目录下的git bash here.输入 1$ cd ~/. ssh #检查本机的ssh秘钥 提示No Such.. 需要我们自己配置. 1$ ssh-keygen -t rsa -C #“你注册的email” 提示: 12Generating public/private rsa key pair.Enter file in which to save the key (/Users/your_user_directory/.ssh/id_rsa): 这里会在你本机生成一个rsa的加密秘钥. 点击回车 接着会提示 1Enter passphrase (empty for no passphrase) 让你输入一个密码, 你可以输入也可以输入空, 直接回车, 这个密码是以后提交项目用的. 提示: Enter same passphrase again: 再次输入 当提示your identification has been saved in …[目录] 和一段随机蚂图形后. 表示已经生成成功. 这时候去你生成的那个目录找id_rsa.pub文件, 这个目录跟操作系统有关, 有的在Documents and Settings, win7可能找不到这个文件夹. 或者被隐藏, 需要设置一下再查找文件. 我的是在C:\\Users\\lenovo.ssh下. 找到这个文件, 把里面的内容复制出来. 到github页面的settings下, 找到SSH keys and GPG keys. 如图所示, 复制到key框中. 添加成功后, 在git bash中输入: $ ssh -T git@github.com 提示: The authenticity of host ‘github.com (207.97.227.239)’ can’t be established.RSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48.Are you sure you want to continue connecting (yes/no)? 输入yes, 接着提示: Hi thank037! You’ve successfully authenticated, but GitHub does not provide shell access. 表示已经连接成功了! 这里继续输入一下以后提交的用户信息(填自己的): git config –global user.name “thank037”git config –global user.email “coderthank@163.com” 建立仓库(repositories)如图所示: 这里的repositories name 需要是你的 用户名+github.io, 如果要用GitHub Pages建立个人博客, 格式必须这样, 不能乱写. 描述自己随便写. 个人博客的仓库只能建立一个. 然后点击 Crete Repositories , 你的个人博客仓库就建立好了. 以后所有的博客内容, 包括模板, 文章等 都会存放在这里. 部署Hexo 搭建工具Hexo是一个简单, 轻便的静态博客框架. 它基于node, 使用之前可以检查下node环境: node -v 然后输入: 1$ npm install -g hexo # 开始安装…. 会在你的用户全局目录下安装. 等待安装完毕后, 随便在目录中创建一个叫”Hexo”的文件夹. 将git bash 切换到这个目录下, 输入: 1$ hexo init # 会在Hexo文件夹中初始化网站需要的所有文件. 安装好的目录结构如图: 这时候, 点击themes中可以看到一个landscape, 它是默认的主题. 你也可启动服务看一下. 选主题挑选自己喜欢的主题, 可以参见知乎这里的主题推荐: http://www.zhihu.com/question/24422335 我选择的是NextT 精于心，简于形. 主题以黑白两种永恒经典的色调搭配, 四周做了大量留白. 看起来就是 简单! 简单!! 简单!!! 进去之后点击Clone or Download, 把它的URL复制下来. 1$ git clone https://github.com/iissnan/hexo-theme-next.git thems/hexo-theme-next 最后一个参数是: 主题/主题建立的名字 克隆完毕后. 打开Hexo目录根下的_config.yml文件(可以把它称作主站的配置文件) 修改它的themes属性为你的主题名字: 1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: hexo-theme-next 在git clone之后, 可以用git pull命令更新一下. 完毕后, 进入Hexo/themes/主题 目录下, 它的结构是这样的:这里也有一个叫_config.yml的文件, 可以把它称作主题配置文件. 本地预览在git bash中输入: 123456hexo g # 生成hexo s # 启动服务# 启动后INFO Start processingINFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. 浏览器中输入http://localhost:4000/ 就可以看到你的博客了. 到此, 这个博客已经搭建完毕. 之后就需要花些时间来研究这个主题. 以及发挥你的想象力来自己定制这个主题. 主题的设置很多很多, 懒得写了, 我也只是配置了一部分. 所以最好的就是参考NextT的官方文档, 这个文档简单明了. http://theme-next.iissnan.com/getting-started.html 可以对主题和功能强大的第三方插件进行配置. 发表文章主题配置完啦, 你也满意了. 接下来的就是日常操作, 例如写文章, 生成, 部署等等… 也懒得写了, 常用命令总结如下: hexo new page “pageName” : 新建页面 hexo new “blogName” : 简写 hexo n #新建文章 hexo p : 简写 hexo publish hexo server : 简写 hexo s #启动服务 hexo generate : 简写 hexo g #生成静态网页 hexo deploy : 简写 hexo d #部署 hexo命令详细的可以参考: https://segmentfault.com/a/1190000002632530 页面在新建时默认是.md格式, 也就是markdown格式的文档. 也是我非常喜欢的书写方式. 在这里, 你只需要关注内容, 而不用过分关心格式. 只需要花上几分钟, 就可熟练掌握它基本的书写语法. 如果你觉得它的局限太多, 很难出来你想要的格式, 它也是支持html的, 你可以在里面加入html标签来做出你想要的格式. 关于markdown可以参考: 认识 Markdown 问题记录在hexo deploy时, 可能出现一个错误: ERROR Deployer not found: git做这两步即可消除: 输入: npm install hexo-deployer-git –save 在_config.yml文件中将deploy: type: github修改为deploy:type: git 个性化域名当在本地预览也调试好之后, 通过hexo deploy部署到了远程仓库, 就可以通过例如: thank037.github.io来访问你的博客. 如果你想要一个域名, 直接访问这个域名来访问你的个人博客, 就需要购买和设置一个域名. 国内和国外网站都可以购买域名. 个有好坏. 我是从godaddy 上购买的. 虽然是国外的, 但是已经支持支付宝. 大致步骤就是: 搜索你想要的域名 选择一些购买项 填写个人信息 验证邮箱 购买完毕后登录godaddy, 进入my product页面. 可以看到你购买的域名信息. 点击DNS管理 对A记录, CNAME做基本的设置就可以了: 对于A记录, github pages目前是这两个(如果变了, 就要做更换): 192.30.252.153192.30.252.154 下一步: 在你github上的博客仓库里面, 新建一个File, 取名CNAME, 里面只编辑一行(就是你购买的域名): www.xxx.com 然后访问这个域名, 就能看到了. 至此, 个人独立博客搭建完毕. 因为是在github上, 访问的速度当然不如CSDN上的. 不过现在已经支持DNS配置访问选择距离你最近的服务器. 速度已经有所提升.","link":"/2016/09/14/Hexo+GitHub搭建独立博客/"},{"title":"Linux远程无密传输实现脚本","text":"序言 作为一名后台开发, 开发过程中, 除了要部署后台的微服务, 还要对前端的WEB目录进行部署.虽然每次只需要2分钟左右, 但却是重复性劳动. 我认为很xx! 听说过俄罗斯一程序员, 认为90秒以上的事情, 就必须要做成脚本, 比如煮咖啡, 骗老婆说要加班等等… 所以我决定花几个小时时间来把这种事情写成脚本. 目前公司的前端WEB目录部署其实就是文件的上传下载, 不需要启动什么的. 既然前端人员不想用Linux也不想用FTP. 那… 在linux下编写成脚本后, 让前端人员直接在windows下通过一个按钮点击, 就完成WEB目录的一键部署. 我就完全可以脱身这件事. 场景描述首先预上线环境部署WEB目录的流程: 开发环境: 213 测试环境: 212 预上线环境: 110 前两个环境是我来手动维护, 因为写了很多脚本(自动Git拉取, Maven打包, 一键杀死/启动微服务等…) 所以维护起来很轻松. 110环境是通过Jenkins来构建的, 每次构建完成都会生成全新的虚拟机镜像 服务器, 数据库, Redis都会是新的(IP不变) 所以WEB目录的部署, 是在212上拉取测试通过的目录到预上线的WEB目录. 但是经常会有bug修正, 每次修正完又要重新拉一遍. 所以阿. 看起来这个脚本应该很简单. 就是copy主机A的WEB目录到主机B 建立通信但是涉及到两台机器通信, 就会有密码. 总不能执行脚本之后再来输入密码. 有以下两种方式: ssh免密:建立互信关系 通过expect自动输入 方式一: ssh关于什么是ssh, 公钥, 密钥 原理什么的可以Google 简单介绍下操作方法: 假设现在有 A(192.168.100.212) B(192.168.100.213) 两台机Linux(CentOS7), 要实现从A用ssh远程登录到B: 1. 首先在A上生成公钥 1ssh-keygen -t rsa 这里一直点回车即可.执行完成后, 在~/.ssh/下就会生成id_rsa和id_rsa.pub两个文件 2. 把A机器刚才生成的id_rsa.pub文件复制到B上. 1scp ~/.ssh/id_rsa.pub root@192.168.100.213:/root/ 然后在B的root目录下就能看到id_rsa.pub了(这里放root下只是临时放一下, 你放哪都行) 3. 在B机器中, 把id_rsa.pub中的内容加到authorized_keys文件 1cat id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 如果~/下没有.ssh或者.ssh下没有authorized_keys, 就自己手动创建 4. 在B上重启sshd服务 1service ssh restart 完了, 现在在A上, 试试登陆B: ssh root@192.168.100.213. 应该就不输入密码了 注意事项： ssh-keygen -t rsa 生成公钥时一直回车不要输入密码，就是空密码 要用哪个用户远程登录就把id_rsa.pub复制到用户对应路径下，如：root用户就复制到/root/下; xxx用户就复制到/home/xxx/下，不要混了 B机上的文件权限： .ssh文件夹(700)；authorized_keys(600) 方式二: expect在我写这个脚本时候, 并未使用上面那种方法. 原因是上面提到过, 每次预上线环境构建完都会是一台全新虚拟机(只有IP不变) 所以不可能每次构建完都去配一次ssh信任关系. 所以介绍一种更优雅的方式, 在脚本里自动输入密码(当屏幕交互需要输入时自动输入) 这里用到的就是expect. 要使用首先要安装: 1sudo yum install expect 建立scp.exp 12345678910111213141516171819202122232425262728293031323334353637#!/usr/bin/expect set timeout 20 if { [llength $argv] &lt; 2} { puts \"Usage:\" puts \"$argv0 local_file remote_path\" exit 1 } set local_file [lindex $argv 0] set remote_path [lindex $argv 1] set passwd your_passwd set passwderror 0 spawn scp $local_file $remote_path expect { \"*assword:*\" { if { $passwderror == 1 } { puts \"passwd is error\" exit 2 } set timeout 1000 set passwderror 1 send \"$passwd\\r\" exp_continue } \"*es/no)?*\" { send \"yes\\r\" exp_continue } timeout { puts \"connect is timeout\" exit 3 } } 这里我找到一个比较通用的(除了输入自动输入密码之外, 还支持yes/no的自动选择) 可以看到第一行并不是#!/bin/bash. 所以这段脚本不是由bash解释执行, 而是expect. 代码的语法和shell也不一样. 1234set local_file [lindex $argv 0] # 本地要拷贝的文件set remote_path [lindex $argv 1] # 远程拷贝文件的目标地址set passwd your_passwd #your_passwd就是你要连接远程机器的密码spawn scp $local_file $remote_path # 这个就是实际的拷贝, 如果要复制目录下的文件, 可以加-r以递归方式复制 例如我要把本机(213)的某个文件复制到主机110上 1./scp.exp /hahah/hello.py root@192.168.120.110:/xx/yy/ scp目录的坑对于上述第二种方式: scp的目录问题, 有个坑记录一下 例如: /scp.exp /hahah/hello.py root@192.168.120.110:/xx/yy/: 是把hello.py文件拷贝到yy目录下, 没问题. ./scp.exp /hahah/dir1 root@192.168.120.110:/xx/yy/: 是把dir1目录拷贝到yy目录下, 也没问题 ./scp.exp /hahah/dir1/ *root@192.168.120.110:/xx/yy/: 按理说应该是把dir1目录下的文件(不包括dir1目录本身)拷贝到yy目录下. 这个直接在scp命令下是没问题的. 但是因为这里执行的是scp.exp脚本. 他会直接把*当成特殊字符, 所以需要”\\”来转义, 例如: 1./scp.exp /hahah/dir1/* root@192.168.120.110:/xx/yy/ 方便调用scp.exp两种通信方式就介绍完了. 因为我要通过一个请求去调用scp脚本. 为了方便, 又写了一个脚本去调用scp.exp update_pre_online.sh内容如下: 123456789101112131415161718192021222324252627282930313233#!/bin/bashecho \"Content-Type: text/html\"echo \"\"# Source pathSRC_PATH=/home/www/update_pre_online/xxws-web-120/# Target path and Ip address#TAG_PATH=/home/xiefy/dir/#TAG_ADDRESS=192.168.100.214TAG_PATH=/sysroot/tmp/xxws/TAG_ADDRESS=192.168.120.110# Pull 212WEB to 213WEBscp -r root@192.168.100.212:/home/www/xxws-web/* ${SRC_PATH}chmod -R 755 ${SRC_PATH}echo \"============================\"echo \"递归修改目录访问权限(755)...\"echo \"============================\"sleep 1# execute expect, pull to 110WEB/home/www/update_pre_online/scp.exp ${SRC_PATH}\\* root@${TAG_ADDRESS}:${TAG_PATH}cat ~/update_pre_online.logecho \"--------------------------------------\"echo \" 更新 212WEB目录 到 110WEB目录 成功! \"#echo \" 禁止更新! 新的预上线后台微服务未更新 \"echo \"--------------------------------------\"sleep 2exit 0 总结完成后, 加一个小页面按钮, 效果如下: 虽然之前部署这个熟练的话每次只需要三分钟. 但我编写脚本加测试就用了好几个小时. 成果就是现在每次只需要点击按钮后等待5秒即可, 最重要的是!!! 不需要我来操作了. The End","link":"/2017/05/15/Linux远程无密传输实现脚本/"},{"title":"IDEA使用","text":"前言 用了一段时间IDEA, 不想再用Eclipse了, 记录一些快捷键和好用插件 [TOC] 快捷键切换功能Ctrl + PgUp/PgDn: 切换文件Ctrl + Alt + [/]: 切换项目Ctrl + Shift + 向上箭头/向下箭头: 切换方法(上一个/下一个)Alt + 数字: 切换窗口, IDE窗口每个都带有数字标号 查找功能Ctrl + Shift + A: 查找选项Ctrl + Shift + R: 查找文件 (如果要包括本项目外的 按2下R)Ctrl + Shift + T: 查找class (如果要包括本项目外的 按2下T)Ctrl + H: 全文查找指定字符串:Ctrl + O `: 方法列表 其他Ctrl + Alt + S: 设置Ctrl + Alt + Shilt + S: 项目设置(包括project, module…) Shift + ESC: 关闭IDEA中的当前窗口ESC: 从其他窗口跳回代码编辑区 Ctrl + Shift + U: 大小写转换Alt + Enter 万能建议:Alt + Insert: 生成(例如setter/getter, 构造方法, Override方法等) 自定义的Ctrl + Q: 跳转到上一个修改的地方Alt + Q: 跳转到下一个修改的地方F10: 添加到喜爱(方法和类都能添加)F11/F12: 添加代码行到书签Alt + 2: 调出Favourites窗口, 里面有书签, 喜爱, 断点Alt + J: AceJump Word 好用插件IDEA旗舰版本身就已经很多插件了, 不想安装太多, 但以下几个很喜欢 Alibaba Java Coding Guidelines: 阿里巴巴代码检查插件 EJS: 我的博客是node js, 需要打开ejs文件的插件 emacsIDEAs: emacs神器, 不用多说, 脱离鼠标的利器 Grep Console: 控制台日志根据级别变色输出, 很好看, 很实用 Lombok Plugin: IDEA中要使用Lombok, 需要安装这个 Maven Helper: 查看Maven依赖很方便 Rainbow Brackets: 代码的选中高亮, 很好看很有趣 Translation: 翻译插件, 很强大","link":"/2017/12/15/IDEA使用/"},{"title":"PowerDesigner使用","text":"转自: http://www.cnblogs.com/langtianya/archive/2013/03/08/2949118.htmlPowerDesigner是一款功能非常强大的建模工具软件，足以与Rose比肩，同样是当今最著名的建模软件之一。Rose是专攻UML对象模型的建模工具，之后才向数据库建模发展，而PowerDesigner则与其正好相反，它是以数据库建模起家，后来才发展为一款综合全面的Case工具。","link":"/2015/04/15/PowerDesigner的简单使用/"},{"title":"Oracle错误排除","text":"这两天使用Oracle遇到很多问题, 大都容易解决 问题这个错误是: 1ERROR: ORA-01034:ORACLE not available ORA-27101:shared memory realm does not exit 排查步骤排查步骤如下: 先看oracle的监听和oracle的服务是否都启动了。启动oracle监听：cmd的命令行窗口下，输入：lsnrctl start，回车即启动监听 查看oracle的sid叫什么，比如创建数据库的时候，实例名叫“abc”，那么先手工设置一下oralce的sid cmd命令窗口中: set ORACLE_SID=abc 再输入sqlplus /nolog 回车 再输入conn / as sysdba 回车 再输入startup ，回车。 这步是启动oracle服务。如果startup启动被告知已经启动了，可以先输入shutdown immediate；等shutdown结束之后，再输入startup。 过几秒钟等命令运行完成，就能连接了。这个时候，可以输入select count(*) from user_tables; 测试一下，看是否有查询结果。 总结出现ORA-01034和ORA-27101的原因是多方面的：主要是oracle当前的服务不可用，shared memory realm does not exist，是因为oracle没有启动或没有正常启动，共享内存并没有分配给当前实例.所以，通过设置实例名，再用操作系统身份验证的方式，启动数据库。这样数据库就正常启动了，就不会报ORA-01034和ORA-27101两个启动异常了。","link":"/2013/12/22/Oracle错误/"},{"title":"Python写网络爬虫(一)","text":"[TOC] 关于Python学过C. 学过C++. 最后还是学Java来吃饭. 一直在Java的小世界里混迹. 有句话说: “Life is short, you need Python!” 翻译过来就是: 人生苦短, 我用Python 究竟它有多么强大, 多么简洁? 抱着好奇心, 趁在公司不忙的几天. 还是忍不住的小学了一下.(- - 其实学了还不到两天) 随便用一个”HelloWorld”的例子 1234567//Javaclass Main{ public static void main(String[] args){ String str = \"HelloWorld!\"; System.out.println(str); }} 123# pythonstr = 'HelloWorld'print str 乍一看Python确实很爽! 简明了断 节省时间 至于效率. 我相信不管是任何语言, 作为开发者重要是的还是思维的精简! 也有人说: “Python一时爽, 重构火葬场” 但是Python的应用场景那么多. 根据自己的需要来选择, 相信那么多前辈没错. Python的确是一门值得学习的课程. 关于网络爬虫这个东西我也也不懂… 随便抓个解释, 淡淡的理解下 网络爬虫（又被称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。 另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。 真正的复杂而强大的爬虫有很多爬行算法和策略. 我写的这个例子简直是简之又简. Python基础还没学完, 就迫不及待的想做个东西来看看 于是就想到了写个网络爬虫. 来小小的练习一下 我会经常看母校的学校新闻, 于是就试着爬学校新闻, 没事的时候拿出来看看 因为学的甚少…这个爬虫的功能还是非常简单. 就是把学校官网中最新的新闻下载下来, 保存成网页. 想看多少页都可以. 这里我就抓了最新的前4页新闻 第一次写爬虫. 一个很简单的功能, 我把它分了3步: 第一步: 先爬一条新闻, 把它下载保存! 第二步: 把这一页的所有新闻都爬下来, 下载保存! 第三步: 把x页的所有新闻爬下来, 下载保存! 网页爬虫很重要的一点就是分析网页元素. 一: 爬一个url先爬一条新闻, 把它下载保存! 123456789101112131415161718192021222324252627282930#crawler: 甘肃农业大学新闻网板块的-&gt;学校新闻.#爬这个页面的第一篇新闻#http://news.gsau.edu.cn/tzgg1/xxxw33.htm#coding:utf-8import urllibstr = '&lt;a class=\"c43092\" href=\"../info/1037/30577.htm\" target=\"_blank\" title=\"双联行动水泉乡克那村工作组赴联系村开展精准扶贫相关工作\"&gt;双联行动水泉乡克那村工作组赴联系村开展精准扶贫相关工作&lt;/a&gt;'hrefBeg = str.find('href=')hrefEnd = str.find('.htm', hrefBeg)href = str[hrefBeg+6: hrefEnd+4]print hrefhref = href[3:]print hreftitleBeg = str.find(r'title=')titleEnd = str.find(r'&gt;', titleBeg)title = str[titleBeg+7: titleEnd-1]print titleurl = 'http://news.gsau.edu.cn/' + href print 'url: '+urlcontent = urllib.urlopen(url).read()#print contentfilename = title + '.html'#将抓取的页面content写入filename保存本地目录中open(filename, 'w').write(content) 这个里面不用分析太多网页元素. 就字符串拼阿截阿就好了. 二: 爬取一个页面的新闻爬取本页面的所有新闻, 每页23篇 这个时候就要小小的分析下, 这23个url, 每个url怎么找? 这里可以先锁定一个元素, 进行查找. 再就是注意每次find时的规律, 其实就是查找的顺序起始 这里我把每个url都保存在一个数组中, 检索完成后, 对数组里的url进行下载. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#crawler甘肃农业大学新闻网板块的-&gt;学校新闻.#http://news.gsau.edu.cn/tzgg1/xxxw33.htm#&lt;a class=\"c43092\" href=\"../info/1037/30567.htm\" target=\"_blank\" title=\"双联行动水泉乡克那村工作组赴联系村开展精准扶贫相关工作\"&gt;双联行动水泉乡克那村工作组赴联系村开展精准扶贫相关工作&lt;/a&gt;#coding:utf-8import urllibimport timeimport stat, ospageSize = 23articleList = urllib.urlopen('http://news.gsau.edu.cn/tzgg1/xxxw33.htm').read()urlList = [' ']*pageSize#锁定class=\"c43092\"hrefClass = articleList.find('class=\"c43092\"')hrefBeg = articleList.find(r'href=', hrefClass)hrefEnd = articleList.find(r'.htm', hrefBeg)href=articleList[hrefBeg+6: hrefEnd+4][3:]print href#url = 'http://news.gsau.edu.cn/' + href #print 'url: '+urli = 0while href!=-1 and i&lt;pageSize: urlList[i] = 'http://news.gsau.edu.cn/' + href hrefClass = articleList.find('class=\"c43092\"', hrefEnd) hrefBeg = articleList.find(r'href=', hrefClass) hrefEnd = articleList.find(r'.htm', hrefBeg) href=articleList[hrefBeg+6: hrefEnd+4][3:] print urlList[i] i = i+1else: print r'本页所有URL已爬完!!!'#将本页每一篇新闻下载到本地(已新闻标题文件名存储)#title: &lt;HTML&gt;&lt;HEAD&gt;&lt;TITLE&gt;酒泉市市长都伟来校对接校地合作事宜-新闻网&lt;/TITLE&gt;j = 0while j&lt;pageSize: content = urllib.urlopen(urlList[j]).read() titleBeg = content.find(r'&lt;TITLE&gt;') titleEnd = content.find(r'&lt;/TITLE&gt;', titleBeg) title = content[titleBeg+7: titleEnd] print title print urlList[j]+r'正在下载...' time.sleep(1) open(r'GsauNews' + os.path.sep + title.decode('utf-8').encode('gbk')+'.html', 'w+').write(content) j = j + 1else: print r'当页全部url下载完毕!' 三: 爬取所有新闻爬取N个页面的所有新闻 这里要爬取N个页面, 首先就要分析你要爬取的是最新的, 而不能是固定的某几页 所以要分析下分页的数据, 正好主页最下面也给出了分页数据, 直接用它! 看下最近几页的url: 1234http://news.gsau.edu.cn/tzgg1/xxxw33.htm 第一页http://news.gsau.edu.cn/tzgg1/xxxw33/221.htm 第二页http://news.gsau.edu.cn/tzgg1/xxxw33/220.htm 第三页http://news.gsau.edu.cn/tzgg1/xxxw33/219.htm 第四页 对比分页数据, 很容易发现规律, 就是: fenyeCount-pageNo+1 这里很烦的一点是不知道为什么, 除了第一页以外的其他页, 都会有不是本页而是前一页的一部分网页数据会掺进来. 导致我找了半天 做了很多判断才检索出来 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#crawler甘肃农业大学新闻网板块的-&gt;学校新闻.#coding:utf-8import urllibimport timeimport stat, ospageCount = 4pageSize = 23pageNo = 1urlList = [' ']*pageSize*pageCount#分析分页的网页元素#&lt;td width=\"1%\" align=\"left\" id=\"fanye43092\" nowrap=\"\"&gt;共5084条 1/222 &lt;/td&gt;indexContent = urllib.urlopen('http://news.gsau.edu.cn/tzgg1/xxxw33.htm').read()fenyeId = indexContent.find('id=\"fanye43092\"') #这里锁定分页的id进行查找fenyeBeg = indexContent.find('1/', fenyeId)fenyeEnd = indexContent.find(' ', fenyeBeg)fenyeCount = int(indexContent[fenyeBeg+2: fenyeEnd])i = 0while pageNo &lt;= pageCount: if pageNo==1: articleUrl = 'http://news.gsau.edu.cn/tzgg1/xxxw33.htm' else: articleUrl = 'http://news.gsau.edu.cn/tzgg1/xxxw33/'+ str(fenyeCount-pageNo+1) + '.htm' print r'--------共爬取'+ str(pageCount) + '页 当前第' + str(pageNo) + '页 URL:' + articleUrl articleList = urllib.urlopen(articleUrl).read() while i&lt;pageSize*pageNo: if pageNo == 1: #i = 0,23,46...时,从头找, 其余从上一个url结束位置开找 if i == pageSize*(pageNo-1): hrefId = articleList.find('id=\"line43092_0\"') else: hrefId = articleList.find('class=\"c43092\"', hrefEnd) else: if i == pageSize*(pageNo-1): hrefId = articleList.find('id=\"lineimg43092_16\"') else: hrefId = articleList.find('class=\"c43092\"', hrefEnd) hrefBeg = articleList.find(r'href=', hrefId) hrefEnd = articleList.find(r'.htm', hrefBeg) if pageNo == 1: href=articleList[hrefBeg+6: hrefEnd+4][3:] else: href=articleList[hrefBeg+6: hrefEnd+4][6:] urlList[i] = 'http://news.gsau.edu.cn/' + href print urlList[i] i = i+1 else: print r'========第'+str(pageNo)+'页url提取完成!!!' pageNo = pageNo + 1print r'============所有url提取完成!!!============'+'\\n'*3print r'==========开始下载到本地==========='j = 0while j &lt; pageCount * pageSize: content = urllib.urlopen(urlList[j]).read() titleBeg = content.find(r'&lt;TITLE&gt;') titleEnd = content.find(r'&lt;/TITLE&gt;', titleBeg) title = content[titleBeg+7: titleEnd] print title print urlList[j]+r'正在下载...'+'\\n' time.sleep(1) open(r'GsauNews' + os.path.sep + title.decode('utf-8').encode('gbk')+'.html', 'w+').write(content) j = j + 1else: print r'下载完成, 共下载'+str(pageCount)+'页, '+str(pageCount*pageSize)+'篇新闻' 这就爬完了…看下爬完的效果 1234567891011121314151617181920212223242526272829303132333435363738==================== RESTART: D:\\python\\CSDNCrawler03.py ====================——–共爬取4页 当前第1页 URL:http://news.gsau.edu.cn/tzgg1/xxxw33.htmhttp://news.gsau.edu.cn/info/1037/30596.htmhttp://news.gsau.edu.cn/info/1037/30595.htmhttp://news.gsau.edu.cn/info/1037/30593.htmhttp://news.gsau.edu.cn/info/1037/30591.htmhttp://news.gsau.edu.cn/info/1037/30584.htmhttp://news.gsau.edu.cn/info/1037/30583.htmhttp://news.gsau.edu.cn/info/1037/30580.htmhttp://news.gsau.edu.cn/info/1037/30577.htmhttp://news.gsau.edu.cn/info/1037/30574.htmhttp://news.gsau.edu.cn/info/1037/30573.htmhttp://news.gsau.edu.cn/info/1037/30571.htmhttp://news.gsau.edu.cn/info/1037/30569.htmhttp://news.gsau.edu.cn/info/1037/30567.htmhttp://news.gsau.edu.cn/info/1037/30566.htmhttp://news.gsau.edu.cn/info/1037/30565.htmhttp://news.gsau.edu.cn/info/1037/30559.htmhttp://news.gsau.edu.cn/info/1037/30558.htmhttp://news.gsau.edu.cn/info/1037/30557.htmhttp://news.gsau.edu.cn/info/1037/30555.htmhttp://news.gsau.edu.cn/info/1037/30554.htmhttp://news.gsau.edu.cn/info/1037/30546.htmhttp://news.gsau.edu.cn/info/1037/30542.htmhttp://news.gsau.edu.cn/info/1037/30540.htm========第1页url提取完成!!!——–共爬取4页 当前第2页 URL:http://news.gsau.edu.cn/tzgg1/xxxw33/221.htmhttp://news.gsau.edu.cn/info/1037/30536.htm...==========开始下载到本地===========校长吴建民带队赴广河县开展精准扶贫和双联工作-新闻网http://news.gsau.edu.cn/info/1037/30574.htm正在下载…动物医学院赴园子村开展精准扶贫相关工作-新闻网http://news.gsau.edu.cn/info/1037/30573.htm正在下载…………. 一分多钟爬了90个网页. 当然代码还可以优化好多. 例如用正则, 匹配自己想要的内容,而不是像这样泛泛全保存. 以后的学习中继续改进. 爬些更有意思的东西 做这个例子只是为了看看, 初学Python能为我做什么.","link":"/2016/03/31/Python写网络爬虫(一)/"},{"title":"SVN分支管理","text":"工作中遇到了, 简单记录下SVN中的分支管理 SVN的标准目录结构：欲了解详细，请参照：http://www.cnmiss.cn/?p=296 概念先来了解几个基本概念: Trunk(主干库)：存放核心项目 Branches(分支库)：存放为不同用户客订制化的版本、或阶段性的稳定 release 版本。这些版本是可以继续进行开发和维护的 Tag(基线库)：存档目录(不允许修改) Release(发布库)：存放历次发布内容 操作假设主干为: svn://192.168.100.212/xxws-web/trunk 新建分支branch112# xxwssvn cp svn://192.168.100.212/xxws-web/trunk svn://192.168.100.212/xxws-web/branch1 -m \"create branch1 version\" branch1下就会有trunk上的完整代码, 互不影响 删除分支branch112# xxwssvn rm svn://192.168.100.212/xxws-web/branch1 -m \"remove branch1\" 检出分支branch1来开发:1svn co svn://192.168.100.212/xxws-web/branch1 合并主干上的最新代码到分支上1234567# 进入分支目录cd branch1svn merge svn://192.168.100.212/xxws-web/trunk# 预览该刷新操作svn mergeinfo svn://192.168.100.212/xxws-web/trunk --show-revs eligible 分支合并到主干123# 进入主干目录cd trunksvn merge --reintegrate svn://192.168.100.212/xxws-web/branch1 分支合并到主干中完成后应当删该分支，因为在SVN中该分支已经不能进行刷新也不能合并到主干 ? ? ? 合并版本并将合并后的结果应用到现有的分支上1svn -r 148:149 merge http://svn_server/xxx_repository/trunk","link":"/2017/12/22/SVN分支管理/"},{"title":"Ribbon——与其它组件超时和重试","text":"[TOC] 前言在上篇源码分析——客户端负载Netflix Ribbon中提到了重试, 在Spring Cloud中各组件关于重试的概念还是很容易混淆 你会发现在Netflix OSS中: Eureka, Ribbon, Zuul, feign, etc… 虽然每个功能组件都很清晰, 能够单独使用 在Spring Cloud中看似也很清晰, 但实际上Spring Cloud的整合中会把多个组件配套组合起来, 以达到简化分布式开发的目的 例如在深入实践组件Feign时, 你是不是需要了解feign-hystrix, ribbon相关的东西呢? 再例如超时, Zuul, Feign, Hystrix还是RestTemplate, 它们都有timeout相关的概念, retry机制也类似… 还好关于各组件中的超时和重试, 已经有人对其做了总结, 附上原文链接: Spring Cloud各组件超时总结 Spring Cloud各组件重试总结 ^_^感谢这些分享的人 补充由于原文中的总结比较简单, 这里再对重试做一些补充 spring-retry在Ribbon的重试中, 除了配置项, 还需要加入spring-retry 官网说明 Retrying Failed RequestsSpring Cloud Netflix offers a variety of ways to make HTTP requests. You can use a load balanced RestTemplate, Ribbon, or Feign. No matter how you choose to create your HTTP requests, there is always a chance that a request may fail. When a request fails, you may want to have the request be retried automatically. To do so when using Sping Cloud Netflix, you need to include Spring Retry on your application’s classpath. When Spring Retry is present, load-balanced RestTemplates, Feign, and Zuul automatically retry any failed requests (assuming your configuration allows doing so). pom依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.retry&lt;/groupId&gt; &lt;artifactId&gt;spring-retry&lt;/artifactId&gt;&lt;/dependency&gt; 配置项 (int)文中提及了三个配置项参数:1234567ribbon: # 同一实例最大重试次数，不包括首次调用 MaxAutoRetries: 1 # 重试其他实例的最大重试次数，不包括首次所选的server MaxAutoRetriesNextServer: 2 # 是否所有操作都进行重试 OkToRetryOnAllOperations: true 在ribbon-core中可以找到对应的默认值1234567891011public class DefaultClientConfigImpl implements IClientConfig { // MaxAutoRetriesNextServer (retryNextServer：重试下一实例) public static final int DEFAULT_MAX_AUTO_RETRIES_NEXT_SERVER = 1; // MaxAutoRetries (retrySameServer：重试相同实例) public static final int DEFAULT_MAX_AUTO_RETRIES = 0; // OkToRetryOnAllOperations public static final Boolean DEFAULT_OK_TO_RETRY_ON_ALL_OPERATIONS = Boolean.FALSE;} 这里要理解三个参数的意义, 可以这样推算出: 当MaxAutoRetries=1, MaxAutoRetriesNextServer=2时: RetryCount = (1+1) * (2+1) = 6次 当MaxAutoRetries=0, MaxAutoRetriesNextServer=1时: RetryCount = (0+1) * (1+1) = 2次 也就是: RetryCount = (maxAutoRetries + 1) * (maxAutoRetriesNextServer + 1) 配置项 (bool)上面三个参数中还有一个bool参数: OkToRetryOnAllOperations 文中的解释是是否所有操作都进行重试, 这里的操作是指什么? 我第一眼看不懂 细节尽在源码中: 12345678910111213141516@Overridepublic RequestSpecificRetryHandler getRequestSpecificRetryHandler( RibbonRequest request, IClientConfig requestConfig) { if (this.ribbon.isOkToRetryOnAllOperations()) { return new RequestSpecificRetryHandler(true, true, this.getRetryHandler(), requestConfig); } if (!request.toRequest().method().equals(\"GET\")) { return new RequestSpecificRetryHandler(true, false, this.getRetryHandler(), requestConfig); } else { return new RequestSpecificRetryHandler(true, true, this.getRetryHandler(), requestConfig); }} 这是源码中的一段逻辑, 可以看到isOkToRetryOnAllOperations在这里进行了两个分支判断 判断isOkToRetryOnAllOperations=true时, 返回RetryHandler实现 判断HTTP Method GET: 返回RetryHandler实现 !GET: 返回RetryHandler实现, 不一样的是其中二个bool值为False 可以看到, 它们都会返回一个RetryHandler实现, 不同处在于传入的参数, 更准确的说是第二个bool值参数 来看下RequestSpecificRetryHandler构造函数中的几个参数 bool okToRetryOnConnectErrors: 字面意思是重试连接错误, 都为true bool okToRetryOnAllErrors: 字面意思是重试所有错误, 只有HTTP Method不是GET时为false etc… 理解这两个参数, 你只需要能够区分两种常见的Exception, 即: java.net.SocketTimeoutException: Read timed out: 这里的超时代表服务器请求超时 java.net.ConnectException: Connection refused: 连接被拒绝, 也就是无法连接到服务器 最大的区别在于SocketTimeoutException是服务端已经接收到请求, 而客户端没有正常接收(例如超时了) 需要明白, 这里的重试是指目标服务不可达或无法正常响应, 而不是指返回一个4xx, 5xx的错误重试 注意: OkToRetryOnAllOperations那里的HTTP Method判断作用的是服务提供者的HTTP方法, 并非调用方本身 关于Open Feign文中提到了Feign的重试中, 虽然Feign和Ribbon充实是独立的, 如果都启用可能会让重试次数叠加 所以Spring Cloud在后来版本改为feign.Retryer#NEVER_RETRY, 即默认不开启Feign的重试, 转而可以使用Ribbon的重试配置即可 但是, 注意过Spring Cloud Finchley版的可能知道Feign的引入出现了变化, 变成了open-feign 重点有以下几点 OpenFeign有自己的重试机制 spring-retry对于OpenFeign是不起作用的 但是OpenFeign中读取的配置还是ribbon.MaxAutoRetries和ribbon.MaxAutoRetriesNextServer, 依然生效 详细及源码分析可参考: Spring Cloud Finchley OpenFeign的重试配置相关的坑 使用建议重试确实能提高服务的容错和可用性, 但是使用不当不如不要使用 1: Hystrix超时时间 Hystrix的超时时间必须大于超时的时间: 这点很容易理解, 如果小于超时时间, 那就熔断了, 没有机会重试了 2: 考虑服务幂等性 这点在判断OkToRetryOnAllOperations逻辑的源码那里也能看出来, 在启用重试时一定要考虑下游服务接口的幂等性 概述为以下两点: 如果你的服务API没有完全考虑幂等性, 建议不要把OkToRetryOnAllOperations设置为True 其次, 即使设置为False, 如果下游服务的GET方法涉及资源操作, 无法满足幂等, 那么不建议启用重试机制","link":"/2019/04/26/Ribbon——与其它组件超时和重试/"},{"title":"Spring Boot 2.0源码解析-配置绑定","text":"[TOC] 一. 前言 开发中, 时常会有获取某个属性资源文件的场景, 尤其是在多个Profile不同配置时 熟悉Spring的可能知道: 来自spring-beans的@Value能够完成这一功能 我们经常在xml中配置的PropertyPlaceholderConfigurer及其父类就是来负责解析属性的 想要了解它的解析过程, 可以从PropertyPlaceholderConfigurer#postProcessBeanFactory()入手 @Value用起来很方便, 而在Spring Boot中, 提供了一种更为优雅的配置绑定方式: @ConfigurationProperties 通过配合该注解来完成属性资源文件到Bean属性的绑定. 之前我很好奇为什么引入rabbit依赖后, 没有配置任何属性源而直接启动就能连接到, Spring Boot的Auto Configuration特性是怎么把连接的配置属性绑定进去的呢 在spring-boot-autoconfigure源码中, 能看到大量@ConfigurationProperties案例的运用, 例如我们熟悉的: RedisProperties JpaProperties RabbitProperties etc… 这次源码分析我选用的版本是Spring Boot 2.0.8.RELEASE, 而网上的源码分析大都是基于1.x版本 二. 变化迁移的变化在Spring Boot 2.x中, 对配置属性绑定有了一些变化, 例如Relaxed Binding(宽松绑定)的约束, 以及新的Binder API 关于Relaxed Binding, 如果是1.x迁移2.x, 这部分内容的变化并不会造成很大麻烦 但如果你之前的配置使用了驼峰或过于”宽松”的写法, 那在初始化时校验就会收到报错的, 不信可以试试 具体变化可以参见wiki: Configuration Property Binding 源码的变化除了功能方面, 相较1.x版本, 代码部分在2.x有了较大的重构, 重写了整个绑定过程. 个人感觉相比1.x好看了 接口和绑定阶段职责的变化后面的源码部分会体现 有意思的是代码风格也有很大变化: 很多地方都可以看到用Stream流编程和链式的写法, 怪不得要以Java 8为基线 不少地方都使用了Java 8中的函数接口 所以看Spring Boot 2.x/Spring Framework 5的源码需要对函数式编程, lambda, Stream流有一些了解 Tip: 需要注意的是在进行源码调试中, 可能会有些不便 比如Stream的惰性求值特性, 也就是中间操作会在终止条件调用时候才执行 后面源码中能看到return结果中的allMatch()和findFirst()就是终止条件中的短路操作! 好在我们可以善用IDE的Evaluate Expression功能来查看 包括调试中, 如果我们想只关注自己配置的属性, 也可以使用IDE的debug condition功能, 很方便! 三. 基本使用@ConfigurationProperties的使用极为简单, 以下三步 属性配置文件 12345custom: name: thank age: 18 address[0]: 上海 address[1]: 北京 属性配置类 1234567891011@Data@Component@ConfigurationProperties(prefix = \"custom\")public class CustomProperties { private String name; private Integer age; private List&lt;String&gt; address;} 简单验证 1234567@Testpublic void testBean() { ConfigurableApplicationContext applicationContext = SpringApplication.run(CacheApplication.class); CustomProperties bean = applicationContext.getBean(CustomProperties.class); System.out.println(bean.toString());} 四. 源码解析源码重点关注的部分是spring-boot-xx模块的, 涉及到Spring Framework部分的源码则不做太多说明 @ConfigurationProperties首先来看下@ConfigurationProperties的源码, 我去掉了注释部分 123456789101112131415@Target({ ElementType.TYPE, ElementType.METHOD })@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface ConfigurationProperties { @AliasFor(\"prefix\") String value() default \"\"; @AliasFor(\"value\") String prefix() default \"\"; boolean ignoreInvalidFields() default false; boolean ignoreUnknownFields() default true;} 简单说明: value() &amp; prefiex(): 前面示例已经使用过, 用来解析属性文件中的前缀 可以看到上面打有@AliasFor, 所以你用哪个都没问题 ignoreInvalidFields(): 是否忽略无效的字段, 默认为false e.g. 有一个age=xxx的配置, 无法将该值绑定到Bean中的Integer age上, 是选择忽略(true)还是抛出异常(false) ignoreUnknownFields(): 是否忽略未知的字段, 默认为true e.g. 有一个在Bean中不存在的属性, 是选择忽略(true)还是抛出异常(false) 使用方式可以说简单了, 那么会有两个疑问: Spring是如何发现我们打有@ConfigurationProperties注解的bean的? 是如何将属性资源文件中的值绑定到Bean中的? ConfigurationPropertiesBindingPostProcessor在@ConfigurationProperties源码上可以看到@see中标明了以下两个类: 12@see ConfigurationPropertiesBindingPostProcessor@see EnableConfigurationProperties 一个处理类和一个Enable模块中的配置属性绑定模块注解 从字面意思看, 我们应该从这个处理类去寻找答案 进入该处理类ConfigurationPropertiesBindingPostProcessor来看看它的图: 相比Spring Boot 1.x版本, 继承结构简化清晰了好多 看到了几个常见接口, 能大概挑一下非重点: ApplicationContextAware: 方便获取上应用下文信息 Ordered: 方法getOrder()指定优先级, 可以理解为用来处理处理器的调用顺序 重点是看到了BeanPostProcessor和InitializingBean, 一定能猜到容器bean初始化实例时会调用一个初始方法, 和另外两个前后回调方法 所以重点关注以下三个Override方法: BeanPostProcessor: postProcessBeforeInitialization() InitializingBean: afterPropertiesSet() BeanPostProcessor: postProcessAfterInitialization() 这里与Spring 4.x不同的是在5.x中BeanPostProcessor的两个回调方法声明都使用了default method, 所以在这里postProcessAfterInitialization()也就是后置并未做任何事情, 可以忽略 afterPropertiesSet()源码如下:123456789@Overridepublic void afterPropertiesSet() throws Exception { this.beanFactoryMetadata = this.applicationContext.getBean( ConfigurationBeanFactoryMetadata.BEAN_NAME, ConfigurationBeanFactoryMetadata.class ); this.configurationPropertiesBinder = new ConfigurationPropertiesBinder(this.applicationContext, VALIDATOR_BEAN_NAME);} 可以看到这里做了两个初始化: beanFactoryMetadata: 初始化了工厂Bean的元数据信息 configurationPropertiesBinder: 从字面翻译感觉它一定与配置属性绑定有关 该类在1.x版本是没有的, 但原理类似 此时需要看一眼ConfigurationPropertiesBinder的构造函数 12345678ConfigurationPropertiesBinder(ApplicationContext applicationContext, String validatorBeanName) { this.applicationContext = applicationContext; this.propertySources = new PropertySourcesDeducer(applicationContext).getPropertySources(); this.configurationPropertiesValidator = getConfigurationPropertiesValidator(applicationContext, validatorBeanName); this.jsr303Present = ConfigurationPropertiesJsr303Validator.isJsr303Present(applicationContext);} 可以看到只是将propertySources, validator封装在内 关于validator这里看到一个jsr303XXX, 如果不清楚JSR-303, 但是你一定熟悉Hibernate-validator 不要跟ORM产生联系, 其是JSR-303规范的实现 所以也能大概明白它的作用: 即在绑定阶段对@ConfigurationPropertiesBean中的属性做校验 前面的简单示例中没有加valid, 其实用法与我们以前校验Form实体没什么两样, 在配置属性中标记valid注解即可: 1234567891011121314@Data@Component@Validated@ConfigurationProperties(value = \"custom\")public class CustomProperties { private String name; @Min(0) @Max(30) private Integer age; @Email(message = \"Not Email!\") private String email;} 需要注意的是, 2.x的@Valid在hibernate-validator已经被标记为@Deprecated, 用javax.validation的吧! 校验部分与配置属性绑定并无太大联系, 在此时不用太关注, 所以后面Validation相关的源码就刻意跳过! 关于propertySources看到propertySources是不是很眼熟? 以及Environment 这两个spring-core中提供的接口及其实现完成了系统属性, 环境配置, 属性资源配置的解析 可以在propertySources构造完成后打一个断点观察下 如图: 它正确的加载了默认的配置文件application.yml及我们定义的几个属性custom.xxx 当然, 可以结合@PropertySources自己指定资源文件的位置. 这仍是spring-core的东西, 不说了 postProcessBeforeInitialization()看完afterPropertiesSet(), 按照顺序, 自然来到了postProcessBeforeInitialization()中 1234567891011@Overridepublic Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException { ConfigurationProperties annotation = getAnnotation( bean, beanName, ConfigurationProperties.class ); if (annotation != null) { bind(bean, beanName, annotation); } return bean;} 该方法很简单, 就是调用一个getAnnotation()方法, 找出有ConfigurationProperties声明的bean, 执行bind()操作 该部分处理一定是被循环调用的, 具体的调用时机在这里: AbstractAutowireCapableBeanFactory#initializeBean –&gt; applyBeanPostProcessorsBeforeInitialization() 回到getAnnotation()方法, 它的源码如下:1234567private &lt;A extends Annotation&gt; A getAnnotation(Object bean, String beanName, Class&lt;A&gt; type) { A annotation = this.beanFactoryMetadata.findFactoryAnnotation(beanName, type); if (annotation == null) { annotation = AnnotationUtils.findAnnotation(bean.getClass(), type); } return annotation;} 可以看到会先从工厂bean中开始寻找起, 看到beanFactoryMetadata了吗, 前面提到过 在ConfigurationBeanFactoryMetadata中有一个这样的集合: Map&lt;String, FactoryMetadata&gt; beansFactoryMetadata 在回调方法postProcessBeanFactory()中, 完成对配置元数据进行处理, 这里的处理仅是对那个Map做put操作 无论是否工厂方式创建的bean, 最终都会调用工具类AnnotationUtils, 反射得到注解ConfigurationProperties信息 Then return it 基于以上, Spring已经找到了标记@ConfigurationProperties的配置bean了 但是还没有发现属性绑定阶段相关的信息 bind()接着往下看, 进入bind(), 猜想它一定负责绑定阶段 123456789101112131415private void bind(Object bean, String beanName, ConfigurationProperties annotation) { ResolvableType type = getBeanType(bean, beanName); Validated validated = getAnnotation(bean, beanName, Validated.class); Annotation[] annotations = (validated != null) ? new Annotation[] { annotation, validated } : new Annotation[] { annotation }; Bindable&lt;?&gt; target = Bindable.of(type) .withExistingValue(bean).withAnnotations(annotations); try { this.configurationPropertiesBinder.bind(target); } catch (Exception ex) { throw new ConfigurationPropertiesBindException(beanName, bean, annotation, ex); }} 同样, 略过Validate的部分, 可以看到使用链式设置属性包装出一个Bindable对象, 包含了以下内容: type: 使用Spring提供的泛型操作API获取到该bean的泛型信息 后面的绑定阶段需要根据属性的类型来判断使用什么方式转换(e.g. String, Collections…) bean: 该bean, 例如我们自己配置的customProperties annotations: 包含注解@ConfigurationProperties和@Validated的信息 进入ConfigurationPropertiesBinder#bind(Bindable&lt;?&gt; target)1234567public void bind(Bindable&lt;?&gt; target) { ConfigurationProperties annotation = target.getAnnotation(ConfigurationProperties.class); Assert.state(annotation != null, () -&gt; \"Missing @ConfigurationProperties on \" + target); List&lt;Validator&gt; validators = getValidators(target); BindHandler bindHandler = getBindHandler(annotation, validators); // 看这里 getBinder().bind(annotation.prefix(), target, bindHandler); // 看这里} 这里的getBindHandler(), 字面意思是返回一个绑定的处理器 进入源码可以看到, 就是根据注解ConfigurationProperties里前面说过的那两个属性来选择的: IgnoreErrorsBindHandler: 设置ignoreInvalidFields = true时: 忽略无效的属性 NoUnboundElementsBindHandler: ignoreUnknownFields = false时: 忽略不存在的的属性 IgnoreTopLevelConverterNotFoundBindHandler: 默认情况使用的 除了以上三个, 你可能还会看见ValidationBindHandler, 是对绑定对象执行Valid的处理器, 同样略过 查看这几个处理器的继承结构可以看出, 他们有共同的接口BindHandler 其中的不同主要体现在: onStart, onSuccess, onFailure, onFinish这几个阶段的回调方法上不同的覆盖行为上 getBinder()接下来看到getBinder(), 还记得之前的afterPropertiesSet()中做了哪些内容的初始化吧 结合它的构造函数, 来看下Binder是怎么构造出来的12345678910private Binder getBinder() { if (this.binder == null) { this.binder = new Binder( getConfigurationPropertySources(), getPropertySourcesPlaceholdersResolver(), getConversionService(), getPropertyEditorInitializer()); } return this.binder;} 至此, 我们见到了Binder, Bindable, 接下来还会见到BindResult, 这都是Spring Boot 2.x中Binder API中的东西 所以对比过1.x源码你会发现, 这里是最大的变化 文档中说明了: 一个Binder采用一个Bindable并返回一个BindResult 接下来你也会发现, Bindable在整个绑定过程中, 贯穿始终! 回到这个getBinder()方法, 解释下涉及到的几个接口: ConfigurationPropertySources: 这也是Spring Boot 2.x中加入的, 所以相较之前版本, 2.x开始使用ConfigurationPropertySource作为配置属性源进行属性绑定, 而非之前的propertySources 通过前面携带配置属性信息的propertySources对象而来它返回的是一个Iterable container, 通过注释也说明了, 他会展开嵌套的属性 ConversionService: 这也是spring-core中的, 很容易想到, 在对资源文件和属性bean之间绑定属性时需要这样一个提供类型转换功能的转换器 大致看一下:123456789101112131415161718192021222324252627282930 /** * Add converters useful for most Spring Boot applications. * @param registry the registry of converters to add to (must also be castable to * ConversionService, e.g. being a {@link ConfigurableConversionService}) * @throws ClassCastException if the given ConverterRegistry could not be cast to a * ConversionService */public static void addApplicationConverters(ConverterRegistry registry) { addDelimitedStringConverters(registry); registry.addConverter(new StringToDurationConverter()); registry.addConverter(new DurationToStringConverter()); registry.addConverter(new NumberToDurationConverter()); registry.addConverter(new DurationToNumberConverter()); registry.addConverterFactory(new StringToEnumIgnoringCaseConverterFactory());}/** * Add converters to support delimited strings. * @param registry the registry of converters to add to (must also be castable to * ConversionService, e.g. being a {@link ConfigurableConversionService}) * @throws ClassCastException if the given ConverterRegistry could not be cast to a * ConversionService */public static void addDelimitedStringConverters(ConverterRegistry registry) { ConversionService service = (ConversionService) registry; registry.addConverter(new ArrayToDelimitedStringConverter(service)); registry.addConverter(new CollectionToDelimitedStringConverter(service)); registry.addConverter(new DelimitedStringToArrayConverter(service)); registry.addConverter(new DelimitedStringToCollectionConverter(service));} 最终会调用多个类型转换服务 Binder接下来, 可以进入Binder#bind()了 bind()里面的构造函数很多, 我们着重来看这个12345678public &lt;T&gt; BindResult&lt;T&gt; bind(ConfigurationPropertyName name, Bindable&lt;T&gt; target, BindHandler handler) { Assert.notNull(name, \"Name must not be null\"); Assert.notNull(target, \"Target must not be null\"); handler = (handler != null) ? handler : BindHandler.DEFAULT; Context context = new Context(); T bound = bind(name, target, handler, context, false); return BindResult.of(bound);} 这里还不是真正的绑定阶段, 但有两个点需要说明: 参数: ConfigurationPropertyName: 是对前面传入的配置前缀prefix进行一些基本校验和处理 e.g.分割.连接的前缀 注释中也提醒了: they must be lower-case and must start with an alpha-numeric character 返回值: BindResult: 可以简单的看下它的源码, 里面有of(), get(), isBound(), orElse()等 是不是想起了Guava或Java 8中的Optional&lt;T&gt;, 没错, 他们的作用基本一样, 强制让调用方考虑处理绑定结果为null的情况 Context: 在这里是绑定上下文, 由前面说过的BindHandler使用 1234567891011121314151617181920212223public interface BindContext { /** * Return the current depth of the binding. Root binding starts with a depth of * {@code 0}. Each subsequent property binding increases the depth by {@code 1}. * @return the depth of the current binding */ int getDepth(); /** * Return an {@link Iterable} of the {@link ConfigurationPropertySource sources} being * used by the {@link Binder}. * @return the sources */ Iterable&lt;ConfigurationPropertySource&gt; getSources(); /** * Return the {@link ConfigurationProperty} actually being bound or {@code null} if * the property has not yet been determined. * @return the configuration property (may be {@code null}). */ ConfigurationProperty getConfigurationProperty();} 这里的绑定上下文中就提供前面说过的ConfigurationPropertySource Iterable Container 跟着代码往下, 看Binder中的下一个构造函数:1234567891011121314protected final &lt;T&gt; T bind(ConfigurationPropertyName name, Bindable&lt;T&gt; target, BindHandler handler, Context context, boolean allowRecursiveBinding) { context.clearConfigurationProperty(); try { if (!handler.onStart(name, target, context)) { return null; } Object bound = bindObject(name, target, handler, context,allowRecursiveBinding); return handleBindResult(name, target, handler, context, bound); } catch (Exception ex) { return handleBindError(name, target, handler, context, ex); }} 看到onStart了吗, 这里就有一个BindHandler回调接口中的一个方法, 也就是绑定开始但还未完成阶段 接下来在Binder中, 会陆续看到其它几个阶段的方法 再来看返回, 是通过handleBindResult()和handleBindError()来处理的. 点开这两个handleBindXXX()能看到在里面进行了onSuccess, onFinish, onFailure的调用, 代码不贴 bindObject()重点来看bindObject() 123456789101112131415161718192021222324private &lt;T&gt; Object bindObject(ConfigurationPropertyName name, Bindable&lt;T&gt; target, BindHandler handler, Context context, boolean allowRecursiveBinding) { ConfigurationProperty property = findProperty(name, context); // Part 1 if (property == null &amp;&amp; containsNoDescendantOf(context.streamSources(), name)) { // Part 1 return null; } AggregateBinder&lt;?&gt; aggregateBinder = getAggregateBinder(target, context); if (aggregateBinder != null) { return bindAggregate(name, target, handler, context, aggregateBinder); // Part 2 } if (property != null) { try { return bindProperty(target, context, property); // Part 2 } catch (ConverterNotFoundException ex) { // We might still be able to bind it as a bean Object bean = bindBean(name, target, handler, context, allowRecursiveBinding); if (bean != null) { return bean; } throw ex; } } return bindBean(name, target, handler, context, allowRecursiveBinding); // Part 2} 这里过程比较多, 可以分为2个部分来看 Part 1: 寻找属性和匹配过程开头的findProperty()和一个匹配方法:containsNoDescendantOf(), 它们的参数都有context 还记得吧? 上面说过了–&gt;提供绑定的上下文信息 这里在debug时候可以利用IDE的Evaluate Expression功能来验证判断的逻辑: Part 2: 绑定过程接下来是bindXXX这三个私有方法: bindAggregate(): 从注释上看出, 主要是负责处理Map, Collections, Array的绑定策略, 及完成多层属性的递归 bindProperty(): 是返回属性值的过程(其中包含类型转换) 例如属性资源文件中配置的name=thank -&gt; java.lang.String thank bindBean(): 会继续调用另外一个私有的重载函数bindBean() bindBean()重点看看这个方法 123456789101112131415161718private Object bindBean(ConfigurationPropertyName name, Bindable&lt;?&gt; target, BindHandler handler, Context context, boolean allowRecursiveBinding) { if (containsNoDescendantOf(context.streamSources(), name) || isUnbindableBean(name, target, context)) { return null; } BeanPropertyBinder propertyBinder = (propertyName, propertyTarget) -&gt; bind( name.append(propertyName), propertyTarget, handler, context, false); Class&lt;?&gt; type = target.getType().resolve(Object.class); if (!allowRecursiveBinding &amp;&amp; context.hasBoundBean(type)) { return null; } return context.withBean(type, () -&gt; { Stream&lt;?&gt; boundBeans = BEAN_BINDERS.stream() .map((b) -&gt; b.bind(name, target, context, propertyBinder)); return boundBeans.filter(Objects::nonNull).findFirst().orElse(null); });} 里面又调用了上层的Binder.bind(), 递归完成绑定 终止条件是上面提到过的containsNoDescendantOf()和另外一个判断isUnbindableBean() 关注一下最终的返回结果, 是调用了另外一个类: JavaBeanBinder的bind方法 JavaBeanBinderJavaBeanBinder是哪来的呢? 在Binder的静态方法中, 已经定义好了, 并被添加到一个不可变集合中 123456789101112public class Binder { private static final List&lt;BeanBinder&gt; BEAN_BINDERS; static { List&lt;BeanBinder&gt; binders = new ArrayList&lt;&gt;(); binders.add(new JavaBeanBinder()); BEAN_BINDERS = Collections.unmodifiableList(binders); } //...} bind()进入JavaBeanBinder#bind()之后看到继续调用了另一个私有的重载方法 1234567private &lt;T&gt; boolean bind(BeanPropertyBinder propertyBinder, Bean&lt;T&gt; bean, BeanSupplier&lt;T&gt; beanSupplier) { boolean bound = false; for (Map.Entry&lt;String, BeanProperty&gt; entry : bean.getProperties().entrySet()) { bound |= bind(beanSupplier, propertyBinder, entry.getValue()); } return bound;} 里面会迭代该配置bean中的所有属性, 调试模式下随便取一个来看看:123456789100 = {LinkedHashMap$Entry@7470} &quot;name&quot; -&gt; key = &quot;name&quot; value = {JavaBeanBinder$BeanProperty@7475} name = &quot;name&quot; declaringClassType = {ResolvableType@7481} &quot;com.example.cache.config.CustomProperties&quot; getter = {Method@7482} &quot;public java.lang.String com.example.cache.config.CustomProperties.getName()&quot; setter = {Method@7483} &quot;public void com.example.cache.config.CustomProperties.setName(java.lang.String)&quot; field = {Field@7484} &quot;private java.lang.String com.example.cache.config.CustomProperties.name&quot;1 = {LinkedHashMap$Entry@7471} &quot;age&quot; -&gt; 2 = {LinkedHashMap$Entry@7472} &quot;email&quot; -&gt; 没错, 配置bean中: 的属性名, Setter和Getter 该有的都有了 下面来到JavaBeanBinder的最后一个重载方法 1234567891011121314151617181920private &lt;T&gt; boolean bind(BeanSupplier&lt;T&gt; beanSupplier, BeanPropertyBinder propertyBinder, BeanProperty property) { String propertyName = property.getName(); ResolvableType type = property.getType(); Supplier&lt;Object&gt; value = property.getValue(beanSupplier); Annotation[] annotations = property.getAnnotations(); Object bound = propertyBinder.bindProperty( propertyName, Bindable.of(type).withSuppliedValue(value).withAnnotations(annotations) ); if (bound == null) { return false; } if (property.isSettable()) { property.setValue(beanSupplier, bound); } else if (value == null || !bound.equals(value.get())) { throw new IllegalStateException(\"No setter found for property: \" + property.getName()); } return true;} 这里就比较简单了, 分步拿到了配置Bean属性的定义和值: field: 即propertyName, e.g. name 属性类型: type, e.g. java.lang.String getter and setter e.g.public void com.example.cache.config.CustomProperties.setName(java.lang.String) 以及调用propertyBinder.bindProperty()拿到了资源属性文件中的属性值bound 该方法的作用前面也提到过(e.g. thank ) 然后调用了属性的setValue()方法: 执行property.setValue(beanSupplier, bound); 至此, 看到了调用属性的set方法, 终于可以放心了! 从ConfigurationPropertiesBindingPostProcessor开始到调用setter结束, 完整的调用栈如下: 123456789101112131415161718192021222324252627bind:85, JavaBeanBinder {org.springframework.boot.context.properties.bind}bind:62, JavaBeanBinder {org.springframework.boot.context.properties.bind}bind:54, JavaBeanBinder {org.springframework.boot.context.properties.bind}lambda$null$5:341, Binder {org.springframework.boot.context.properties.bind}apply:-1, 1445225850 {org.springframework.boot.context.properties.bind.Binder$$Lambda$267}accept:193, ReferencePipeline$3$1 {java.util.stream}tryAdvance:1351, ArrayList$ArrayListSpliterator {java.util}forEachWithCancel:126, ReferencePipeline {java.util.stream}copyIntoWithCancel:498, AbstractPipeline {java.util.stream}copyInto:485, AbstractPipeline {java.util.stream}wrapAndCopyInto:471, AbstractPipeline {java.util.stream}evaluateSequential:152, FindOps$FindOp {java.util.stream}evaluate:234, AbstractPipeline {java.util.stream}findFirst:464, ReferencePipeline {java.util.stream}lambda$bindBean$6:342, Binder {org.springframework.boot.context.properties.bind}get:-1, 2008619427 {org.springframework.boot.context.properties.bind.Binder$$Lambda$266}withIncreasedDepth:441, Binder$Context {org.springframework.boot.context.properties.bind}withBean:427, Binder$Context {org.springframework.boot.context.properties.bind}access$400:381, Binder$Context {org.springframework.boot.context.properties.bind}bindBean:339, Binder {org.springframework.boot.context.properties.bind}bindObject:278, Binder {org.springframework.boot.context.properties.bind}bind:221, Binder {org.springframework.boot.context.properties.bind}bind:210, Binder {org.springframework.boot.context.properties.bind}bind:192, Binder {org.springframework.boot.context.properties.bind}bind:82, ConfigurationPropertiesBinder {org.springframework.boot.context.properties}bind:107, ConfigurationPropertiesBindingPostProcessor {org.springframework.boot.context.properties}postProcessBeforeInitialization:93, ConfigurationPropertiesBindingPostProcessor {org.springframework.boot.context.properties} 有兴趣debug的可以按照调用栈的顺序由下至上来参考 五. 总结总体阅读下来, 相较1.x版本封装度更高, 而且代码风格大量java 8中的写法, 确实给阅读带来了些麻烦, 但是优秀的封装能明显感觉到职责更加明确 顺着源码读下来, 有意略过了很多细节, 例如: Validator结合配置Bean怎么进行属性验证的? 2.x中的宽松绑定约束是在什么地方体现的? 等等 重心还是放在了如何发现我们的属性配置Bean以及如何将属性资源文件中的值绑定到属性配置Bean中这两点 所以只是针对以上两点知其然, 初步窥探了ConfigurationProperties的黑盒","link":"/2019/04/01/Spring Boot 配置绑定源码解析/"},{"title":"Docker初探","text":"[TOC] 前言学习环境是本地虚拟机, linux版本使用CentOS 7. 由于使用最小安装方式, 在学习Docker之前, 顺带总结了下jdk安装和防火墙相关配置. 一: jdk安装 在Centos下安装jdk有多种方式, 例如手动下载tar文件解压, yum安装, rpm安装等. 这里我用yum方式安装: list installed |grep java``` 12345678910111213 查看系统中是否已经安装jdk, 如果有安装想卸载: ```yum remove xxx```即可.* ```yum list java-1.8.0*``` 或 ```yum search java|grep jdk``` 查看yum库中的jdk安装包.* ```yum -y install java-1.8.0-openjdk ``` 这里我安装jdk1.8* ```/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-1.b16.el7_3.x86_64 安装完成后的默认目录 验证java命令就可以了. 注意: 这种yum方式安装的是openjdk, 并不是sun jdk. 如果想要sunjdk就用安装方式. 因为这种方式使用alternatives进行版本控制, 所以在没有设置环境变量的情况下也可以执行java命令. 但是对于tomcat或其他软件要使用jdk的话就要设置环境变量了. 对于环境变量设置: 修改1234567```#set java environmentJAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.141-1.b16.el7_3.x86_64JRE_HOME=$JAVA_HOME/jreCLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libPATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binexport JAVA_HOME JRE_HOME CLASS_PATH PATH 让修改生效/etc/profile```123456789101112131415161718### 二: 防火墙配置CentOS 7中默认使用firewall作为防火墙, 而6.x中是iptables. CentOS 7下防火墙的配置, 也支持iptables防火墙. 前提是需要关闭禁止firewall然后安装iptables-services. &gt; 二者不能共用.&gt;&gt; CentOS7下Firewall防火墙配置用法详解(推荐): https://yq.aliyun.com/ziliao/94786我觉得既然CentOS 7 中升级到了firewall防火墙, 就没必要切换iptables. 不如就用firewall.关于firewall的配置, 首先来看两个目录:* 系统配置目录: ```/usr/lib/firewalld/services 目录中存放定义好的网络服务和端口参数，系统参数，不能修改 用户配置目录: 1234567891011121314#### 自定义添加端口 firewall支持两种自定义添加的方式: 命令添加和配置文件添加. 其中, 命令添加方式也会在配置文件中体现* **命令添加方式**: ```shell firewall-cmd --permanent --add-port=8050/tcp # 指定zone: firewall中有Zone的概念, 可以将具体的端口指定到具体的zone配置文件中. firewall-cmd --zone=public --permanent --add-port=8050/tcp 12345# 参数介绍:1. firwall-cmd：是Linux提供的操作firewall的一个工具;2. --permanent：表示设置为持久;3. --add-port：标识添加的端口;4. --zone=public：指定的zone为public; 如果命令指定–zone=dmz, 会在dmz.xml文件中新增一条. 默认zone=public 配置文件添加方式: 如public.xml:1234567891011```xml&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;zone&gt; &lt;short&gt;Public&lt;/short&gt; &lt;description&gt;xxx&lt;/description&gt; &lt;service name=&quot;dhcpv6-client&quot;/&gt; &lt;service name=&quot;ssh&quot;/&gt; &lt;port protocol=&quot;tcp&quot; port=&quot;8050&quot;/&gt; &lt;port protocol=&quot;tcp&quot; port=&quot;8761&quot;/&gt;&lt;/zone&gt; 常用命令firewall 1234567891011# 查看firewall服务状态:systemctl status firewalld# 查看firewall的运行状态(running or not running):firewall-cmd --state# 查看防火墙规则:firewall-cmd --list-all # 查看已开放的端口(--zone可以不写, 默认为public)firewall-cmd --zone=public --list-ports 123456# 重启, 开启, 关闭service firewalld restart service firewalld start service firewalld stop # orsystemctl [stop/stop/restart] firewalld 补充下iptables的基本使用吧 1234567891011121314151617181920# 查看防火墙启动状态service iptables status# 关闭防火墙service iptables stop# 启动防火墙service iptables start# 查看规则列表iptables --list-rules# 编辑规则vim /etc/sysconfig/iptables# 加入一行-A INPUT -p tcp -m state --state NEW -m tcp --dport 25 -j ACCEPT# 保存，重启防火墙service iptables restart 三: docker安装方式一 12# docker安装: yum install -y docker-io 这里docker也可以, 网上有说centos7以上用docker, 我这里用docker-io也没问题 12345# 启动systemctl start docker# 设置开机启动systemctl enable docker 12345# 查看docker信息docker info # 查看版本信息docker version 安装方式二 上面介绍了一键的安装方式, 这里推荐使用官方的安装方式 官方文档: https://docs.docker.com/install/linux/docker-ce/centos/ 卸载12345678910$ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine 1$ sudo rm -rf /var/lib/docker 安装 安装相关工具类 123$ sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 配置docker仓库 123$ sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo 安装docker 1$ sudo yum install docker-ce 在第二步配置docker仓库错误: 12345Loaded plugins: fastestmirroradding repo from: https://download.docker.com/linux/centos/docker-ce.repograbbing file https://download.docker.com/linux/centos/docker-ce.repo to /etc/yum.repos.d/docker-ce.repoCould not fetch/save url https://download.docker.com/linux/centos/docker-ce.repo to file /etc/yum.repos.d/docker-ce.repo: [Errno 14] curl#35 - &quot;TCP connection reset by pe 这是由于国内访问不到docker官方镜像的缘故, 可以通过aliyun的源来完成 1sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 镜像在线拉取: http://hub.docker.com 中查找需要的镜像 或search xxx```命令查找docker.io上的镜像123456789101112131415161718192021222324252627- 例如Mysql镜像: 使用```docker pull mysql``` , 会默认从docker.io中拉取.&gt; Tip: docker.io太慢可以用国内的镜像仓库```hub.daocloud.io```, 或使用daocloud加速器离线安装: 拷贝tar包至文件中: `docker load &lt;./xxx.tar`列出当前镜像: `docker images`打包镜像: `docker save &gt;./mysql123.tar docker`#### 容器```docker create```: image(unioned read file system) -&gt; container(unioned RW file system)容器创建. 在镜像创建一个容器后加了一层读写层```docker start```: 在容器(container)之上启动一个可使用的进程空间, 执行后会进入Created状态```shelldocker create -p 3308:3306 --name mysql2 -e MYSQL_ROOT_PASSWORD=root docker.io/mysql:5.7.14docker start [container id] run```来启动容器12该命令实际包含两个命令: ```docker create```和```docker start 123# 例如启动两个mysqldocker run -d -p 3306:3306 --name mysql1 -e MYSQL_ROOT_PASSWORD=root docker.io/mysql:5.7.14docker run -d -p 3308:3306 --name mysql2 -e MYSQL_ROOT_PASSWORD=root docker.io/mysql:5.7.14 参数解释: -d: 后台运行-p: 对外暴露的端口-e: 环境参数–name: 指定容器名(不指定的话系统随机分配name名) 12# 查看容器(加-a表示包括运行和非运行所有状态的)docker ps -a 12345678# 停止docker stop [id]# 删除容器(-f:强制)docker rm -f [id] # 删除镜像docker rmi [imageID] 注意: 容器在运行状态时: 可以强制删除容器, 但不能强制删除镜像. 进入容器: exec -it [container name or conteiner id]link123456789101112131415#### 数据卷容器中管理数据主要有两种方式：数据卷（Data Volumes）数据卷容器（Data Volumes Dontainers）##### 数据卷管理示例```shelldocker run -d -it --name=centos1 -v /root/dbdata:/test/dbdata docker.io/centos:7.2.1511 123# 参数解释-v: 挂载一个本地的目录到容器中作为数据卷.例如: -v /xx:/yy: 将容器中的目录/yy挂载到机器中/xx中 这里我遇到过一个问题: denied```1234567&gt;&gt; 这可能会导致容器中映射的目录无权限打开, 或者启动容器后是exited状态. &gt;&gt; 可以通过```docker logs [container id]```查看日志.&gt;&gt; 原因是SELinux导致的. 这货的访问控制策略相当复杂, 很多运维人员的建议都是将他关闭.&gt; 查看SELinux状态: ``` /usr/sbin/sestatus``` 或 ```getenforce 关闭: /etc/selinux/config``` 文件, 将```SELINUX12345678910111213* 在启动centos1容器后, 在宿主机```/root/dbdata```和容器```/test/dbdata```中的任何操作都会互相映射, 相当于一份数据的备份. * 关闭或者删除centos1容器后, 宿主机数据依然在, 也可以启动多个容器, 映射数据卷目录中的数据. **示例**: mysql数据备份```shelldocker run -d -p 3307:3306 \\-v /mysql/data:/var/lib/mysql \\-v /mysql/config:/etc/mysql/conf.d \\-e MYSQL_ROOT_PASSWORD=root \\--name mysqldb1 \\docker.io/mysql:5.7.14 这里如果再启一个mysqldb2测试, mysqldb1状态是run, 启动mysqldb2后, 会出现mysqldb2异常关闭或连接不通的现象. 所以再启动mysqldb2容器时候, mysqldb1不能是run的状态, 因为在-v参数中都有对123456789101112##### 数据卷容器数据卷容器: 其实就是一个正常容器, 专门用来提供数据卷供其他容器挂载. 如果你有持续更新的数据需要在容器之间共享. 那么可以创建数据卷容器.1. 创建创建一个数据卷容器 data_container ``` docker run -it -d -v /dbdata --name data_container docker.io/centos:7.2.1511 创建centos1, centos2 两个容器, 并挂载data_container容器中的数据卷. 12docker run -it -d --volumes-from data_container --name centos1 docker.io/centos:7.2.1511docker run -it -d --volumes-from data_container --name centos2 docker.io/centos:7.2.1511 然后这两个容器中都能够共享到数据卷目录123456789101112131415161718192021222324252627关于```-v```参数:* 在创建数据卷容器时, ```-v /dbdata 或 -v /root/dbdata:/dbdata```都可以. 前者只在数据卷容器中创建dbdata目录, 不和本地宿主机映射. 后者会在本地宿主机也有映射的目录```/root/dbdata```.关于````--volumes-from````参数:* 所挂载数据卷的容器自己并不需要保持在运行状态. * 可以使用多个, 从而实现从多个容器中挂载多个数据卷. * 可以从其他已经挂载了数据卷的容器来挂载数据卷.##### 利用数据卷容器进行数据迁移备份* **备份**```shell# 将数据卷容器(data_container)中的数据备份到宿主机的当前目录docker run --volumes-from data_container -v $(pwd):/backup --name worker docker.io/centos:7.2.1511 tar cvf /backup/backup.tar /dbdata 这条命令包含以下几个步骤: 首先利用centos镜像创建了一个容器worker. 使用12345678910111213143. 使用-v $(pwd):/backup参数来挂载本地的当前目录到worker容器的/backup目录.4. worker容器启动后, 使用了```tar cvf /backup/backup.tar /dbdata```命令来将```/dbdata```下内容备份为容器 内的/backup/backup.tar, 即也映射到宿主主机当前目录(pwd)下的backup.tar.执行成功后在宿主机的当前目录下执行```tar xvf backup.tar```后看到看到```/dbdata```备份内容.* **恢复** 1. 创建一个带有数据卷的容器: ```docker run -d -it -v /dbdata --name db docker.io/centos:7.2.1511 创建另外一个新的容器, 挂载db的容器, 并解压备份文件bakup.tar到所挂载的容器卷中. 1docker run --volumes-from db -v $(pwd):/backup docker.io/centos:7.2.1511 tar xvf /backup/backup.tar 这样就将宿主机当前目录下的backup.tar恢复到db容器:1234567891011121314151617181920212223242526272829#### Dockerfile如何使用Dockerfile构建一个应用程序的镜像.将应用程序的jpa.jar和Dockerfile文件拷贝到```/root/trainning```目录下Dockerfile的内容如下:```dockerfile# 指定基础image, 要放在前面, 因为后续的指令有依赖域这个imageFROM java:8u102-jdk# 指定镜像创建者信息MAINTAINER thank# 构建指令, RUN可以运行任何被基础image支持的命令RUN mkdir /app# 构建指令, ADD &lt;src&gt; &lt;dest&gt; 从宿主机的src路径拷贝文件到容器中的dest路径ADD . /app# 指定容器需要映射到宿主机器的端口EXPOSE 8888# 设置container启动时执行的操作ENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;/app/jpa.jar&quot;,&quot;--spring.profiles.active=prod&quot;] build -t jpa:1.0 .```: 用当前目录```.```中的内容构建一个名叫jpa:1.0的镜像.12执行命令后, 会看到以下输出, 每一层都对应Dockerfile文件中定义的指令. Sending build context to Docker daemon 27.7 MBStep 1 : FROM java:8u102-jdk —&gt; 69a777edb6dcStep 2 : RUN mkdir /app —&gt; Running in 6779b11de804 —&gt; 44fc7d80f5f6Removing intermediate container 6779b11de804Step 3 : ADD . /app —&gt; c419c4d34889Removing intermediate container 00fa44dcbc4eStep 4 : EXPOSE 8888 —&gt; Running in bb4e70a56e04 —&gt; 24e66dd52ee1Removing intermediate container bb4e70a56e04Step 5 : ENTRYPOINT java -jar /app/jpa.jar –spring.profiles.active=prod —&gt; Running in b5cbcfce01b1 —&gt; 45e74f2c2173Removing intermediate container b5cbcfce01b1Successfully built 45e74f2c2173 123456执行成功后, 该应用程序镜像就构建出来了. 接下来可以启动容器来验证.```docker run -d --name myapp --link mysql:mysql -p 8901:8888 jpa:1.0 12345678参数解释: --link mysql:mysql前面的mysql是mysql容器名. 后面的mysql是别名, 对应应用程序中的spring.datasource.url=jdbc:mysql://mysql:3306/test中的mysql机器名-p 8901:8888前面: 对外暴露的端口号(外界访问的)后面: 程序容器内暴露的端口号 Docker-composeCompose是Docker的服务编排工具，主要用来构建基于Docker的复杂应用，Compose 通过一个配置文件来管理多个Docker容器，非常适合组合使用多个容器进行开发的场景 安装方式： https://github.com/docker/compose/releases docker-compose up 123curl -L https://github.com/docker/compose/releases/download/1.8.0-rc1/docker-compose-uname -s-uname -m &gt; /usr/local/bin/docker-composechmod +x /usr/local/bin/docker-compose 待续… 问题记录:问题一systemctl start docker时报错 有些机器上会出现, 有些不会 通过systemctl status docker.service -l查看详细错误12Error starting daemon: SELinux is not supported with the overlay2 graph driver on this kernel. Either boot into a newer kernel or disable selinux in docker (--selinux-enabled=false) 意思是说， 此linux的内核中的SELinux不支持 overlay2 graph driver ，解决方法有两个，要么启动一个新内核，要么就在docker里禁用selinux，--selinux-enabled=false 解决方法:vi /etc/sysconfig/docker 将--selinux-enabled改为--selinux-enabled=false 问题二我碰到过在虚拟机中启动一个容器时 报出: WARNING: IPv4 forwarding is disabled. Networking will not work. 虽然容器起来了, 但是导致端口转发了, 外部无法访问: 解决办法: /etc/sysctl.conf```1234或者``` vi /usr/lib/sysctl.d/00-system.conf 添加如下代码： 1234重启network服务```systemctl restart network 查看是否修改成功 sysctl net.ipv4.ip_forward 如果返回为“net.ipv4.ip_forward = 1”则表示成功了 推荐一个解决Docker学习中的疑难杂症: Docker问答录","link":"/2016/12/31/docker初探/"},{"title":"Spring Data JPA源码分析-方法命名查询","text":"[TOC] 前言为什么要学习Spring Data JPA, 这里引用《Spring Data JPA实战》一书的一段话 为什么要重新学习“Spring Data JPA”？俗话说的好：“未来已经来临，只是尚未流行”，纵观市场上的 ORM 框架，MyBatis 以灵活著称，但是要维护复杂的配置，并且不是 Spring 官方的天然全家桶，还得做额外的配置工作，如果资深的架构师还得做很多封装；Hibernate 以 HQL 和关系映射著称，但是就是使用起来不是特别灵活；那么 Spring Data JPA 来了，感觉要夺取 ORM 的 JPA 霸主地位了，底层以 Hibernate 为封装，对外提供了超级灵活的使用接口，又非常符合面向对象和 Rest 的风格，感觉是架构师和开发者的福音，并且 Spring Data JPA 与 Spring Boot 配合起来使用具有天然的优势，你会发现越来越多的公司的招聘要用会有传统的 SSH、Spring、MyBatis 要求，逐步的变为 Spring Boot、Spring Cloud、Spring Data 等 Spring 全家桶的要求，而很多新生代的架构师基于其生态的考虑，正在逐步推动者 Spring Data JPA 的更多的使用场景。 方法命名查询方法命名查询: Defining Query Method 在最初, 我用到方法命名查询时觉得很酷, 这应该算是Spring Data JPA的一大特色了. 在Spring Data JPA中可以用创建规范的自定义方法进行查询 尽管这种方式是很方便的, 但是难免会遇到这样的情况: 方法名解析器不支持要使用的关键字 复杂糟糕的查询约束会导致方法名也变得很糟糕(例如: findByXxxXxxXxInXxxOrderXxxXxx...) 当然Specification, @Query能够解决这样的情况, 但并不是本节的重点. 首先要知道, 在自定义方法名称中, 只需要在自己的实体仓储(Repository)上继承Repository接口即可. 当然, 如果想要有其它默认的通用方法实现, 可以有选择性的继承CrudRepository, PagingAndSortingRepository, JpaRepository等接口. 以及JpaSpecificationExecutor, QueryByExampleExecutor, QuerydslPredicateExecutor 和自定义Repository, 都可以达到同样的效果. 并且在默认情况下, 通用的默认方法(例如: findOne(), save()...)无需我们实现, SimpleJpaRepository会对其提供实现. Repository作用为什么继承一个Repository接口就可以呢? 首先Repository由Spring Data Common提供, 如果点进去可以发现它是一个空接口, 没有任何方法声明, 也就是常说的标记接口(Mark Interface). JDK中提供了也存在这样的接口: 例如最常见的Serializable 而标记接口的作用就是当某个类去实现了标记接口, 就会认为这个类具有了标记的某种能力. 观察Repository的层次关系, 大致可以分出三条路线来: CrudRepository -&gt; PagingAndSortingRepository -&gt; JpaRepository -&gt; SimpleJpaRepository ReactiveRepository RxJava2CrudRepository 需要知道的是: Repository, CrudRepository, PagingAndSortingRepository都是Spring Data Common下的标准接口, 而到了JpaRepository 开始才进入了Spring Data JPA模块 Spring Data 公共模块所以到这里也能大致看出Spring Data下模块划分的意义 Spring Data Common作为Spring Data所有模块的公用部分. 如果我们使用JPA, 那它的实现就是Spring Data JPA下的SimpleJpaRepository. 如果是其他NoSQL实现, 例如redis, 那他的实现就是Spring Data Redis里的SimpleKeyValueRepository 提供Repository默认方法实现的源码并不会在这里分析, 因为它不是本节的重点. 在接口中定义规范的方法名就可以推导出查询来, 我们并不需要做任何实现, 那么Spring Data JPA是如何做到的呢? 源码分析Repository工厂类我们在Java Config方式配置Spring Data JPA的@EnableJpaRepositories注解里可以发现这样一句: Class&lt;?&gt; repositoryFactoryBeanClass() default JpaRepositoryFactoryBean.class; 配置中默认指定了Repository的工厂类 -&gt; JpaRepositoryFactoryBean 来看下源码: 123456789101112public class JpaRepositoryFactoryBean&lt;T extends Repository&lt;S, ID&gt;, S, ID extends Serializable&gt; extends TransactionalRepositoryFactoryBeanSupport&lt;T, S, ID&gt; { //... private EntityManager entityManager; @Override public void afterPropertiesSet() { Assert.notNull(entityManager, \"EntityManager must not be null!\"); super.afterPropertiesSet(); } //...} 通过这个类的命名可以猜到是个工厂Bean, 并且有看到Bean初始化后的回调处理afterPropertiesSet() 所以立刻点进它的父类继承到抽象类RepositoryFactoryBeanSupport中 没错! 它实现InitializingBean接口和FactoryBean接口. 接下来就是寻找这个RepositoryFactoryBean是在哪里得到的呢. 自然是从获取工厂实例的实现方法下手: RepositoryFactoryBeanSupport.getObject(): 123456789101112131415public T getObject() { return initAndReturn();}public Class&lt;? extends T&gt; getObjectType() { return (Class&lt;? extends T&gt;) (null == repositoryInterface ? Repository.class : repositoryInterface);}/* * 这里是单例 * @see org.springframework.beans.factory.FactoryBean#isSingleton() */public boolean isSingleton() { return true;} RepositoryFactoryBeanSupport.initAndReturn(): 123456789101112131415/** * Returns the previously initialized repository proxy or creates * and returns the proxy if previously uninitialized. */private T initAndReturn() { Assert.notNull(repositoryInterface, \"Repository interface must not be null on initialization!\"); if (this.repository == null) { // 看这里 this.repository = this.factory.getRepository(repositoryInterface, customImplementation); } return this.repository;} 可以看到Repository Instance是在this.factory中得到, 那么再追入RepositoryFactorySupport factory. RepositoryFactorySupport#factory的创建不难找到, 其实上面已经提到过bean初始化回调方法afterPropertiesSet() , 就是在这里创建的factory 12345678910111213141516171819public void afterPropertiesSet() { Assert.notNull(repositoryInterface, \"Repository interface must not be null on initialization!\"); // 看这 this.factory = createRepositoryFactory(); this.factory.setQueryLookupStrategyKey(queryLookupStrategyKey); this.factory.setNamedQueries(namedQueries); this.factory.setEvaluationContextProvider(evaluationContextProvider); this.factory.setRepositoryBaseClass(repositoryBaseClass); this.factory.setBeanClassLoader(classLoader); this.factory.setBeanFactory(beanFactory); this.repositoryMetadata = this.factory.getRepositoryMetadata(repositoryInterface); if (!lazyInit) { initAndReturn(); } } createRepositoryFactory()的实现是在TransactionalRepositoryFactoryBeanSupport中 123456789@Overrideprotected final RepositoryFactorySupport createRepositoryFactory() { RepositoryFactorySupport factory = doCreateRepositoryFactory(); factory.addRepositoryProxyPostProcessor(exceptionPostProcessor); factory.addRepositoryProxyPostProcessor(txPostProcessor); return factory;} 再往下查看源码实际就是在JpaRepositoryFactoryBean中注入entityManager后 return new JpaRepositoryFactory(entityManager); 然后添加两个PostProcessor后返回factory 继续回到RepositoryFactoryBeanSupport.initAndReturn() 1this.repository = this.factory.getRepository(repositoryInterface, customImplementation); ####获取Repository实例代理 getRepository() 的实现就在JpaRepositoryFactory的父类RepositoryFactorySupport中: 123456789101112131415161718192021222324252627282930313233public &lt;T&gt; T getRepository(Class&lt;T&gt; repositoryInterface, Object customImplementation) { RepositoryMetadata metadata = getRepositoryMetadata(repositoryInterface); Class&lt;?&gt; customImplementationClass = null == customImplementation ? null : customImplementation.getClass(); RepositoryInformation information = getRepositoryInformation(metadata, customImplementationClass); validate(information, customImplementation); Object target = getTargetRepository(information); // Create proxy ProxyFactory result = new ProxyFactory(); result.setTarget(target); result.setInterfaces(new Class[] { repositoryInterface, Repository.class }); result.addAdvice(ExposeInvocationInterceptor.INSTANCE); if (TRANSACTION_PROXY_TYPE != null) { result.addInterface(TRANSACTION_PROXY_TYPE); } for (RepositoryProxyPostProcessor processor : postProcessors) { processor.postProcess(result, information); } if (IS_JAVA_8) { result.addAdvice(new DefaultMethodInvokingMethodInterceptor()); } result.addAdvice(new QueryExecutorMethodInterceptor(information, customImplementation, target)); return (T) result.getProxy(classLoader); } 在这个实现类中, 有获取仓库元数据, 验证, 注册方法拦截器的advice等. 注意: 在创建ProxyFactory实例后, 调用getProxy()返回的是Repository接口代理. 这也是常说的: Spring Data JPA 的实现原理是动态代理机制 到此, 已经大致了解了一些工厂BeanJpaRepositoryFactoryBean所做的一些事情. 接下来分析方法命名查询的重点就在 QueryExecutorMethodIntrceptor中了. ####QueryExecutorMethodIntrceptor方法拦截 首先看到它是RepositoryFactorySupport 的内部类, 并实现MethodInterceptor方法拦截器接口 我们在Repository接口中定义的查询方法是怎么被识别的? 在方法调用之前又经过了哪些处理? 那么在这个拦截器中应该能找到想要的答案 在实例化时执行构造方法: QueryExecutorMethodInterceptor() 12345678910111213141516171819202122232425262728293031public QueryExecutorMethodInterceptor(RepositoryInformation repositoryInformation, Object customImplementation, Object target) { //... QueryLookupStrategy lookupStrategy = getQueryLookupStrategy(queryLookupStrategyKey, RepositoryFactorySupport.this.evaluationContextProvider); lookupStrategy = lookupStrategy == null ? getQueryLookupStrategy(queryLookupStrategyKey) : lookupStrategy; Iterable&lt;Method&gt; queryMethods = repositoryInformation.getQueryMethods(); if (lookupStrategy == null) { if (queryMethods.iterator().hasNext()) { throw new IllegalStateException(\"You have defined query method in the repository but \" + \"you don't have any query lookup strategy defined. The \" + \"infrastructure apparently does not support query methods!\"); } return; } SpelAwareProxyProjectionFactory factory = new SpelAwareProxyProjectionFactory(); factory.setBeanClassLoader(classLoader); factory.setBeanFactory(beanFactory); for (Method method : queryMethods) { RepositoryQuery query = lookupStrategy.resolveQuery(method, repositoryInformation, factory, namedQueries); invokeListeners(query); queries.put(method, query); }} 该方法的有这样一句原文注释Builds a model of {@link QueryMethod}s to be invoked on execution of repository interface methods. 上面的源码大致可以分为两部分: 获取lookupStrategy(查找策略) 对Method进行迭代遍历, 根据lookupStrategy(查找策略)查询RepositoryQuery 为其添加监听 将Method和RepositoryQuery放入Map&lt;Method, RepositoryQuery&gt; queries缓存中 一句话总结就是为Repository接口中每个查询定义方法Method去构造相应的RepositoryQuery, RepositoryQuery的相关概念后面还会分析到. lookupStrategy方法查询定义的查找策略先从获取lookupStrategy(查找策略)看起: org.springframework.data.jpa.repository.query.JpaQueryLookupStrategy#create 12345678910111213141516171819public static QueryLookupStrategy create(EntityManager em, Key key, QueryExtractor extractor, EvaluationContextProvider evaluationContextProvider) { Assert.notNull(em, \"EntityManager must not be null!\"); Assert.notNull(extractor, \"QueryExtractor must not be null!\"); Assert.notNull(evaluationContextProvider, \"EvaluationContextProvider must not be null!\"); switch (key != null ? key : Key.CREATE_IF_NOT_FOUND) { case CREATE: return new CreateQueryLookupStrategy(em, extractor); case USE_DECLARED_QUERY: return new DeclaredQueryLookupStrategy(em, extractor, evaluationContextProvider); case CREATE_IF_NOT_FOUND: return new CreateIfNotFoundQueryLookupStrategy(em, extractor, new CreateQueryLookupStrategy(em, extractor), new DeclaredQueryLookupStrategy(em, extractor, evaluationContextProvider)); default: throw new IllegalArgumentException(String.format(\"Unsupported query lookup strategy %s!\", key)); } } 通过这段switch代码的条件判断和这三种策略对应的不同实现类, 可以总结出以下内容: 方法的查询策略有三种: Create: 通过解析规范命名的方法名来创建查询, 此时即使有@Query或@NameQuery也会忽略. 如果方法名不符合规则，启动的时候会报异常 USE_DECLARED_QUERY: 也就是使用注解方式声明式创建, 启动时就会去尝试找声明的查询, 也就是使用@Query定义的语句来执行查询, 没有则找@NameQuery的, 再没有就异常. CREATE_IF_NOT_FOUND: 先找@Query或@NameQuery定义的语句来查询, 如果没有就通过解析方法名来创建查询. 其中, 第三种方式CREATE_IF_NOT_FOUND相当于第一种和第二种的结合版. 也是默认的配置策略. QueryLookupStrategy 是策略的定义接口，JpaQueryLookupStrategy 是具体策略的实现类 本节的重点是自定义方法命名的推导策略, 所以2和3就先略过, 直接找Create策略对应的实现类CreateQueryLookupStrategy 1234567891011private static class CreateQueryLookupStrategy extends AbstractQueryLookupStrategy { //... @Override protected RepositoryQuery resolveQuery(JpaQueryMethod method, EntityManager em, NamedQueries namedQueries) { //... return new PartTreeJpaQuery(method, em, persistenceProvider); //... }} ####PartTreeJpaQuery实例推导 这时, 终于来到了实现name和method的拆分逻辑逻辑的方法了. 重点PartTreeJpaQuery 1234567891011121314151617181920212223242526272829303132/** * A {@link AbstractJpaQuery} implementation based on a {@link PartTree}. */public class PartTreeJpaQuery extends AbstractJpaQuery { private final Class&lt;?&gt; domainClass; private final PartTree tree; private final JpaParameters parameters; private final QueryPreparer query; private final QueryPreparer countQuery; private final EntityManager em; public PartTreeJpaQuery(JpaQueryMethod method, EntityManager em, PersistenceProvider persistenceProvider) { super(method, em); this.em = em; this.domainClass = method.getEntityInformation().getJavaType(); this.parameters = method.getParameters(); // 。。。 try { this.tree = new PartTree(method.getName(), domainClass); this.countQuery = new CountQueryPreparer(persistenceProvider, recreationRequired); this.query = tree.isCountProjection() ? countQuery : new QueryPreparer(persistenceProvider, recreationRequired); //。。。 } } // 。。。} 看到注释, 是基于PartTree实现 呢么一会的重点就是它了 其次, 它继承了抽象类AbstractJpaQuery 看下它的层次结构: 1234567AbstractJpaQuery - StoredProcedureJpaQuery - PartTreeJpaQuery - NamedQuery - AbstractStringBasedJpaQuery - SimpleJpaQuery - NativeJpaQuery SimpleJpaQuery, NativeJpaQuery: 就是@Query查询定义中的nativeQuery=false|true 前者使用JPQL, 后者使用原生SQL NamedQuery: 不用解释了, @NamedQuery StoredProcedureJpaQuery: 根据名字也猜的差不多, 是跟存储过程调用有关的查询 PartTreeJpaQuery: 这就是本节的重点, 方法命名查询的Query实例 到这里可以总结下: RepositoryQuery代表Repository接口中的一个查询方法 RepositoryQuery中持有QueryMethod实例, QueryMethod中持有Method实例. RepositoryQuery根据Repository接口中方法查询定义的不同, 被实例化成不同的子类实现. PartTree语法树回到PartTree中 123456789101112131415161718192021222324252627282930313233343536public class PartTree implements Iterable&lt;OrPart&gt; { private static final String KEYWORD_TEMPLATE = \"(%s)(?=(\\\\p{Lu}|\\\\P{InBASIC_LATIN}))\"; private static final String QUERY_PATTERN = \"find|read|get|query|stream\"; private static final String COUNT_PATTERN = \"count\"; private static final String EXISTS_PATTERN = \"exists\"; private static final String DELETE_PATTERN = \"delete|remove\"; private static final Pattern PREFIX_TEMPLATE = Pattern.compile( // \"^(\" + QUERY_PATTERN + \"|\" + COUNT_PATTERN + \"|\" + EXISTS_PATTERN + \"|\" + DELETE_PATTERN + \")((\\\\p{Lu}.*?))??By\"); private final Subject subject; private final Predicate predicate; /** * Creates a new {@link PartTree} by parsing the given {@link String}. * * @param source the {@link String} to parse * @param domainClass the domain class to check individual parts against to ensure they refer to a property of the * class */ public PartTree(String source, Class&lt;?&gt; domainClass) { Assert.notNull(source, \"Source must not be null\"); Assert.notNull(domainClass, \"Domain class must not be null\"); Matcher matcher = PREFIX_TEMPLATE.matcher(source); if (!matcher.find()) { this.subject = new Subject(null); this.predicate = new Predicate(source, domainClass); } else { this.subject = new Subject(matcher.group(0)); this.predicate = new Predicate(source.substring(matcher.group().length()), domainClass); } } //...} 配合着注释, 看到除了一些关键字定义外, 还定义了Subject和Predicate PartTree.Subject: 主语对象 for example &quot;findDistinctUserByNameOrderByAge&quot; would have the subject &quot;DistinctUser&quot;. PartTree.Predicate: 谓语对象 for example&quot;findDistinctUserByNameOrderByAge&quot; would have the predicate &quot;NameOrderByAge&quot;. Subject主语部分123456789101112131415private static class Subject { // ... public Subject(String subject) { this.distinct = subject == null ? false : subject.contains(DISTINCT); this.count = matches(subject, COUNT_BY_TEMPLATE); this.exists = matches(subject, EXISTS_BY_TEMPLATE); this.delete = matches(subject, DELETE_BY_TEMPLATE); this.maxResults = returnMaxResultsIfFirstKSubjectOrNull(subject); } // ... } Subject(主语对象)的职责很简单: 通过正则提取几个构建PartTree的属性 也就是几个布尔常量: distince, count, exists, delete, maxResults Predicate谓语部分12345678910111213141516private static class Predicate { //... public Predicate(String predicate, Class&lt;?&gt; domainClass) { String[] parts = split(detectAndSetAllIgnoreCase(predicate), ORDER_BY); if (parts.length &gt; 2) { throw new IllegalArgumentException(\"OrderBy must not be used more than once in a method name!\"); } buildTree(parts[0], domainClass); this.orderBySource = parts.length == 2 ? new OrderBySource(parts[1], domainClass) : null; }} 构造参数predicate是方法名的谓语部分, domainClass就是domainClass 可以看到大致做了这几件事: detectAndSetAllIgnoreCase()删除AllIgnoreCase关键字, 然后设置alwaysIgnoreCase=true 将剩余部分根据orderBy关键字分割成parts数组 用排序关键字orderBy之前的部分 buildTree() 用排序关键字orderBy之后的部分构建排序部分 举个例子: 方法名为findDistinctUserByNameOrderByAgeAllIgnoringCase() 解析出的谓语部分为: NameOrderByAgeAllIgnoringCase, 首先会删除AllIgnoreCase 得到NameOrderByAge 再根据排序关键字分割为Name和Age两部分, buildTree(), 设置排序orderBySource为Order By age: ASC buildTree() 该方法实际是对谓语部分进行or和and分割, 生成语法树节点的过程. Nodes: 首先用or分割, 构建OrPart子节点 Children: 子节点包含的child node由and继续分割Part 在Part中还设置了每个属性的Type和PropertyPath Part.Type: 属性约束的关键字枚举, 例如BETWEEN, IS_NOT_NULL等 PropertyPath: 映射对应的domain class中的属性路径, 例如User.name, User.age 到这里整个方法命名查询的PartTreeJpaQuery实例就推导完成了 命名方法执行过程PartTreeJpaQuery实例分析完成后会放入上面讲到的QueryExecutorMethodInterceptor.queries中 以ConcurrentHashMap&lt;Method, RepositoryQuery&gt; queries的形式存放 注意: Spring容器启动时候, 初始化时就会创建这些实例. 如果定义了错误的命名查询, 在启动时就会抛出异常. 有了queries 在什么时候执行呢? 还记得RepositoryQuery接口中的方法execute（ 通过find used找到上面提到的RepositoryFactorySupport.QueryExecutorMethodInterceptor#doInvoke中. 123456789101112131415161718192021private Object doInvoke(MethodInvocation invocation) throws Throwable { Method method = invocation.getMethod(); Object[] arguments = invocation.getArguments(); if (isCustomMethodInvocation(invocation)) { Method actualMethod = repositoryInformation.getTargetClassMethod(method); return executeMethodOn(customImplementation, actualMethod, arguments); } if (hasQueryFor(method)) { // 看这里! return queries.get(method).execute(arguments); } // Lookup actual method as it might be redeclared in the interface // and we have to use the repository instance nevertheless Method actualMethod = repositoryInformation.getTargetClassMethod(method); return executeMethodOn(target, actualMethod, arguments);} 看到了吧? 在实际调用时, 会直接从queries缓存中获取RepositoryQuery, 执行查询. 总结整个推导过程到最后, 实际就是去看PartTree语法生成树的逻辑. 无论是SQL解析器的逆向解析SQL语义还是本节PartTree中根据语义解析出SQL. 对我们来说这都不是重点. 所以本节大量篇幅都放在根据方法命名推导的原理上 包括Repository的工厂类 -&gt; JpaRepositoryFactoryBean的由来和Bean初始化后的一系列回调操作 并补充了Repository及其子接口和lookupStrategy查询策略的相关知识. 所以涉及到了Spring Data Common和Spring Data JPA模块的接口和实现类. 所以最懵的可能就是这些接口和实现的关系了. 所以这里引用一张UML图: http://www.dewafer.com/uploads/2017/2017-02-21-JpaRepoUML.png","link":"/2018/06/01/Spring Data JPA 源码分析 - 方法命名查询/"},{"title":"Spring Boot 初探","text":"前言工作中要用到Spring Boot和Spring Cloud了! 但是现在网上关于Spring Boot成体系的东西真的太少了, 只找到了翟勇超写的一个系列 下面对Spring Boot中我认为比较特色的归纳 启动类@SpringBootApplication 开启组件扫描和自动配置. 其中包含三个常用注解: @Configuration : 表明基于Java配置 @ComponentScan : 开启组件扫描, 自动发现控制器类和其他组件并注册为应用程序上下文的Bean @EnableAutoConfiguration : 开启Spring Boot自动配置功能 所以它一般加在需要启动引导和自动配置的类 负责启动引导应用程序 1SpringApplication.run(Object source, String… args); 一 起步依赖Gradle和Maven都是Spring Boot为应用程序提供的构建工具 12345678910111213&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;{springBootVersion}&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; ...&lt;/dependencies&gt; 父起步依赖:从spring-boot-starter-parent中继承版本号, 把它作为上一级, 利用Maven的依赖管理. 所以在下面的dependency中就不用写版本了. 除了手工添加的一些以外, 其他是’spring-boot-starter-‘打头的都是Spring Boot的起步依赖, 有助于我们程序的构建, 如果没有这些依赖, 你可能需要考虑需要哪个功能需要哪些模块, 它们之间是否兼容等等问题. 事实上我们需要关注的, 仅仅是功能而已. 所以, 通过传递依赖, 就=加了一大堆需要的库. 此外, 还支持你排除和覆盖某个起步依赖 二 自动配置Spring Boot的自动配置是一个运行时（更准确地说，是应用程序启动时）的过程. 每当应用程序启动的时候，Spring Boot的自动配置都要做将近200个这样的决定，涵盖安全、 集成、持久化、Web开发等诸多方面。所有这些自动配置就是为了尽量不让你自己写配置。 自动配置 让我们专注于应用程序代码 三 条件化配置spring-boot-autoconfigure中提供了用于Spring MVC, Spring Data JPA, Thymeleaf等等的各种配置. 这些配置存在于应用程序配置当中, 但可以根据你的需要在满足某个条件时忽略这个配置. 例如要对JdbcTemplate进行自动配置(该条件为:只有在Classpath里存在 JdbcTemplate 时才会生效) 首先实现Condition接口, 覆盖Matches() method 1234567891011public class JdbcTemplateCondition implements Condition { @Override public boolean matches(ConditionContext context,AnnotatedTypeMetadata metadata) { try { context.getClassLoader().loadClass(\"org.springframework.jdbc.core.JdbcTemplate\"); return true; } catch (Exception e) { return false; } }} 再来声明Bean时 1234@Conditional(JdbcTemplateCondition.class) public MyService myService() { ...} 表示只有当JdbcTemplateCondition类中的条件成立(classpath中有JdbcTemplate)后才能创建myService这个Bean. @Conditional只是Spring Boot中简单的一个注解条件, 除此之外还有很多丰富的条件注解. Spring Boot自动配置承担起了配置Spring的重任，因此你能专注于编写自己的应用程序。 四: 自定义配置4.1例如我们要覆盖Spring Secuirty的安全配置,首先要创建自定义的安全配置: 123@Configuration@EnableWebSecuritypublic class SecurityConfig extends WebSecurityConfigurerAdapter {...} 就行了 这么简单? 什么原理呢? 举个例子Spring Boot的 DataSourceAutoConfiguration 中定义的 JdbcTemplate Bean 12345@Bean@ConditionalOnMissingBean(JdbcOperations.class)public JdbcTemplate jdbcTemplate() { return new JdbcTemplate(this.dataSource);} Spring Boot会跳过自动的安全配置，转而使用我们的SecurityConfig 还记得标题三中通过注解的条件配置, 其中@ConditionalOnMissingBean注解是覆盖自动配置的关键. 要求当前不存在 JdbcOperations类型（ JdbcTemplate 实现了该接口）的Bean时才生效。 如果当前已经有一个 JdbcOperationsBean了，条件即不满足，不会执行 jdbcTemplate() 方法。 什么情况下有一个了呢? Spring Boot的设计是加载应用级配置， 随后再考虑自动配置类。 因此，如果你已经配置了一个 JdbcTemplate Bean，那么在执行自动配置时就已经存在一个 JdbcOperations 类型的Bean了，于是忽略自动配置的 JdbcTemplate Bean。 4.2(常用:自动配置微调)Spring Boot自动配置的Bean提供了300多个用于微调的属性, 而且能从多种属性源获得属性. 其中就包括application.properties呀. 4.3(Profile配置)如果一个自定义的安全验证只用在生产环境,怎么做? step1.在你自定义的配置类上加入@Profile(value=”production”) step2.在application.properties中加入spring.profiles.active=production 注意: 如果没有找到. 这个自定义的安全配置就会被忽略! 如果每个环境分别需要不同的配置,怎么做? 例如生产环境中，你只关心 WARN 或更高级别的日志项，想把日志写到日志文件里。 在开发环境中，你只想把日志输出到控制台，记录 DEBUG或更高级别。 创建额外的properties文件, 遵守application-{profile}.properties的命名规范: 123456application-development.properties 开发环境logging.level.root=DEBUGapplication-production.properties 生产环境logging.path=/var/logs/logging.file=BookWorm.loglogging.level.root=WARN","link":"/2016/11/01/SpringBoot 初探/"},{"title":"easyui下拉选择树(ComboTree)","text":"前言 很多时候都会用到下拉框,有时候为了显示更好的效果,会用到树型下拉框,实际就是下拉框中带了一个树型的控件,所以它自然继承自tree和combobox. 后台没什么问题, 但是easyui我是真的不会阿 一: 前台1 首先从前台来讲, 先把html元素写好 12&lt;select id=\"cc\" value=\"01\" style=\"width:200px;\" &gt;&lt;/select&gt;&lt;a href=\"#\" id=\"btn\" class=\"easyui-linkbutton\" onclick=\"getValue()\"&gt;getValue&lt;/a&gt; 这里用js加载方式: 123$('#cc').combotree({ url:'tree_data.json',}); 这里的url是跟html统一文件夹下的一个json格式文件.是一个模拟的json数据.大致结构如下: 1234567891011[{ id: 1, text: 'Languages', children: [{ id: 11, text: 'Java' },{ id: 12, text: 'C++' }]}] 一个简单基础的下拉树就形成了: 在下拉树旁边写了一个按钮, 这个按钮有一个普通的onclick事件,就是获取下拉树的选择的值.代码如下: 1234function getValue(){ var value = $('#cc').combotree('getValue'); alert(value);} 还有下拉树实现多选: 实际就是加了个multiple属性,代码如下 123456$('#ccm').combotree({ url:'tree_data.json', multiple: true, //cascadeCheck: false //onlyLeafCheck: true}); 如果要获取它选择的值只需要把前面呢个getValue换成getValues var value = $(‘#ccm’).combotree(‘getValues’);//会返回id值的数组,逗号连接这种方式getValues默认是级联的,也就是选择子节点,他的父节点也可能get上,所以有一个属性cascadeCheck. cascadeCheck是定义是否层叠选中状态,也就是前面的小方框会不会自动级联,如图变成false就是不级联,但是这样很怪. onlyLeafCheck是定义是否只在末级节点之前显示复选框,也就是只有子节点才会有复选框,也就是只能选择叶子节点,没什么级联了. 二:数据假如下拉树是要选择班级下的学生信息, 那么随便造个数据. 三:后台既然前台的url接收是一个json格式的东西.可以看到这里面有三个关键属性, id, text, children.三者缺一不可. 那数据中的两个表在程序中的persistence层对应着两个实体. Entity Class和Entity Student. 除此之外,还需要一个Model. 1234567public class ComboTreeModel { private String id; private String text; private List&lt;ComboTreeModel&gt; children; //setter and getter} 而最后传给url的,应该是json格式的: 下面就是通过业务关联, 将两个实体组合,查出需要数据,放到Model模型. 123456789101112131415161718192021222324252627282930313233@RequestMapping(value=\"/getSelectData\")@ResponseBodypublic List&lt;ComboTreeModel&gt; getTreeData(HttpServletRequest request, HttpServletResponse response,ComboTreeModel treeModel){ List classList = this.studentService.queryClassList(); //[{eventid:\"c001\",classname:'奥赛班',eventid:\"c002\",classname:'直播班',eventid:\"c003\",classname:'火箭班'}] List&lt;ComboTreeModel&gt; list = new ArrayList&lt;ComboTreeModel&gt;(); for(int i=0; i&lt;classList.size(); i++){ ComboTreeModel model = new ComboTreeModel(); Map classMap = (Map) classList.get(i); //{eventid:\"c001\",classname:'奥赛班'} String classId = (String) classMap.get(\"eventid\"); String className = (String)classMap.get(\"classname\"); model.setId(classId); model.setText(className); List studentList = this.studentService.queryByClassId(classId); if(studentList.size() &gt; 0){ List&lt;ComboTreeModel&gt; childrenList = new ArrayList&lt;ComboTreeModel&gt;(); for(int j = 0;j&lt;studentList.size();j++){ ComboTreeModel model2 = new ComboTreeModel(); Map studentMap = (Map)studentList.get(j); String studentEventId = (String) studentMap.get(\"eventid\"); String studentName = (String) studentMap.get(\"studentname\"); model2.setId(studentEventId); model2.setText(studentName); childrenList.add(model2); } model.setChildren(childrenList); } list.add(model); } return list;} ResponseBody 会自动返回JSON格式的数据.","link":"/2016/09/04/easyui 下拉选择树(ComboTree) 的实现/"},{"title":"freemarker导出复杂Excel","text":"[TOC] 序言用Freemarker做Excel导出确实很容易. 但是导出复杂Excel, 例如多行合并的还是费了一天时间 步骤首先是pom依赖, 构建工具使用Maven 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-freemarker&lt;/artifactId&gt;&lt;/dependency&gt; 准备模板接下来, 制作ftl模板 在Office中编辑一个Excel文件, 这就是最后要生成的模板. 注意: 另存为xml格式. 不要直接改后缀名 拷贝到项目下, 修改后缀名为.ftl (这里我放到resources/templates目录下) 用xml方式打开这个.ftl文件. 找到如下代码 123456789101112131415161718&lt;Table ss:ExpandedColumnCount=\"4\" ss:ExpandedRowCount=\"3\" x:FullColumns=\"1\"x:FullRows=\"1\" ss:DefaultColumnWidth=\"54\" ss:DefaultRowHeight=\"13.5\"&gt; &lt;Row&gt; &lt;Cell ss:MergeAcross=\"3\" ss:StyleID=\"s66\"&gt;&lt;Data ss:Type=\"String\"&gt;人员列表&lt;/Data&gt;&lt;/Cell&gt; &lt;/Row&gt; &lt;Row&gt; &lt;Cell ss:StyleID=\"s67\"/&gt; &lt;Cell ss:StyleID=\"s67\"&gt;&lt;Data ss:Type=\"String\"&gt;name&lt;/Data&gt;&lt;/Cell&gt; &lt;Cell ss:StyleID=\"s67\"&gt;&lt;Data ss:Type=\"String\"&gt;age&lt;/Data&gt;&lt;/Cell&gt; &lt;Cell ss:StyleID=\"s67\"&gt;&lt;Data ss:Type=\"String\"&gt;address&lt;/Data&gt;&lt;/Cell&gt; &lt;/Row&gt; &lt;Row&gt; &lt;Cell ss:StyleID=\"s67\"&gt;&lt;Data ss:Type=\"Number\"&gt;1&lt;/Data&gt;&lt;/Cell&gt; &lt;Cell ss:StyleID=\"s67\"&gt;&lt;Data ss:Type=\"String\"&gt;zhangsan&lt;/Data&gt;&lt;/Cell&gt; &lt;Cell ss:StyleID=\"s67\"&gt;&lt;Data ss:Type=\"Number\"&gt;22&lt;/Data&gt;&lt;/Cell&gt; &lt;Cell ss:StyleID=\"s67\"&gt;&lt;Data ss:Type=\"String\"&gt;BeiJing&lt;/Data&gt;&lt;/Cell&gt; &lt;/Row&gt;&lt;/Table&gt; 主要看上面这段, 对比你的Excel很容易看出: 每个就是一行, 每个是一个单元格 找到要循环的一行, 添加表达式和标签 关于Freemarker的语法, 可以参考http://freemarker.foofun.cn/index.html 加完标签后如下: 1234567891011121314151617181920212223&lt;Table ss:ExpandedColumnCount=\"4\" ss:ExpandedRowCount=\"${userListSize}\" x:FullColumns=\"1\"x:FullRows=\"1\" ss:DefaultColumnWidth=\"54\" ss:DefaultRowHeight=\"13.5\"&gt;&lt;Row&gt; &lt;Cell ss:MergeAcross=\"3\" ss:StyleID=\"s66\"&gt;&lt;Data ss:Type=\"String\"&gt;人员列表&lt;/Data&gt;&lt;/Cell&gt;&lt;/Row&gt;&lt;Row&gt; &lt;Cell ss:StyleID=\"s67\"/&gt; &lt;Cell ss:StyleID=\"s67\"&gt;&lt;Data ss:Type=\"String\"&gt;姓名&lt;/Data&gt;&lt;/Cell&gt; &lt;Cell ss:StyleID=\"s67\"&gt;&lt;Data ss:Type=\"String\"&gt;年龄&lt;/Data&gt;&lt;/Cell&gt; &lt;Cell ss:StyleID=\"s67\"&gt;&lt;Data ss:Type=\"String\"&gt;地址&lt;/Data&gt;&lt;/Cell&gt;&lt;/Row&gt;&lt;#assign index = 0 &gt;&lt;#list userList as u&gt;&lt;#assign index = index + 1 &gt; &lt;Row&gt; &lt;Cell ss:StyleID=\"s67\"&gt;&lt;Data ss:Type=\"Number\"&gt;${index}&lt;/Data&gt;&lt;/Cell&gt; &lt;Cell ss:StyleID=\"s67\"&gt;&lt;Data ss:Type=\"String\"&gt;${u.name}&lt;/Data&gt;&lt;/Cell&gt; &lt;Cell ss:StyleID=\"s67\"&gt;&lt;Data ss:Type=\"Number\"&gt;${u.age}&lt;/Data&gt;&lt;/Cell&gt; &lt;Cell ss:StyleID=\"s67\"&gt;&lt;Data ss:Type=\"String\"&gt;${u.address}&lt;/Data&gt;&lt;/Cell&gt; &lt;/Row&gt;&lt;/#list&gt;&lt;/Table&gt; 注意ExpandedRowCount的值可以变量传进来, 如果不好计算可以写一个很大的值. 自己写的值有什么影响我没测试… 准备数据源对于你要动态生成的List, 里面可以放Map, 也可以放对象, 都行. 这里我先用Map, 等下用对象 模拟获取数据 12345678910111213141516171819/** * 构造user数据List&lt;Map&lt;String, Object&gt;&gt; */private static List&lt;Map&lt;String, Object&gt;&gt; getUserList(){ List&lt;Map&lt;String, Object&gt;&gt; returnList = new ArrayList&lt;Map&lt;String, Object&gt;&gt;(); Map&lt;String,Object&gt; map1 = new HashMap&lt;String, Object&gt;(); Map&lt;String,Object&gt; map2 = new HashMap&lt;String, Object&gt;(); map1.put(\"name\", \"张三\"); map1.put(\"age\", \"18\"); map1.put(\"address\", \"广东\"); map2.put(\"name\", \"王五\"); map2.put(\"age\", \"22\"); map2.put(\"address\", \"北京\"); returnList.add(map1); returnList.add(map2); return returnList;} 控制器 12345678910111213141516171819202122232425262728293031@SuppressWarnings(\"unchecked\")@RequestMapping(value=\"/exportExcel\", method=RequestMethod.GET)public void exportExcelByFreeMarker(HttpServletRequest request, HttpServletResponse response) { try { List&lt;Map&lt;String, Object&gt;&gt; userList= this.getUserList(); configuration.setDefaultEncoding(\"UTF-8\"); configuration.setTemplateUpdateDelayMilliseconds(0); configuration.setTemplateExceptionHandler(TemplateExceptionHandler.RETHROW_HANDLER); //获取模板 Template template = configuration.getTemplate(\"userlist.ftl\"); Map&lt;String,Object&gt; root = new HashMap&lt;String,Object&gt;(); root.put(\"userList\", userList); root.put(\"userListSize\", String.valueOf(userList.size()+2)); SimpleDateFormat sdf = new SimpleDateFormat(\"yyyyMMddHHmmss\"); Date today = new Date(); String fileName = \"人员列表\" + sdf.format(today);//以今天的日期为文件名 response.setContentType(\"application/msexcel;charset=UTF-8\"); response.setHeader(\"Content-disposition\",\"attachment;filename=\\\"\"+new String((fileName+\".xls\").getBytes(\"GBK\"),\"ISO8859-1\")+\"\\\"\"); //response字符流转换成字节流，template需要字节流作为输出 OutputStream outputStream = response.getOutputStream(); OutputStreamWriter outputWriter = new OutputStreamWriter(outputStream,\"UTF-8\"); Writer writer = new BufferedWriter(outputWriter); template.process(root, writer); writer.flush(); writer.close(); } catch (Exception e) { e.printStackTrace(); }} 在浏览器中输入请求地址就可以导出了. 复杂Excel生成Excel也有很多复杂的情况, 比如带图片, 跨行合并等. 这里例多行合并的, 例如: 生成的xml: 123456789101112131415161718192021222324&lt;Table ss:ExpandedColumnCount=\"5\" ss:ExpandedRowCount=\"99999\" x:FullColumns=\"1\" x:FullRows=\"1\" ss:DefaultColumnWidth=\"54\" ss:DefaultRowHeight=\"13.5\"&gt; &lt;Row&gt; &lt;Cell ss:MergeAcross=\"3\" ss:StyleID=\"s66\"&gt;&lt;Data ss:Type=\"String\"&gt;人员列表&lt;/Data&gt;&lt;/Cell&gt; &lt;Cell ss:StyleID=\"s62\"/&gt; &lt;/Row&gt; &lt;Row&gt; &lt;Cell ss:StyleID=\"s67\"/&gt; &lt;Cell ss:StyleID=\"s67\"&gt;&lt;Data ss:Type=\"String\"&gt;姓名&lt;/Data&gt;&lt;/Cell&gt; &lt;Cell ss:StyleID=\"s67\"&gt;&lt;Data ss:Type=\"String\"&gt;年龄&lt;/Data&gt;&lt;/Cell&gt; &lt;Cell ss:StyleID=\"s67\"&gt;&lt;Data ss:Type=\"String\"&gt;孩子&lt;/Data&gt;&lt;/Cell&gt; &lt;Cell ss:StyleID=\"s69\"&gt;&lt;Data ss:Type=\"String\"&gt;地址&lt;/Data&gt;&lt;/Cell&gt; &lt;/Row&gt; &lt;Row&gt; &lt;Cell ss:MergeDown=\"1\" ss:StyleID=\"m87387632\"&gt;&lt;Data ss:Type=\"Number\"&gt;1&lt;/Data&gt;&lt;/Cell&gt; &lt;Cell ss:MergeDown=\"1\" ss:StyleID=\"m87387612\"&gt;&lt;Data ss:Type=\"String\"&gt;张三&lt;/Data&gt;&lt;/Cell&gt; &lt;Cell ss:MergeDown=\"1\" ss:StyleID=\"m87387592\"&gt;&lt;Data ss:Type=\"Number\"&gt;22&lt;/Data&gt;&lt;/Cell&gt; &lt;Cell ss:StyleID=\"s67\"&gt;&lt;Data ss:Type=\"String\"&gt;小张&lt;/Data&gt;&lt;/Cell&gt; &lt;Cell ss:MergeDown=\"1\" ss:StyleID=\"m87387572\"&gt;&lt;Data ss:Type=\"String\"&gt;北京&lt;/Data&gt;&lt;/Cell&gt; &lt;/Row&gt; &lt;Row&gt; &lt;Cell ss:Index=\"4\" ss:StyleID=\"s67\"&gt;&lt;Data ss:Type=\"String\"&gt;二张&lt;/Data&gt;&lt;/Cell&gt; &lt;/Row&gt;&lt;/Table&gt; 仔细观察这段代码, 跟上面的不带合并的xml进行对比: 每个带合并的Cell都带有一个属性MergeDown, 它的值为 (合并格数-1) 合并的格数是通过一个对象的循环来得到, 比如这里的”孩子”字段 扩展的Row会有一个Index属性, 它的值就是字段的列数, 这里是4 分析好之后开始加标签: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546 &lt;Table ss:ExpandedColumnCount=\"5\" ss:ExpandedRowCount=\"4\" x:FullColumns=\"1\" x:FullRows=\"1\" ss:DefaultColumnWidth=\"54\" ss:DefaultRowHeight=\"13.5\"&gt; &lt;Row&gt; &lt;Cell ss:MergeAcross=\"3\" ss:StyleID=\"s66\"&gt;&lt;Data ss:Type=\"String\"&gt;人员列表&lt;/Data&gt;&lt;/Cell&gt; &lt;Cell ss:StyleID=\"s62\"/&gt; &lt;/Row&gt; &lt;Row&gt; &lt;Cell ss:StyleID=\"s67\"/&gt; &lt;Cell ss:StyleID=\"s67\"&gt;&lt;Data ss:Type=\"String\"&gt;姓名&lt;/Data&gt;&lt;/Cell&gt; &lt;Cell ss:StyleID=\"s67\"&gt;&lt;Data ss:Type=\"String\"&gt;年龄&lt;/Data&gt;&lt;/Cell&gt; &lt;Cell ss:StyleID=\"s67\"&gt;&lt;Data ss:Type=\"String\"&gt;孩子&lt;/Data&gt;&lt;/Cell&gt; &lt;Cell ss:StyleID=\"s69\"&gt;&lt;Data ss:Type=\"String\"&gt;地址&lt;/Data&gt;&lt;/Cell&gt; &lt;/Row&gt; &lt;#assign index = 0 &gt;&lt;#list userBoList as u&gt;&lt;#assign index = index + 1 &gt;&lt;#assign num = u.children?size-1&gt;&lt;#if num lt 0&gt;&lt;#assign num = 0&gt;&lt;/#if&gt; &lt;Row&gt; &lt;Cell ss:MergeDown=\"${num}\" ss:StyleID=\"m87387632\"&gt;&lt;Data ss:Type=\"Number\"&gt;${index}&lt;/Data&gt;&lt;/Cell&gt; &lt;Cell ss:MergeDown=\"${num}\" ss:StyleID=\"m87387612\"&gt;&lt;Data ss:Type=\"String\"&gt;${u.name}&lt;/Data&gt;&lt;/Cell&gt; &lt;Cell ss:MergeDown=\"${num}\" ss:StyleID=\"m87387592\"&gt;&lt;Data ss:Type=\"Number\"&gt;${u.age}&lt;/Data&gt;&lt;/Cell&gt; &lt;!-- 在children的List中取出第一个 --&gt; &lt;#list u.children as firstChildren&gt; &lt;#if firstChildren_index == 0&gt; &lt;Cell ss:StyleID=\"s67\"&gt;&lt;Data ss:Type=\"String\"&gt;${firstChildren.name}&lt;/Data&gt;&lt;/Cell&gt; &lt;/#if&gt; &lt;/#list&gt; &lt;Cell ss:MergeDown=\"${num}\" ss:StyleID=\"m87387572\"&gt;&lt;Data ss:Type=\"String\"&gt;${u.address}&lt;/Data&gt;&lt;/Cell&gt; &lt;/Row&gt; &lt;!-- 从第一个之后开始取 --&gt; &lt;#list u.children as c&gt; &lt;#if (u.children?size &gt; 1 &amp;&amp; c_index &gt; 0 )&gt; &lt;Row&gt; &lt;Cell ss:Index=\"4\" ss:StyleID=\"s67\"&gt;&lt;Data ss:Type=\"String\"&gt;${c.name}&lt;/Data&gt;&lt;/Cell&gt; &lt;/Row&gt; &lt;/#if&gt; &lt;/#list&gt; &lt;/#list&gt; &lt;/Table&gt; 注意:MergeDown这个属性中判断了是否小于0, 亲测在office中&lt;0会导致Excel报错, WPS不会, 最好还是判断一下ExpandedRowCount可以给个很大的值, 最好还是通过程序来传大小 12345在这个Excel中需要两个List:User: List&lt;UserBo&gt;Children: List&lt;ChildrenBo&gt;User -&gt; Children : 1-&gt;N也可以是Map: List&lt;Map&lt;String, Object&gt;&gt;","link":"/2017/04/20/freemarker导出复杂Excel/"},{"title":"Shell脚本方便微服务部署","text":"序言 之前作为一个开发狗. 并不太多涉及Linux系统管理. 对shell脚本也一无所知. 但是对于jar包的部署和打包是要经常在Linux环境下的. 对于极度懒人的我来说过于繁琐, 每次都要命令来查询, 复制粘贴执行. 能让计算机做的为什么要我们自己做呢哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈 只是为了解决当务之需才去研究shell. 并没有系统完整的学习shell 目前在测试和开发环境已经有十多个脚本来负责服务的批量和单个操作, 用起来得心应手, 能够实现: Git远端拉取指定分支的服务 mvn打包编译 单个/批量启停 关于shellshell用C语言编写, 是Linux管理中必不可少的. 也是系统和硬件交互的一种介质. 我们通过指令传给shell, shell再通过系统内核就能实现对计算机硬件的操作. 他有自己的命令和程序设计语言, 供我们编写, 在我看来, 如同PL/SQL编程 我们需要了解他的语法. shell需要的环境超级简单: 文本编辑器 脚本解释器 所以在Linux环境下可以轻而易举的编写shell 部署脚本接下来按照我要实现的目的, 选择性的记录一些命令 杀死服务进程要杀死服务的进程, 需要先找到他的网络连接信息. 所以通常做法是先显示:程序 1ps -aux|grep java 程序的标识, 状态, ID, 资源占用等信息就会显示出来 再找到进程的ID, 通过kill ID 来杀死这个服务 lsof命令 这里要说的是lsof命令: 很好的lsof命令介绍 在这里我只用到 -i, -t, -sTCP -i:[port]: 显示与指定端口相关的网络信息-t: 仅获取进程的ID-iTCP: 仅显示TCP连接(UDP同理) 找出监听状态的端口: 1lsof -i -sTCP:LISTEN 使用grep命令 也可以获得: 1lsof -i | grep -i LISTEN 所以要杀死某个启动服务的进程如下: 1kill -15 `lsof -i:[端口号] -t -sTCP:LISTEN` 再通过一个简单的if判断来确定进程已经杀死 123456kill -15 `lsof -i:[端口号] -t -sTCP:LISTEN`sleep 1if [ -z $(lsof -i:[端口号] -t -s TCP:LISTEN) ]then echo \" port killed! \"fi 在这里, 我写一个通过传参杀死服务进程的脚本 killed_single_port.sh 123456789101112131415161718192021222324#!/bin/bashprintf \" Please input killed port: \"read portport_array=(8901 8902 8903 8905 8906 8907 8908 8909 8761 8050)result=`echo \"${port_array[@]}\" | grep -wq \"$port\" &amp;&amp; echo \"y\" || echo \"n\"`if [ \"n\" == \"${result}\" ]then echo \"port:\\\"${port}\\\" not exits ! \" exit 5fikill -15 `lsof -i:${port} -t -sTCP:LISTEN`sleep 1if [ -z $(lsof -i:${port} -t -s TCP:LISTEN) ]then echo \" port \\\"${port}\\\" is killed! \"fiexit 5 启动服务通常, 用nohup命令来不挂断的运行命令来启动一个我们的服务 nohup命令 在Linux/Unix启动程序中, 我们一般希望是在后台运行, 在末尾加”&amp;”即可 1nohup java -jar /aa/bb/xxx.jar 如果需要程序的启动信息 通过”&gt;”重定向到日志中 0、1和2分别表示标准输入、标准输出和标准错误信息输出，可以用来&gt; 指定需要重定向的标准输入或输出。 0: 标准输入信息 1: 标准输出信息 2: 标准错误输出信息 /dev/null: 正常输出和错误信息都不显示 例如: 1234nohup java -jar /aa/bb/xxx.jar \\--server.port=8050 \\--eureka.client.serviceUrl.defaultZone=http://127.0.0.1:8761/eureka/ \\&gt;/my_log/zuul.log 2&gt;/my_log/zuul_error.log &amp; 如果要批量启动几个程序, 这几个服务之间可能有关系. 如果把这几个程序的启动命令写到一个脚本中顺序执行肯定不行. 所以这里又用到lsof命令来获得一个程序的状态, 进而来判断是否继续执行 以下script实现判断当x服务启动以后, 再顺序执行a,b程序. 最后再判断a,b是否启动成功 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#!/bin/bashecho \"x server booting...\"nohup java -jar /aa/bb/x.jar &amp;&gt;/dev/null &amp;# 判断9900的x服务启动与否while [ -z \"$(lsof -i:9000 -t -s TCP:LISTEN)\" ] do sleep 3doneif [ -n \"$(lsof -i:9000 -t -s TCP:LISTEN)\" ]then echo \"x server start success!\"else echo \"ERROR: x server boot failure !!!! \" exit 5fi# 只有当9900的x服务启动成功后, 继续执行下面# 启动a服务nohup java -jar /aa/bb/a_server.jar \\--server.port=8901 \\&gt;/my_log/a.log 2&gt;/my_log/a_error.log &amp;# 启动b服务nohup java -jar /aa/bb/b_server.jar \\--server.port=8902 \\&gt;/my_log/b.log 2&gt;/my_log/b_error.log &amp;# 判断a(port:8901),b(port:8902)服务是否启动成功i=2a_server=1b_server=1while [ 1 -gt 0 ] &amp;&amp; [ ${i} -gt 0 ]do if [ -n \"$(lsof -i:8901 -t -s TCP:LISTEN)\" ] ] then echo \"a server start success !!!\" i=$[ ${i} - 1 ] fi if [ -n \"$(lsof -i:8902 -t -s TCP:LISTEN)\" ] ] then echo \"b server start success !!!\" i=$[ ${i} - 1 ] fidone","link":"/2017/02/15/shell脚本之方便微服务部署/"},{"title":"明日黄花Struts2","text":"[TOC] 虽然Struts2已成为明日黄花 但还是决定来学习一下, 体会设计思想, 实用为辅. Struts2 登录 需要注意: Struts2需要运行在JRE1.5及以上 1 创建Java Web项目 2 引入Struts2的依赖包. 将依赖包放在WEB-INFO的lib目录下 commons-logging-1.0.4.jar freemarker-2.3.15.jar ognl-2.7.3.jar struts2-core-2.1.8.1.jar xwork-core-2.1.6.jar commons-fileupload-1.2.1.jar 3 在web.xml配置文件中, 配置StructPrepareAndExecuteFilter或FilterDispatcher 123456789&lt;filter&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;filter-class&gt;org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; 4 提供Struts2配置文件struts.xml, 放在src下. 5 建立JSP(login.jsp, login_success.jsp, login_error.jsp) 6 创建Struts2的Action, Struts2的Action可以不用继承Struts2框架中的任意一个类.也不用实现Struts2框架中的任何接口, 所以Struts2的Action可以是一个POJO(纯粹的Java对象)所以Struts2的Action测试更容易.Struts2缺省方法名称: public String execute() throw Exception; 7 在Action中提供getter和setter方法, 便于收集数据(这样收集数据的模式一般可以称为属性驱动模式) 8 将JSP和Action配置到struts.xml配置文件中 9 了解structs-default.xml配置文件, default.properties, Struts2的默认后缀是: .action struts2-core-2.1.8.1.jar/structs-default.xml struts2-core-2.1.8.1.jar/org.apache.struts2/default.properties Struts2的常用配置1 标签的name属性, 如果不配置, 缺省为success.2 Struts2提供了一个Action接口, 在Action接口中定义了一些常量和execute方法, 我们可以使用该接口, 使得开发更加规范.3 常用配置参数: (在default.properties配置文件中能找到) struts.configuration.xml.reload=true–当struts.xml配置文件发生更改, 会立刻加载, 一般在开发环境, 在生产环境别配 * struts.devMode=true– 会提供更加友好的错误提示 以上配置方式有两种: 在struts.properties中配置 在struts.xml配置文件中(建议) Struts2对团队开发的支持(多配置文件)1 可以为某个模块建立单独的配置文件, 该配置文件的格式需要和struts.xml文件的格式一样2 在struts.xml文件中用标签引入 Struts2对ModelDriven模式的支持Struts2可以采用类似于Struts1中的ActionForm方式收集数据, 这样的方式叫做ModelDriven模式 如何实现模型驱动模式? 创建User Action需要实现ModelDriven接口 实现getModel()方法, 返回Bean对象 Struts2中直接对Action中的对象进行赋值(不new)在html中可以通过如下方式命名输入域: 12345&lt;form action=\"login.action\"&gt; 用户：&lt;input type=\"text\" name=\"user.username\"&gt;&lt;br&gt; 密码：&lt;input type=\"password\" name=\"user.password\"&gt;&lt;br&gt; &lt;input type=\"submit\" value=\"登录\"&gt; &lt;/form&gt; Struts2的Action访问Servlet API方式一:1 可以通过ActionContext访问Servlet API，此种方式没有侵入性 2 在Struts2中默认为转发，也就是标签中的type=”dispatcher”，type的属性可以修改为重定向. Struts的重定向有两种： type=”redirect”,可以重定向到任何一个web资源，如：jsp或Action 如果要重定向到Action，需要写上后缀：xxxx.action type=”redirectAction”,可以重定向到Action，不需要写后缀，此种方式更通用些,不会因为后缀的改变影响配置 3 关于Struts2的type类型，也就是Result类型，他们都实现了共同的接口Result，都实现了execute方法 他们体现了策略模式，具体Result类型参见：struts-default.xml文件： 123456789101112&lt;result-types&gt; &lt;result-type name=\"chain\" class=\"com.opensymphony.xwork2.ActionChainResult\"/&gt; &lt;result-type name=\"dispatcher\" class=\"org.apache.struts2.dispatcher.ServletDispatcherResult\" default=\"true\"/&gt; &lt;result-type name=\"freemarker\" class=\"org.apache.struts2.views.freemarker.FreemarkerResult\"/&gt; &lt;result-type name=\"httpheader\" class=\"org.apache.struts2.dispatcher.HttpHeaderResult\"/&gt; &lt;result-type name=\"redirect\" class=\"org.apache.struts2.dispatcher.ServletRedirectResult\"/&gt; &lt;result-type name=\"redirectAction\" class=\"org.apache.struts2.dispatcher.ServletActionRedirectResult\"/&gt; &lt;result-type name=\"stream\" class=\"org.apache.struts2.dispatcher.StreamResult\"/&gt; &lt;result-type name=\"velocity\" class=\"org.apache.struts2.dispatcher.VelocityResult\"/&gt; &lt;result-type name=\"xslt\" class=\"org.apache.struts2.views.xslt.XSLTResult\"/&gt; &lt;result-type name=\"plainText\" class=\"org.apache.struts2.dispatcher.PlainTextResult\" /&gt; &lt;/result-types&gt; 我们完全可以自己根据需求扩展Result类型 4 全局Result和局部Result 方式二: 可以通过实现装配接口, 完成对Servlet-API的访问 123* ServletRequestAware取得HttpServletRequest对象 * ServletResponseAware取得HttpServletResponse对象 * ServletContextAware取得ServletContext对象(工具类) 方式三:可以通过ServletActionContext提供的静态方法取得Servlet API getPageContext(); getRequest(); getResponse(); getServletContext(); Struts2命名空间设置: 采用命名空间, 可以区分不同包下的相同Action名称. 如果package的namespace属性没有设定, 使用默认的命名空间为””. Struts2中Action的完整路径为: namespace + Action的名称. 寻找原则: 首先在指定的命名空间下寻找Action, 如果找到了就使用此Action, 如果没有找到, 就在上层目录中查找, 一直到根(缺省命名空间), 找到则使用此Action, 没找到就抛出Action没找到异常. Struts2的字符集设置:123456789101112* struts.properties配置文件中: struts.i18n.encoding * struts.xml配置文件: &lt;constant name=\"struts.i18n.encoding\" value=\"GB18030\"/&gt; * 在StrutsPrepareAndExecuteFilter中配置: &lt;filter&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;filter-class&gt;org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;struts.i18n.encoding&lt;/param-name&gt; &lt;param-value&gt;GB18030&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; 注: 这个配置只针对表单提交为post方式, 如果提交方式为get, 则需要在tomcat_home/conf/server.xml文件中在8080port呢个位置后面加个URIEncoding=”GB18030”. Struts2的Action包含多个方法时如何调用 (方法的动态调用)具体的调用方式:1 方法的动态调用2 在中配置method属性3 使用通配符 方式一: 方法的动态调用 12345[action名称] + ! + 方法名称 + 后缀 如: &lt;a href=&quot;user!add.action&quot;&gt;添加用户&lt;/a&gt;&lt;br&gt; &lt;a href=&quot;user!del.action&quot;&gt;删除用户&lt;/a&gt;&lt;br&gt; &lt;a href=&quot;user!update.action&quot;&gt;修改用户&lt;/a&gt;&lt;br&gt; &lt;a href=&quot;user!list.action&quot;&gt;查询用户&lt;/a&gt;&lt;br&gt; Action不用实现Action接口, 只需继承ActionSupport(它实现了Actio接口) 动态调用的参数配置, 默认为true, 可以调用, 为false表示禁用: 1&lt;constant name=\"struts.enable.DynamicMethodInvocation\" value=\"false\" /&gt; Action中的所有方法最好和execute方法一致(参数, 返回值, 异常 最好都一致) 方式二: 在标签中配置method属性 添加method属性, jsp页面要写name名, 而不是method名 12345678910111213141516171819202122232425262728293031&lt;pre name=\"code\" class=\"html\"&gt;&lt;!-- &lt;package name=\"user-package\" extends=\"struts-default\"&gt; &lt;action name=\"add\" class=\"com.xie.struts2.UserAction\" method=\"add\"&gt; &lt;result&gt;/success.jsp&lt;/result&gt; &lt;/action&gt; &lt;action name=\"delete\" class=\"com.xie.struts2.UserAction\" method=\"delete\"&gt; &lt;result&gt;/success.jsp&lt;/result&gt; &lt;/action&gt; &lt;action name=\"update\" class=\"com.xie.struts2.UserAction\" method=\"update\"&gt; &lt;result&gt;/success.jsp&lt;/result&gt; &lt;/action&gt; &lt;action name=\"list\" class=\"com.xie.struts2.UserAction\" method=\"list\"&gt; &lt;result&gt;/success.jsp&lt;/result&gt; &lt;/action&gt; &lt;/package&gt; --&gt; &lt;package name=\"user-package\" extends=\"struts-default\"&gt; &lt;action name=\"addUser\" class=\"com.xie.struts2.UserAction\" method=\"add\"&gt; &lt;result&gt;/success.jsp&lt;/result&gt; &lt;/action&gt; &lt;action name=\"deleteUser\" class=\"com.xie.struts2.UserAction\" method=\"delete\"&gt; &lt;result&gt;/success.jsp&lt;/result&gt; &lt;/action&gt; &lt;action name=\"updateUser\" class=\"com.xie.struts2.UserAction\" method=\"update\"&gt; &lt;result&gt;/success.jsp&lt;/result&gt; &lt;/action&gt; &lt;action name=\"listUser\" class=\"com.xie.struts2.UserAction\" method=\"list\"&gt; &lt;result&gt;/success.jsp&lt;/result&gt; &lt;/action&gt; &lt;/package&gt; 方式三: 使用通配符 具体配置: 123&lt;action name=\"*User\" class=\"com.xie.struts2.UserAction\" method=\"{1}\"&gt; &lt;result&gt;/{1}Success.jsp&lt;/result&gt; &lt;/action&gt; 在Struts2的标签中的class, name, method和result标签上都可以使用通配符通配符的作用就是为了减少配置量, 通配符需要建立在约定的基础上. Struts2的上传1 Struts2默认采用了apache commons-fileupload.2 Struts2支持三种类型的上传组件3 需要引入commons-fileupload相关依赖包 commons-io-1.3.2.jar commons-fileupload-1.2.1.jar 4 表单中需要采用post提交方式, 编码方式需要是: multipart/form-data5 Struts2的Action 取得文件名称: 输入域的名称 + 固定字符串”FileName” 取得文件数据: File输入域的名称 取得内容类型: 输入域的名称 + 固定字符串”ContentType” 6 得到输入流, 通过输出流写出文件 Struts2的类型转换如何实现Struts2的类型转换器?1 继承StrutsTypeConverter2 覆盖convertFromString和convertToString方法 注册类型转换器:这里我们手动编写一个转换日期的转换器. 局部类型转换器:只对当前Action起作用, 需要提供如下配置文件: [MyActionName]-conversion.properties–[MyActionName]指需要使用转换器的Action名称(不是转换器的名称) –conversion.properties”固定字符串, 不能修改. 例如: 我们addUserAction类型转换器的配置文件名称应该为: AddUserAction-conversion.properties.该配置文件一定要和该Action放在同一个目录中, 该配置文件的格式为:Action中的属性名称=转换器的完整路径如: birthday=com.xie.struts2.UtilDateConverter 全局类型转换器:可以对所有的Action起作用(同Struts1的类型转换器), 需要提供如下配置文件:xwork-conversion.properties, 该配置文件需要放在src目录下该配置文件的格式: 需要转换的类型的完整路径=转换器的完整路径.如: java.util.Date=com.xie.struts2.UtilDateConverter 如果全局类型转换器和局部类型转换器同时存在, 则局部优先. Struts2的标签库就一个, 类似于JSTL, 默认以s开头. 12引入: &lt;%@ taglib uri=&quot;/struts-tags&quot; prefix=&quot;s&quot;%&gt; 读取: &lt;s:property value=&quot;birthday&quot; /&gt; 采用标签读取属性, 才能调用转换器的convertToString方法, 采用JSTL就不会. Struts2对国际化的支持1 Locale是由国家和语言代码构成的2 国际化资源文件是由: baseName + locale.properties3 国际化的内容: 在页面中的硬编码 动态文本(提示或错误) 4 配置baseName: 1&lt;constant name=\"struts.custom.i18n.resources\" value=\"Message\"/&gt; 5 提供国际化资源文件: Message.properties Message_zh_CN.properties Message_en_US.properties 6 在开发阶段, 可以进行如下配置: 当国际化资源文件发生修改, 则立刻加载, 在生产环境下一般不要配置 1&lt;constant name=\"struts.i18n.reload\" value=\"true\"/&gt; Struts2对异常的支持(声明式异常, 自动的异常处理)局部异常 配置: 12&lt;exception-mapping result=\"error\" exception=\"com.xie.struts2.ApplicationException\" /&gt; &lt;result name=\"error\"&gt;/login_error.jsp&lt;/result&gt; 全局异常 配置: 123456&lt;global-results&gt; &lt;result name=\"global-error\"&gt;/global_error.jsp&lt;/result&gt; &lt;/global-results&gt; &lt;global-exception-mappings&gt; &lt;exception-mapping result=\"global-error\" exception=\"com.xie.struts2.ApplicationException\"&gt;&lt;/exception-mapping&gt; &lt;/global-exception-mappings&gt; 在页面可以使用EL取得异常信息 12${exception.message} --自定义错误信息${exceptionStack} --异常信息堆栈 Struts2的拦截器1 Struts2的拦截器只能拦截Action，拦截器是AOP的一种思路，可以使我们的系统架构更松散（耦合度低），可以插拔，容易互换，代码不改变的情况下很容易满足客户需求其实体现了OCP 2 如何实现拦截器？（整个拦截器体现了责任链模式，Filter也体现了责任链模式） 继承AbstractInterceptor（体现了缺省适配器模式） 实现Interceptor 3 如果自定了拦截器，缺省拦截器会失效，必须显示引用Struts2默认的拦截器 4 拦截器栈，多个拦截器的和 5 定义缺省拦截器,所有的Action都会使用 6 拦截器的执行原理，在ActionInvocation中有一个成员变量Iterator，这个Iterator中保存了所有拦截器，每次都会取得Iterator进行next,如果找到了拦截器就会执行，否则就执行Action，都执行完了拦截器出栈（其实出栈就是拦截器的intercept方法弹出栈）","link":"/2015/10/19/struts2/"},{"title":"关于Java克隆","text":"补充关于Java克隆的一点点东西 [TOC] 开始先来看一段代码 12345int a = 1;int b = a;b = 2;System.out.println(a); //1System.out.println(b); //2 a的值并没有因为复制给b而改变. 这就实现了b对a的一份拷贝. 对于byte, short, char等基本数据类型, 也同样适用. (可以理解为深拷贝) 但是如果要复制一份对象, 就无法适用了. (可以理解为浅拷贝) 何为深浅呢?一个重要的标识:是否完全复制一个新的对象，需要申请新的内存空间 对象的直接拷贝只是两个在栈不同的引用指向堆中的同一块存储空间. 所以一个引用的操作是会影响原数据的.这就是浅拷贝. 可见，对于基本类型，都是深拷贝，这由栈处理变量的机制有关； 对于自定义的对象，由于引用和对象空间分配在不同的存储中，取决于你在拷贝时是否选择在堆中去申请新的对象空间。 对于String，Integer这种，如果直接赋值，String str = “qwqw” 遵循基本类型，如果利用new去创建，遵循自定义对象。 浅拷贝12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package com.xie.core.base;import java.io.IOException;class School { private String name; public String getName() { return name; } public void setName(String name) { this.name = name; }}class Student implements Cloneable { private School school; private String name; public School getSchool() { return school; } public void setSchool(School school) { this.school = school; } public String getName() { return name; } public void setName(String name) { this.name = name; } /** * 浅拷贝 */ @Override protected Object clone() throws CloneNotSupportedException{ return super.clone(); }}public class CloneTest01 { public static void main(String[] args) throws CloneNotSupportedException, ClassNotFoundException, IOException { School school = new School(); school.setName(\"Thank School\"); Student student1 = new Student(); student1.setName(\"Thank\"); student1.setSchool(school); Student student2 = (Student) student1.clone(); student2.setName(\"Chunhua\"); student2.getSchool().setName(\"Chunhua School\"); System.out.println(student1.getName() + \", \" + student1.getSchool().getName()); System.out.println(student2.getName() + \", \" + student2.getSchool().getName()); }} 输出结果为: 12345Thank, Chunhua SchoolChunhua, Chunhua School我分别打印了school的toString():student1 --&gt; school: com.xie.core.base.School@762efe5dstudent2 --&gt; school: com.xie.core.base.School@762efe5d 可以看到, student2的拷贝只是对String类型name实现了拷贝, 而对School对象没有实现拷贝这就是浅拷贝. 深拷贝再来看看深拷贝 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package com.xie.core.base;import java.io.IOException;class School implements Cloneable{ private String name; public String getName() { return name; } public void setName(String name) { this.name = name; } /** * 深拷贝 */ @Override protected Object clone() { Object o = null; try { o = super.clone(); } catch (CloneNotSupportedException e) { e.printStackTrace(); } return o; }}class Student implements Cloneable{ private School school; private String name; public School getSchool() { return school; } public void setSchool(School school) { this.school = school; } public String getName() { return name; } public void setName(String name) { this.name = name; } /** * 深拷贝 */ @Override protected Object clone() throws CloneNotSupportedException { Student o = null; try { o = (Student) super.clone(); // 调用School的clone方法实现深拷贝 o.school = (School) school.clone(); } catch (Exception e) { e.printStackTrace(); } return o; }}public class CloneTest01 { public static void main(String[] args) throws CloneNotSupportedException, ClassNotFoundException, IOException { School school = new School(); school.setName(\"Thank School\"); Student student1 = new Student(); student1.setName(\"Thank\"); student1.setSchool(school); Student student2 = (Student) student1.clone(); student2.setName(\"Chunhua\"); student2.getSchool().setName(\"Chunhua School\"); System.out.println(student1.getName() + \", \" + student1.getSchool().getName()); System.out.println(student2.getName() + \", \" + student2.getSchool().getName()); }} 1234567输出结果为:Thank, Thank SchoolChunhua, Chunhua School我分别打印了school的toString():student1 --&gt; school: com.xie.core.base.School@762efe5dstudent2 --&gt; school: com.xie.core.base.School@41a4555e 是两个不同的对象. 符合’完全复制一个新的对象，需要申请新的内存空间’ 的定义, 这就是深拷贝 序列化拷贝再来介绍一种实现深度拷贝的方法. 利用序列化实现对象的拷贝 把对象写到流里的过程是串行化（Serilization）过程，又叫对象序列化， 而把对象从流中读出来的（Deserialization）过程叫反序列化。 应当指出的是，写在流里的是对象的一个拷贝，而原对象仍然存在于JVM里面， 因此在Java语言里深复制一个对象，常常可以先使对象实现Serializable接口， 然后把对象（实际上只是对象的一个拷贝）写到一个流里，再从流里读出来便可以重建对象。 此时不用实现Cloneable的clone(), 但是必须要实现Serializable接口 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879package com.xie.core.base;import java.io.ByteArrayInputStream;import java.io.ByteArrayOutputStream;import java.io.IOException;import java.io.ObjectInputStream;import java.io.ObjectOutputStream;import java.io.Serializable;class School implements Serializable { //实现序列化接口 private String name; public String getName() { return name; } public void setName(String name) { this.name = name; }}class Student implements Serializable{ //实现序列化接口 private School school; private String name; public School getSchool() { return school; } public void setSchool(School school) { this.school = school; } public String getName() { return name; } public void setName(String name) { this.name = name; } /** * 串行化深复制 */ public Object deepClone() throws IOException, ClassNotFoundException{ ByteArrayOutputStream bo = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(bo); oos.writeObject(this); ByteArrayInputStream bi = new ByteArrayInputStream(bo.toByteArray()); ObjectInputStream osi = new ObjectInputStream(bi); return osi.readObject(); }}public class CloneTest01 { public static void main(String[] args) throws CloneNotSupportedException, ClassNotFoundException, IOException { School school = new School(); school.setName(\"Thank School\"); Student student1 = new Student(); student1.setName(\"Thank\"); student1.setSchool(school); Student student2 = (Student) student1.deepClone(); student2.setName(\"Chunhua\"); student2.getSchool().setName(\"Chunhua School\"); System.out.println(student1.getName() + \", \" + student1.getSchool().getName()); System.out.println(student2.getName() + \", \" + student2.getSchool().getName()); }} 总结在内存中通过字节流的拷贝是比较容易实现的。把母对象写入到一个字节流中，再从字节流中将其读出来，这样就可以创建一个新的对象了，并且该新对象与母对象之间并不存在引用共享的问题，真正实现对象的深拷贝。","link":"/2016/11/18/关于Java克隆/"},{"title":"内网穿透工具","text":"介绍局域网内计算机通过NAT穿透(NAT Traveral: 网络地址转换), 在内网与外网之间建立连接通信. 推荐几个内网穿透的工具网站: 摘自github: Wechat-Group http://wendal.cn Nutz社区提供的ngrok服务，详细访问 https://nutz.cn/yvr/t/33b68q9106imspallbj4c6aa0p http://natapp.cn/ http://ngrok.io = http://ngrok.com http://ngrok.2bdata.com/ http://qydev.com/ http://www.ymgy.org/ （貌似山寨qydev的） http://www.ngrok.cc/ http://www.nat123.com/ http://blog.qqbrowser.cc/ （已停止服务） http://www.mofasuidao.cn/ http://hsk.oray.com/ 花生壳，以前免费，现在收费8块钱，作为认证费,认证之后有两个端口映射免费。 http://www.pubyun.com/ 以前的3322.org 有免费版、收费版 现在tplink的部分路由器也有DNS服务 natapp这里我用natapp, 下载地址: https://natapp.cn 介绍natapp实际是把外网域名与客户端地址做了一个桥梁. 当访问外网地址时, natapp服务端会通过隧道转发到客户端(我们自己的机器)上. 从而实现内网穿透. 传输数据经过加密, 安全性有一定的保障. 所以, 看出来了吧, 我们需要购买隧道和域名! 购买步骤较为简单: 注册 购买隧道并设置域名, 拷贝authtoken 下载客户端 下载config.ini文件至客户端同目录下, 修改authtoken 启动natapp客户端. 购买完成如图所示: 注意: 在购买隧道时候, 有一个选项是隧道协议, 一旦购买, 不可更改. Web: 普通型http(s)隧道穿透,用于搭建网站,微信开发等穿透到本地web服务. TCP: 端口转发 应用于SSH,数据库,远程桌面,GAME,WebSocket等基于TCP连接的一切应用任您想象~ UDP: 端口转发 应用于游戏, 远程开机等基于UDP协议的一切应用 注意: 如果你要做微信相关开发, 虽然在步骤2中隧道设置了临时三级域名 *.*natapp.cc, 但是三级已被微信屏蔽,无法用于微信开发. 所以我们还需要再购买二级域名或自主域名, 说白了, 还要花钱. 不过这里的二级域名很便宜而且也经过备案. 我买了一个月的VIP隧道和一年的二级域名, 共计20元. 说明如果对上面所说的内网穿透的概念还没有清楚, 不知道它能用来做什么, 那么举两个例子. 在公司里项目开发都会用内网, 想在家或其他地方演示项目. 在公司里开发都会通过内网连接公司的服务器, 数据库等. 想在家或其他地方来连接. 还用开两台电脑, 用TeamViewer连接吗? 不用! 微信本地开发时, 微信回调地址需要配置域名, 从而转发到我们的应用接口中. 内网穿透能够满足上面的场景. 只要清楚了natapp的作用, 适用什么场景, 它的使用就很简单了.","link":"/2017/11/01/内网穿透工具/"},{"title":"动态规划最优解(0-1背包)","text":"[TOC] 序言场景: 一天深夜. 路过一家商品店, 门未锁, 附近无人. 于是溜入店中. 店内商品五花八门, 掂了掂自己的书包, 差不多能承重10公斤. 于是立刻挑了四件不超过10公斤的商品, 价值如下: Item1 Item2 Item3 Item4 Weight 7 4 2 6 Value 140 60 50 130 对于任意一件商品, 我只有拿和不拿两种选择, 不能对某件商品进行拆分. 该怎么拿使得价值最大而不会破包? … 广告: 用markdown写表格很麻烦, 但突然在网上发现一个神器:excel转markdown神器 思考首先, 想到一种解决办法1! 把这四种物品的所有排列可能搞出来, 依次尝试, 这个复杂度2的n次方. 要是有几十个物品呢? 繁琐程度可想而知. 排除这种办法. 又想到一种貌似不错的解决办法2:先算出四件商品单位重量下的价值(value/kg) value/weight =&gt; value/kg 结果如下: Item1 Item2 Item3 Item4 value/kg 20 15 25 21.5 再由高到低排序得到: Item3&gt;Item4&gt;Item1&gt;Item2 所以得到如下步骤: 放商品3, 背包剩余重量=(10-2)=8kg, 检查还有无能放的? 有- 放商品4, 背包剩余重量=(8-6)=2kg, 检查还有无能放的? 没了… 所以最终得到的价值为商品3和商品4的价值总和(50+130)=180元 然而, 这并不是最优的解法. 随便把商品1和商品3放进去, 价值都是190. 所以很遗憾, 用这种简单的小计算可能会导致得到的价值反而最低. 动态规划介绍动态规划是解决最优问题的一种方法. 他可以解决很多动归问题. 我们刚才的问题0-1背包问题就是可以用它来解决的. 除此之外, 类似的还有”部分背包问题”, “完全背包问题” 如果用贪心算法来解决”0-1背包问题”, 得到的并不是最优解! 基本原理再来看一下动规解决背包问题的原理: 假如在最优解场景下, 我们最终只能在n个物品中拿x个(Item1, Item2, Item3 … Itemx) 那么这x个物品在满足小于背包重量的同时就已经形成最优解. 假如拿走最后一个放入的物品Itemx. 该物品的价值为Vx, 重量为Wx. 拿掉后对于剩余x-1个物品, 在重量Wp-Wx, V-vx下应该也成立最优解? (Wp:背包的最大重量, V:最终总价值) 可以用反证法来推导. 上面的递推关系证明了在最优解下, 它的子解集也是最优解; 于是可以将全局最优解扩展为局部最优解, 通过一步步缩小递推; 初始最小解应该为背包承受重量(W)为0, 物品数量为(N)0; 下面将W和N作为两个标量, 定义为二维矩阵matrix[][]; 因为有初始最小姐(为0)的情况, 矩阵应该为matrix[N+1][W+1]; 也就是N+1行, W+1列的二维数组; 将矩阵结合上述推导, 我们分出三种情况(item:物品个数, weight:背包承重数): 初始姐: item=0, weight=0 选择的物品大于背包承重了, 很简单, 那就不放呗, 价值仍为matrix[i-1][w]! 在不大于背包承重限制下, 对于是否要选择第x个物品, 我们判断的依据是什么? a:如果不放, 价值为matrix[i-1][w]; b:如果放了, 价值为matrix[i-1][w] + matrix[i-1][w-wi] 再来比较a,b值.若a&gt;b则选a, a&lt;b则选b 可以将上述推导具化为公式: 代码实现分析推导完毕, 进入最激动人心的实现环节! 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class Knapsack { public static void main(String[] args){ int packWt = 10; int[] itemWtArr = {7, 4, 2, 6}; int[] itemValArr = {140, 60, 50, 130}; System.out.println(\"动态最优解: \" + knapsack(packWt, itemWtArr, itemValArr)); } public static int knapsack(int packWt, int[] itemWtArr, int[] itemValArr){ int itemCount = itemWtArr.length; int[][] matrix = new int[itemWtArr.length+1][packWt+1]; //初始情况: 物品数为0 for(int i=0; i&lt;=packWt; ++i) { matrix[0][i] = 0; } //初始情况: 背包重数为0 for(int i=0; i&lt;=itemCount; ++i) { matrix[i][0] = 0; } for(int i=1; i&lt;=itemCount; ++i){ for(int j=1; j&lt;=packWt; ++j){ if(itemWtArr[i-1] &lt;= j) { //物品重量 &lt; 当前背包重量 /** * 1.不加入该物品时该重量的最大价值: matrix[i-1][j] * 2.当前物品的价值: itemValArr[i-1]; * 3.当前物品的重量 : itemWtArr[i-1]; * 4.背包剩余重量: j-itemWtArr[i-1]; * 5.可以容纳剩余重量的价值: matrix[i-1][j-itemWtArr[i-1]] */ int affordItemVal = matrix[i-1][j-itemWtArr[i-1]]; //放还是不放? 得出子集最优解 matrix[i][j] = Math.max(matrix[i-1][j], affordItemVal + itemValArr[i-1]); }else { //物品重量 &gt;背包重量 =&gt; 不放 int forwardItemVal = matrix[i-1][j]; matrix[i][j] = forwardItemVal; } } } printMatrix(matrix); return matrix[itemCount][packWt]; } public static void printMatrix(int[][] matrix){ System.out.println(\"---------------------------------------\"); for(int i=0; i&lt;matrix.length; ++i){ for(int j=0; j&lt;matrix[i].length; ++j){ System.out.format(\"%4d\", matrix[i][j]); } System.out.println(\"\\n\"); } System.out.println(\"---------------------------------------\"); }} 至此, 序言中提到的问题 通过推导和编码已经完全实现了! 图解下面用表格来描述一下过程 在初始状态下(物品数量Item=0, 背包承受重量Weight=0): Value Weight0 Weight1 Weight2 Weight3 Weight4 Weight5 Weight6 Weight7 Weight8 Weight9 Weight10 Item0 0 0 0 0 0 0 0 0 0 0 0 Item1 0 Item2 0 Item3 0 Item4 0 得出最优矩阵: Value Weight0 Weight1 Weight2 Weight3 Weight4 Weight5 Weight6 Weight7 Weight8 Weight9 Weight10 Item0 0 0 0 0 0 0 0 0 0 0 0 Item1 0 0 0 0 0 0 0 140 140 140 140 Item2 0 0 0 0 60 60 60 140 140 140 140 Item3 0 0 50 50 60 60 110 140 140 190 190 Item4 0 0 50 50 60 60 130 140 180 190 190 取值验证取值验证是个看似很傻, 但很有效的方法 我记得我在开始学习编程语言时, 第一次碰到双层for循环, 就是取值验证才理解 如果对前面的结果还有疑惑, 可以取值来验证之前的推导: example 1: 分析item2,weight4 (V:60): 能否放物品2 ? 可以阿! 物品2重量为4. 不放物品2是否价值更大? 不是阿! 不放价值为0(看前一行数据) 放入物品2后还能放? 不能阿! 放了之后背包剩余承重为0了 example 2: 再来分析一下item3,weight9 (V:190): 不放物品3时的最大价值, 看前一行是140 能放物品3么, 能阿! 放上物品3之后还能再放么? 能阿! 放完之后还剩余(10-2)=8呢 那计算当前物品的价值+可以容纳的剩余重量的价值: Vitem2+martix[item=2][weight=8]=50+140=190190 &gt; 140 阿. 肯定选钱多的. the end","link":"/2017/03/03/动态规划最优解(0-1背包)/"},{"title":"基于WEB微信通信实现智能聊天机器人","text":"序言 在QQ群里突然看到一个叫QQ小冰的机器人, 在群里只要@他 就会出来跟你聊天. 可以讲笑话, 查天气等功能, 跟人聊天的语义理解也非常智能. 类似的还有美拍的小冰.于是去找了相关的机器人, 有微软小冰, 茉莉机器人, 图灵机器人等…还有些是收费的. 果断跳过 我已经放到github上了: https://github.com/thank037/wechat-robot 关于实现, 发现现有很多机器人都有API, 提供第三方接入. 当然微信和QQ也支持. 通过微信公众号接入机器人或关注机器人好友都可以快速实现与机器人聊天. 对于这种接入, 我还试着去注册了微信公众号. 发现并不是我想要的. 首先, 不想让机器人作为一个公众号, 我希望他的消息出现在好友对话列表, 而非订阅号列表中. 而且要支持群聊! 其次, 我希望能作为一个开发者, 能够自由的为这个机器人写出想要的功能. 参考 挖掘微信Web版通信的全过程 Python版本 Java版本 看了网上的一些参考后, 不得不说, Python的版本很多, 而且功能普遍要比Java的完善. Java版本的还有部分bug. 这里我参考了这个版本. 除了修正部分bug之外, 根据自己想法, 又加入了如下: 123456789101112131415161718192021**开发日志**修复bug: 1. 对群聊中的消息判断不准确.(WechatServiceImpl --&gt; handleMsg()) 新增功能: 1. 机器人调用变为图灵机器人(原来是茉莉机器人) 2. 群聊中被@回复消息 3. 增加给特定用户定时发送问候语 4. 在定时发送功能中增加金山和茉莉机器人的API调用 a. 金山API(获取每日一句英语) b. 茉莉机器人(获取当天当地天气信息) 5. 增加Emoji表情, 并随机发送 6. 程序处理&quot;图灵机器人&quot;消息内容的水印 7. 增加消息防撤回(识别撤回消息并保存到消息字典) 8. 增加语义处理(趣味回答, 口头禅等...) 9. 完善控制台和记录文件的LOGGER日志, 方便日后维护及调试 10. 调用API异常的处理(例如茉莉机器人的接口有时很不稳定, 为了不影响功能, 增加备用接口处理异常)TODO: 1. 增加发送图片和语音的功能. 2. 如何不依赖手机端, 程序出现异常后重新选择线路 3. 增强程序稳定性 执行流程其实这里与机器人的对话并不是难得, 因为已经有现成的API提供 主要是需要研究微信WEB协议与API 123456789101112131415161718192021222324 +--------------+ +---------------+ +---------------+ | | | | | | | Get UUID | | Get Contact | | Status Notify | | | | | | | +-------+------+ +-------^-------+ +-------^-------+ | | | | +-------+ +--------+ | | | +-------v------+ +-----+--+------+ +--------------+ | | | | | | | Get QRCode | | Weixin Init +------&gt; Sync Check &lt;----+ | | | | | | | +-------+------+ +-------^-------+ +-------+------+ | | | | | | | +-----------+ | | | +-------v------+ +-------+--------+ +-------v-------+ | | Confirm Login | | | |+------&gt; Login +---------------&gt; New Login Page | | Weixin Sync || | | | | | || +------+-------+ +----------------+ +---------------+| ||QRCode Scaned|+-------------+ API获取会话ID Get UUID 12345678910URL: https://login.wx.qq.com/jslogin请求方式: GET参数: a. appid: wx782c26e4c19acffb(固定字符串) b. fun: new(固定值) c. lang: zh_CN(固定值) d. _: 1491804797(13位毫秒时间戳)返回数据(String):window.QRLogin.code = 200; window.QRLogin.uuid = &quot;[UUID]&quot;状态码code=200表示成功 显示二维码图片 Get QRCode 123URL: https://login.weixin.qq.com/qrcode/[UUID] (上一步获取到的返回值window.QRLogin.uuid)请求方式: GET返回数据: 二维码 手机端扫描二维码等待确认登录1234567891011URL: https://login.weixin.qq.com/cgi-bin/mmwebwx-bin/login请求方式: GET参数: a. uuid: [UUID](前面获取到的UUID) b. tip: 1 (1-未扫描 0-已扫描) d. _: 1491804797(13位毫秒时间戳)返回数据(String):window.code=xxx(408 登陆超时, 201 扫描成功但未确认, 200 确认登录)由于该请求需要用户在手机端连续做几个操作, 所以代码里要轮询来实现. 直到返回结果为200.获取到以下URL后需要继续访问当前链接获取wxuin和wxsidwindow.redirect_uri=&quot;https://wx.qq.com/cgi-bin/mmwebwx-bin/webwxnewloginpage?ticket=xxx&amp;uuid=xxx&amp;lang=xxx&amp;scan=xxx&quot;; 后续后面还有一些步骤，麻烦不想写了啊 大致步骤是 初始化微信, 开启状态通知, 保存个人信息, 登录信息, 并将联系人列表和群组列表保存下来. 然后选择同步线路, 轮询进行消息检查. 获取到最新消息后调用机器人API(这里我用的是图灵机器人)获得回答结果. 然后调用消息发送API, 完成消息发送. 相关的通信过程和API网上有很多. 在开头参考中有推荐 附注为了方便开发, 加几个附注: 1: 同步状态在同步消息检查的API中:https://webpush2.weixin.qq.com/cgi-bin/mmwebwx-bin/synccheck 为了模拟实时消息的更新, 在程序中轮询2秒检查一次, 此接口的返回值如下: 12345678window.synccheck={retcode:&quot;xxx&quot;,selector:&quot;xxx&quot;}第一步判断: retcode 0-正常 1100-失败/登出微信第二步判断: selector 0 正常 2/6 新的消息 7 进入/离开聊天界面 所以当selector=2/6时, 我们就可以进行消息处理. 这里selector有个很奇怪的返回值, 就是3! 我翻阅各种API也没找到为什么有时会返回3导致程序死掉 2: 消息账户类型在发送消息之前, 需要获取同步消息.URL: https://wx.qq.com/cgi-bin/mmwebwx-bin/webwxsync?sid=xxx&amp;skey=xxx&amp;pass_ticket=xxx返回值包括了消息发送方, 接收方, 消息内容, 消息类型.消息来源的账号类型大致有这几类:来自个人: 以@开头来自群聊: 以@@开头来自公众号/服务号: 以@开头，VerifyFlag &amp; 8 != 0来自特殊账号: 123456// 特殊用户 须过滤(&quot;newsapp&quot;, &quot;fmessage&quot;, &quot;filehelper&quot;, &quot;weibo&quot;, &quot;qqmail&quot;, &quot;fmessage&quot;, &quot;tmessage&quot;, &quot;qmessage&quot;, &quot;qqsync&quot;, &quot;floatbottle&quot;, &quot;lbsapp&quot;, &quot;shakeapp&quot;, &quot;medianote&quot;, &quot;qqfriend&quot;, &quot;readerapp&quot;, &quot;blogapp&quot;, &quot;facebookapp&quot;, &quot;masssendapp&quot;, &quot;meishiapp&quot;, &quot;feedsapp&quot;, &quot;voip&quot;, &quot;blogappweixin&quot;, &quot;weixin&quot;, &quot;wxitil&quot;,&quot;brandsessionholder&quot;, &quot;weixinreminder&quot;, &quot;wxid_novlwrv3lqwv11&quot;, &quot;gh_22b87fa7cb3c&quot;, &quot;officialaccounts&quot;,&quot;notification_messages&quot;, &quot;wxid_novlwrv3lqwv11&quot;, &quot;gh_22b87fa7cb3c&quot;, &quot;userexperience_alarm&quot;); 3: 消息类型 MsgType 说明 1 文本消息 3 图片消息 34 语音消息 37 VERIFYMSG 40 POSSIBLEFRIEND_MSG 42 共享名片 43 视频通话消息 47 动画表情 48 位置消息 49 分享链接 50 VOIPMSG 51 微信初始化消息 52 VOIPNOTIFY 53 VOIPINVITE 62 小视频 9999 SYSNOTICE 10000 系统消息 10002 撤回消息 图灵机器人关于图灵机器人的调用, 去官网注册一个就可以. so easy! 效果演示在调试功能时, 可以加上log, 查看同步连接信息和消息 附上几张和儿子的聊天:","link":"/2017/04/10/基于WEB微信通信实现智能聊天机器人/"},{"title":"年中总结与目标OKR","text":"[TOC] 昨天看到了狗哥的年中总结, 虽然开头和结尾比较丧, 但还是比较详细的列出了半年回顾和计划 也让我感受到了像狗哥这样的年轻人有目标真好~ 自己没制定过目标和计划, 发现有点难或者好奇心没啦又断了 所以, 结合前段时间看到的OKR, 这次来学习做一次年中总结, 内容尽可能细致且可度量, 以便年终进行检验 一. OKR先来简单说下OKR是什么 OKR: Objectives and Key Results的简称，中文名是目标与关键成果法 从名称上看到它包含两个要素 目标（Objectives） 关键结果（Key Results） 它由因特尔公司发明, 并被广泛推及到众多互联网公司, Oracle, Google, Uber, MongoDB等 OKR最广泛的使用是在企业和团队中, 根据战略和项目目标分解, 制定出符合OKR原则的目标(Objectives) 和用来考量目标和可量化的关键结果（Key Results） 当然, 我把它用在个人计划和目标的设定考量上同样适用, 包括工作、家庭、学习、兴趣、健康等多个方面，可以确保个人的多维度平衡发展 遵循两个指导原则 目标要有挑战, 野心, 有些让自己不舒服的, 甚至是有些吹牛比的 关键结果要有可量化的单位, 例如数字单位, 能够持续推进, 便于考量的 二. 具体列项上半年回顾忘了 下半年目标工作篇无 身体健康篇 虽然目前还没感觉到身体哪里不对, 但还是得提前预防下腰椎和秃头吧 O: 保持身体健康 KR: 周内每天坚持吃早餐, 周一到五工作日内不点外卖, 在无刮风下雨, 高温情况下去北科大进食 KR: 每周20分钟左右的身体锻炼不少于3次(不跑步) KR: 健康作息, 周内保持2点前入睡, 休息日3.30前 KR: 减少烟, 薄荷糖, 碳酸饮料摄入量 兴趣篇 楼上的舍友是中央音乐学院毕业, 让我见识了很多没见过的装备, 给了一厚沓的指弹资料 在他的熏陶下决定还是捡起半年没动过的琴吧 O: 提升吉他技能 KR: 年终前再学习两首以上指弹曲目(指弹曲, 流行歌曲翻弹曲都可) KR: 阅读南澤大介的指弹演奏书籍, 并尝试练习其中几个选段 O: 系统学习扒谱 KR: 保持每周至少两次音感, 音程和弹奏练习 KR: 掌握各类和弦, 和各类转调的判断和转调的过度和弦, 补充乐理知识 KR: 对编配有初步的认识, 并能够自己扒出一首简单的流行或民谣歌曲 持续学习篇 这是重中中重 O: 继续区块链学习 KR: 细读一遍EOS, Filecoin白皮书 KR: 学习以太坊开发, 深入了解Ethereum架构和设计理念, 区块链体系, EVM, 共识协议等 O: 编程语言学习 KR: Python, 重新复习基础, 目标只达到最基础水平(能够完成蓝桥杯几个算法题目)即可 KR: Golang, 开始学习基础, 目标只达到最基础水平(能够完成蓝桥杯几个算法题目)即可 O: Spring全家桶学习 KR: Spring Boot源码学习 KR: Spring Cloud巩固已学知识, 扩展源码学习 KR: Spring Security栈系统学习, 包括Spring Social第三方社交登录, OAuth认证 KR: Spring Data JPA, 把买来的SpringDataJPA实战书籍阅读完, 有自己对于该框架的理解, 深度实践, 并能针对现有公司业务做简单实用的封装 KR: Spring WebFlux, 理解响应式编程, 掌握最基础的使用即可, 并有自己的理解 O: ELK KR: 巩固已学的Elastic Search知识 KR: 系统学习Elasticsearch , LogStash, Kibana基本知识与实践应用 O: JVM KR: 这部分知识匮乏, 没有理清怎么写KR O: 并发编程 KR: 对计算机硬件CPU, 缓存, 内存等工作原理有基本认识, 不学深 KR: 着重学习JMM, 线程安全处理, AQS, J.U.C组件 KR: 对应对高并发的一些处理有一定的了解, 例如: 缓存, 消息, 分库分表, 熔断, 限流等 O: 中间件学习 KR: 消息中间件, RabbitMQ, Kafka深入学习原理 KR: 数据库中间件, ShardingJDBC基础掌握 O: NIO KR: 学有余力时 O: 持续阅新&amp;总结输出 KR: 在工作日每天路上, 如果不睡觉就看手机, 阅读感兴趣的技术内容(主要是微信公众号) KR: 基于以上的技术学习内容, 保证每月不少于三篇博客输出(凑数也要完成) 其它篇O: 游戏 KR: 本赛季再上一次王者 O: 在年前取得驾照, 并习得不错的驾驶技术 KR: 在周末约到车的情况下, 不约朋友不睡懒觉去练车(优先级最高) O: 在年前办理好落户事宜 KR: 去成都玩 KR: 去杭州玩 三. END? ? ?, 随便一写就这么多?, 按计划全部完成?, 不可能不可能, 这辈子都不可能的 有些目标的KRs没有具体量化, 但是在年终检阅时要尽可能标明进度百分比 在写完这些目标后, 我尝试找一款OKR管理工具, 来管理和跟踪这些目标进展情况, 但是没有找到很好用的, 尽力吧!","link":"/2018/07/16/年中总结与目标OKR/"},{"title":"微信开发四篇","text":"[TOC] 序言 上一篇微信相关的开发是《基于WEB微信通信实现智能聊天机器人》. 而怎么保持机器人稳定在线, 不被微信封号, 是一直遗留的问题… 网上一个稳定版本的IOS微信协议已经卖到2000! 所以无力研究了. 相比之下, 微信公众号开发和小程序更加火爆. 本篇将通过微信公众平台, 微信开放平台, 微信商户平台. 分为四篇来完成以下几个常用功能. 第一篇: 微信网页授权 第二篇: 微信支付 第三篇: 网站应用微信登录 第四篇: 推送微信模版消息 相关文档 (必须了解) 微信公众平台技术文档: https://mp.weixin.qq.com/wiki 微信支付开发文档(公众号支付): https://pay.weixin.qq.com/wiki/doc/api/index.html 微信开放平台: https://open.weixin.qq.com/ 使用SDK (开发效率事半功倍) 微信支付、小程序、公众号&amp;企业号开发Java SDK: https://github.com/Wechat-Group/weixin-java-tools 可能是目前最好的支付SDK: https://github.com/Pay-Group/best-pay-sdk 你需要知道的事通过阅读微信公众平台文档: 可以知道公众平台账号分为: 订阅号 服务号 企业号 每类账号的适用人群不同, 开放的接口权限不同, 申请门槛也不同. 公众号接口权限说明 官方参考: http://kf.qq.com/faq/170104AJ3y26170104Yj673y.html 可以看到, 微信授权和支付功能这两项权限只有微信认证服务号才有. 服务号一般人没有企业资质很难申请到, 更别说微信认证了. 好的一点是微信公众平台提供了一个测试号, 可以通过手机扫描二维码来获得测试号. 申请地址: https://mp.weixin.qq.com/debug/cgi-bin/sandbox?t=sandbox/login 注意: 测试公众号有很多体验接口, 包括网页授权获取(无支付权限). 你需要准备 微信开发需要手机电脑来回调试, 过程还是比较繁琐. 如何在本地方便的进行调试, 就要依赖一些东西. 域名微信网页授权的回调地址不能是ip地址. 这一点在官方文档中有提到. 所以在回调到我们的应用程序接口时要支持域名/接口路径 的方式, 例如: http://thank.mynatapp.cc/wechat/xxx 内网映射在手机上调试微信客户端时, 调用PC接口, 肯定无法用 http://localhost:8888/wechat/xxx 的方式了, 为了在本地方便的调试, 就需要用到内网穿透工具. 可以参考我的另一篇: 内网穿透介绍 抓包工具在手机上点击时, 无法像电脑浏览器中的F12一样看到详细的网络请求. 所以使用fiddler或charles等抓包工具来抓取网络请求. 手机Wi-Fi中的HTTP代理中填写主机的ip地址和8888端口(默认)即可. 微信web开发者工具 这是微信官方提供给开发者的工具, 很方便. 不用下载抓包工具, 也不用设置手机代理. 可以像电脑浏览器一样模拟手机发送请求. 支持公众号网页调试和小程序调试. 注意: 微信支付相关的调试在授权时需要在微信公众平台的账号中绑定开发者. 第一篇: 微信网页授权网页授权是指用户在微信客户端访问第三方网页时, 公众号可通过授权机制来获取用户的基本信息. 从而实现业务逻辑. 授权流程: https://mp.weixin.qq.com/wiki?t=resource/res_main&amp;id=mp1421140842 通过了解授权流程, 可以看到. 网页授权流程主要分为四步： 引导用户进入授权页面同意授权，获取code 通过code换取网页授权access_token（与基础支持中的access_token不同） 如果需要，开发者可以刷新网页授权access_token, 避免过期 通过网页授权access_token和openid获取用户基本信息（支持UnionID机制） 主要来看前2步: 12345# 替换相关参数,相关参数说明在文档中有详细介绍...https://open.weixin.qq.com/connect/oauth2/authorize?appid=APPID&amp;redirect_uri=REDIRECT_URI&amp;response_type=code&amp;scope=SCOPE&amp;state=STATE#wechat_redirect# 例如: https://open.weixin.qq.com/connect/oauth2/authorize?appid=wxdcb894a6a375ed25&amp;redirect_uri=http://thank.mynatapp.cc/sell/weixin/auth&amp;response_type=code&amp;scope=snsapi_base&amp;state=STATE#wechat_redirect 在微信客户端访问该url后, 会进行跳转, redirect_uri 会重定向到我们的应用. 在跳转之前, 需要用户同意授权. 而用户同意授权的方式, 取决于上面的 scope 参数: snsapi_base: 静默授权, 微信端用户会在无感知的情况下同意授权, 但是拿到的信息也较少, 只有用户openid. snsapi_userinfo: 微信端会弹出授权页面点击确认. 这种情况下拿到的openid可以再继续拿到用户的更多信息 如昵称, 性别, 所在地等. 当微信客户端用户同意授权后, 页面将跳转路径为: redirect_uri/?code=CODE&amp;state=STATE 也就是携带着code参数进入我们的应用接口中, 从而在应用接口中获取到code. code作为换取access_token的票据，每次用户授权带上的code将不一样，code只能使用一次，5分钟未被使用自动过期。 编码实现123456789101112131415161718192021@GetMapping(&quot;/auth&quot;)public void auth(@RequestParam(&quot;code&quot;) String code){ //第一步: 用户同意授权, 通过回调, 进入该方法, 获取到code log.info(&quot;-----进入auth方法: code={}&quot;, code); //第二步：通过code换取网页授权access_token String getTokenUrl = &quot;https://api.weixin.qq.com/sns/oauth2/access_token?appid=&quot; +appid+&quot;&amp;secret=&quot; +appsecret+&quot;&amp;code=&quot; +code+&quot;&amp;grant_type=authorization_code&quot;; RestTemplate restTemplate = new RestTemplate(); String response = restTemplate.getForObject(getTokenUrl, String.class); log.info(&quot;-----response={}&quot;, response ); Map&lt;String, Object&gt; map = new Gson().fromJson(response, new TypeToken&lt;HashMap&lt;String,Object&gt;&gt;(){}.getType()); log.info(&quot;--- accessToken={}&quot;, map.get(&quot;access_token&quot;)); log.info(&quot;--- openid={}&quot;, map.get(&quot;openid&quot;)); log.info(&quot;--- scope={}&quot;, map.get(&quot;scope&quot;));} 授权过程是可以用手工编写代码来完成, 但比较繁琐. 而github上有很多现成的SDK提供, 所以不需要重复造轮子. sdk实现 Github地址: 微信支付、小程序、企业号和公众号（包括服务号和订阅号） Java SDK开发工具包 Maven依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.github.binarywang&lt;/groupId&gt; &lt;artifactId&gt;weixin-java-mp&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt;&lt;/dependency&gt; 完成微信公众号相关配置读取 123456789101112131415161718192021222324@Componentpublic class WechatMpConfig { // 配置读取类 @Autowired private WechatAccountConfig wechatAccountConfig; @Bean public WxMpService wxMpService(){ WxMpService wxMpService = new WxMpServiceImpl(); wxMpService.setWxMpConfigStorage(wxMpConfigStorage()); return wxMpService; } @Bean public WxMpConfigStorage wxMpConfigStorage(){ WxMpInMemoryConfigStorage wxMpConfigStorage = new WxMpInMemoryConfigStorage(); wxMpConfigStorage.setAppId(wechatAccountConfig.getMpAppId()); wxMpConfigStorage.setSecret(wechatAccountConfig.getMpAppSecret()); return wxMpConfigStorage; }} 找到该项目的github 开发wiki: 公众号开发 -&gt; OAuth2网页授权 授权过程分为这几步: 首先构造网页授权url，然后构成超链接让用户点击 123wxMpService.oauth2buildAuthorizationUrl(redirectURI, WxConsts.OAUTH2_SCOPE_USER_INFO, URLEncoder.encode(returnUrl)); 该方法返回一个url, 让微信客户端用户点击, 当用户同意授权, 会回调至设置的redirectURI. 并把code携带过去. 用户点击的页面如下: 12WxMpOAuth2AccessToken wxMpOAuth2AccessToken = wxMpService.oauth2getAccessToken(code);String openid = wxMpOAuth2AccessToken.getOpenId(); 再通过该方法获取openid. (除了openid以外, 还可以获得用户其它信息), 如下图: 1WxMpUser wxMpUser = this.wxMpService.oauth2getUserInfo(wxMpOAuth2AccessToken, null); 第二篇: 微信支付 sdk: Pay-Group/best-pay-sdk 虽然微信官方有提供SDK和DEMO, 但是并不好用 这款SDK是在github中找的, 里面有很详细的视频说明, 而且还提供支付账号借用, 对于拿不到公众号认证资格的开发者来说很方便学习. 123456&lt;!--微信支付相关SDK--&gt;&lt;dependency&gt; &lt;groupId&gt;cn.springboot&lt;/groupId&gt; &lt;artifactId&gt;best-pay-sdk&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt;&lt;/dependency&gt; 支付流程下面来描述一个最基本的支付流程, 包括以下几个环节: 统一下单 微信客户端H5支付 接收微信异步通知 退款 1. 统一下单API 应用场景 除被扫支付场景以外，商户系统先调用该接口在微信支付服务后台生成预支付交易单，返回正确的预支付交易回话标识后再按扫码、JSAPI、APP等不同场景生成交易串调起支付。 配置WxPayH5Config 1234567891011121314151617181920212223242526@Componentpublic class WechatPayConfig { // 配置读取类 @Autowired private WechatAccountConfig accountConfig; @Bean public BestPayServiceImpl bestPayService() { BestPayServiceImpl bestPayService = new BestPayServiceImpl(); bestPayService.setWxPayH5Config(wxPayH5Config()); return bestPayService; } @Bean public WxPayH5Config wxPayH5Config() { WxPayH5Config wxPayH5Config = new WxPayH5Config(); wxPayH5Config.setAppId(accountConfig.getMpAppId()); wxPayH5Config.setAppSecret(accountConfig.getMpAppSecret()); wxPayH5Config.setMchId(accountConfig.getMchId()); wxPayH5Config.setMchKey(accountConfig.getMchKey()); wxPayH5Config.setKeyPath(accountConfig.getKeyPath()); wxPayH5Config.setNotifyUrl(accountConfig.getNotifyUrl()); return wxPayH5Config; }} 设置发起支付的相关参数 PayRequest , 调用创建支付接口 1PayResponse payResponse = this.bestPayService.pay(payRequest); 这样就完成了生成预付单. payResponse 返回结果中携带预付单信息(prepay_id及其他信息) 2. 微信内H5调起支付注意: 必须在微信客户端浏览器中 下单的返回对象 payResponse 会返回 getBrandWCPayRequest 中的参数 参数列表详细描述:https://pay.weixin.qq.com/wiki/doc/api/jsapi.php?chapter=7_7&amp;index=6 12345678910111213141516171819202122232425function onBridgeReady(){ WeixinJSBridge.invoke( 'getBrandWCPayRequest', { \"appId\":\"wx2421b1c4370ec43b\", //公众号名称，由商户传入 \"timeStamp\":\"1395712654\", //时间戳，自1970年以来的秒数 \"nonceStr\":\"e61463f8efa94090b1f366cccfbbb444\", //随机串 \"package\":\"prepay_id=u802345jgfjsdfgsdg888\", \"signType\":\"MD5\", //微信签名方式： \"paySign\":\"70EA570631E4BB79628FBCA90534C63FF7FADD89\" //微信签名 }, function(res){ if(res.err_msg == \"get_brand_wcpay_request:ok\" ) {} // 使用以上方式判断前端返回,微信团队郑重提示：res.err_msg将在用户支付成功后返回 ok，但并不保证它绝对可靠。 } ); }if (typeof WeixinJSBridge == \"undefined\"){ if( document.addEventListener ){ document.addEventListener('WeixinJSBridgeReady', onBridgeReady, false); }else if (document.attachEvent){ document.attachEvent('WeixinJSBridgeReady', onBridgeReady); document.attachEvent('onWeixinJSBridgeReady', onBridgeReady); }}else{ onBridgeReady();} 此时在微信客户端生成JSAPI页面, 用户点击即可发起支付. 关于支付成功的判断 后台的商户系统通常会根据支付状态去修改商品的状态, 库存等. 如何判断用户是否支付成功呢? 在H5调起支付中, 网页支付接口中在完成后会返回 err_msg . 返回值 描述 get_brand_wcpay_request:ok 支付成功 get_brand_wcpay_request:cancel 支付过程中用户取消 get_brand_wcpay_request:fail 支付失败 但是这种方式并不可靠, 微信团队也有说明. 所以, 可靠的方式是通过微信返回给我们的支付结果通知 . 3. 支付结果通知API 微信异步通知 支付完成后，微信会把相关支付结果和用户信息发送给商户，商户需要接收处理，并返回应答。 注意微信的异步通知需要设置 notify_url , 否则无法接收. 关于 notify_url : 微信没有设置白名单, 所以只要外网能访问到的地址即可. 例如: http://thank.mynatapp.cc/wechat/pay/notify 微信返回给商户系统的数据如下: (SUCCESS情况下) 12345678910111213141516171819&lt;xml&gt; &lt;appid&gt;&lt;![CDATA[wx2421b1c4370ec43b]]&gt;&lt;/appid&gt; &lt;attach&gt;&lt;![CDATA[支付测试]]&gt;&lt;/attach&gt; &lt;bank_type&gt;&lt;![CDATA[CFT]]&gt;&lt;/bank_type&gt; &lt;fee_type&gt;&lt;![CDATA[CNY]]&gt;&lt;/fee_type&gt; &lt;is_subscribe&gt;&lt;![CDATA[Y]]&gt;&lt;/is_subscribe&gt; &lt;mch_id&gt;&lt;![CDATA[10000100]]&gt;&lt;/mch_id&gt; &lt;nonce_str&gt;&lt;![CDATA[5d2b6c2a8db53831f7eda20af46e531c]]&gt;&lt;/nonce_str&gt; &lt;openid&gt;&lt;![CDATA[oUpF8uMEb4qRXf22hE3X68TekukE]]&gt;&lt;/openid&gt; &lt;out_trade_no&gt;&lt;![CDATA[1409811653]]&gt;&lt;/out_trade_no&gt; &lt;result_code&gt;&lt;![CDATA[SUCCESS]]&gt;&lt;/result_code&gt; &lt;return_code&gt;&lt;![CDATA[SUCCESS]]&gt;&lt;/return_code&gt; &lt;sign&gt;&lt;![CDATA[B552ED6B279343CB493C5DD0D78AB241]]&gt;&lt;/sign&gt; &lt;sub_mch_id&gt;&lt;![CDATA[10000100]]&gt;&lt;/sub_mch_id&gt; &lt;time_end&gt;&lt;![CDATA[20140903131540]]&gt;&lt;/time_end&gt; &lt;total_fee&gt;1&lt;/total_fee&gt; &lt;trade_type&gt;&lt;![CDATA[JSAPI]]&gt;&lt;/trade_type&gt; &lt;transaction_id&gt;&lt;![CDATA[1004400740201409030005092168]]&gt;&lt;/transaction_id&gt;&lt;/xml&gt; 具体的参数信息在官方API中都有详细描述 接受到微信异步通知后, 通过SDK来接收微信支付结果消息 1234567891011// 1. 验证签名 (SDK已做)// 2. 验证支付状态 (SDK已做)PayResponse payResponse = this.bestPayService.asyncNotify(notifyData);// 3. 商户系统自己做验证金额, 支付人等处理.// 4. 验证完成后即可修改商品和订单的相关状态// 5. 返回微信处理结果 商户同步返回处理结果 商户系统在接收到微信的异步通知后, 就可以进行商品状态/库存修改等操作了, 处理完成后需要告诉微信处理结果, 否则微信会一直发送异步通知过来. 1234&lt;xml&gt; &lt;return_code&gt;&lt;![CDATA[SUCCESS]]&gt;&lt;/return_code&gt; &lt;return_msg&gt;&lt;![CDATA[OK]]&gt;&lt;/return_msg&gt;&lt;/xml&gt; 4. 申请退款API 当交易发生之后一段时间内，由于买家或者卖家的原因需要退款时，卖家可以通过退款接口将支付款退还给买家，微信支付将在收到退款请求并且验证成功之后，按照退款规则将支付款按原路退到买家帐号上。 与统一下单接口很类似, 不同的是申请退款请求需要双向证书, 开发者需要微信商户平台下载相关证书文件. 12# 设置RefundRequestRefundResponse refundResponse = this.bestPayService.refund(refundRequest); 第三篇: 微信登录功能这里介绍网站应用的微信登录功能, 该功能是在微信开放平台下. 网站应用微信登录是基于OAuth2.0协议标准构建的微信OAuth2.0授权登录系统. 微信OAuth2.0授权登录是让微信用户以微信身份安全登录第三方应用或网站. 扫码登录体验: 一号店 需要注意! 微信登录相关的授权除了要有开放平台开发者账号, 还需要用户号有企业资质的认证. 普通账号无法授权! 开放平台的授权流程和公众平台类似, 但是 AppID 和 AppSecret 不同, 所以获得的 openid 也不同. 每个用户针对每个公众号会产生一个安全的 OpenID . 一个微信号在不同公众号下的 openid 是不同的 授权流程 第三方发起微信授权登录请求，微信用户允许授权第三方应用后，微信会拉起应用或重定向到第三方网站，并且带上授权临时票据code参数； 通过code参数加上AppID和AppSecret等，通过API换取access_token； 通过access_token进行接口调用，获取用户基本数据资源或帮助用户实现基本操作。 详细的流程参考开发指南, 这里我使用sdk方式:best-pay-sdk sdk实现首先完成微信开放平台相关配置, 配置 AppID 和 AppSecret 12345678910111213141516171819202122@Componentpublic class WechatOpenConfig { // 配置读取类 @Autowired private WechatAccountConfig wechatAccountConfig; @Bean public WxMpService wxOpenService(){ WxMpService wxOpenService = new WxMpServiceImpl(); wxOpenService.setWxMpConfigStorage(wxOpenConfigStorage()); return wxOpenService; } @Bean public WxMpConfigStorage wxOpenConfigStorage(){ WxMpInMemoryConfigStorage wxMpInMemoryConfigStorage = new WxMpInMemoryConfigStorage(); wxMpInMemoryConfigStorage.setAppId(wechatAccountConfig.getOpenAppId()); wxMpInMemoryConfigStorage.setSecret(wechatAccountConfig.getOpenAppSecret()); return wxMpInMemoryConfigStorage; }} 第一步: 请求code 用户向第三方应用发起登录请求: 123456789@GetMapping(\"qrAuthorize\")public String qrAuthorize(@RequestParam(\"state\") String state){ String redirectUrl = this.projectUrlConfig.getWechatOpenAuthorize() + \"/wechat/qrUserInfo\"; String qrPageUrl = wxOpenService.buildQrConnectUrl(redirectUrl, WxConsts.QRCONNECT_SCOPE_SNSAPI_LOGIN, state); return \"redirect:\" + qrPageUrl;} 跳转至微信登录的二维码界面, 等待用户扫码授权后, 会重定向到 redirectUrl , 并携带 code 和 state 参数. redirect_uri?code=CODE&amp;state=STATE 第二步: 通过code换取access_token 1234567891011121314151617@GetMapping(\"qrUserInfo\")public String qrUserInfo(@RequestParam(\"code\") String code, @RequestParam(\"state\") String state) { WxMpOAuth2AccessToken wxMpOAuth2AccessToken = new WxMpOAuth2AccessToken(); try { wxMpOAuth2AccessToken = this.wxOpenService.oauth2getAccessToken(code); } catch (WxErrorException e) { log.error(\"[微信网页授权错误: {}]\", e); } log.info(\"state={}\", state); log.info(\"wxMpOAuth2AccessToken={}\", wxMpOAuth2AccessToken); log.info(\"accessToken={}\", wxMpOAuth2AccessToken.accessToken); log.info(\"openid={}\", wxMpOAuth2AccessToken.getOpenId()); return \"redirect:\";} 获取到用户的 openid , 授权完成. 第四篇: 微信模版消息推送这里用到微信公众平台中消息管理功能. 模板消息仅用于公众号向用户发送重要的服务通知，只能用于符合其要求的服务场景中，如信用卡刷卡 通知，商品购买成功通知等。不支持广告等营销类消息以及其它所有可能对用户造成骚扰的消息。 官方文档: https://mp.weixin.qq.com/wiki?t=resource/res_main&amp;id=mp1433751277 通过阅读文档: 这种容易产生骚扰, 欺骗功能的接口肯定还是需要公众账号, 认证服务号的. 还好的一点是测试号有模版消息(业务通知)的接口. 所以这里我用测试账号. 1. 设置模版在公众号管理中添加模版(测试号也类似) 模版标题: 随意填写 模版内容: 如果是公众号可以选择现有的模版, 测试号只能自己输入(需要遵循格式), 例如: 123456{{first.DATA}}收款方:{{keyword1.DATA}}支付方:{{keyword2.DATA}}支付方式:{{keyword3.DATA}}交易状态:{{keyword4.DATA}} {{remark.DATA}} 模版ID: 自动生成(用于接口调用) 2. 发送模版消息12http请求方式: POSThttps://api.weixin.qq.com/cgi-bin/message/template/send?access_token=ACCESS_TOKEN post发送模版内容data数据 使用sdk 实现 1234567891011121314151617181920212223242526272829@Autowiredprivate WxMpService wxMpService;public void pushTemplateMessage() { String templateId = \"f-wEkgooC6My0792dHeoEZ-Zd85cOXYJShMB45sCdV4\"; String openId = \"ol4h7wot1fneGq1RN0LfWaNBtsYQ\"; WxMpTemplateMessage templateMessage = new WxMpTemplateMessage(); templateMessage.setTemplateId(templateId); templateMessage.setToUser(openId); List&lt;WxMpTemplateData&gt; data = Arrays.asList( new WxMpTemplateData(\"first\", \"微信支付凭证哈哈哈！\"), new WxMpTemplateData(\"keyword1\", \"谢谢谢\", \"#173177\"), new WxMpTemplateData(\"keyword2\", \"可口可乐\"), new WxMpTemplateData(\"keyword3\", \"支付宝转账\"), new WxMpTemplateData(\"keyword4\", \"$10000 欠款未付\", \"#CD2626\"), new WxMpTemplateData(\"remark\", \"欢迎再次购买！\") ); templateMessage.setData(data); try { wxMpService.getTemplateMsgService().sendTemplateMsg(templateMessage); } catch (WxErrorException e) { log.error(\"微信模版消息发送失败, {}\", e); }} 用了SDK, 就非常简单吧! 效果如图:","link":"/2017/11/02/微信开发四篇/"},{"title":"扒谱系列—音程和音符","text":"音程和音符[TOC] 在扒谱过程中, 虽然大多数是扒和弦, 但是掌握单音程之间的关系是基础中的基础 在记忆音名, 唱名的同时视唱练耳 介绍从键盘和吉他把位可以看到mi fa和si do之间的半音关系 键盘的软件有很多: 推荐Everyone Piano 吉他前三把位的音阶图要熟练记忆 简谱各种常用的音符对照表 名称 简谱 时值(四分音符为一拍) 全音符 X - - - 四拍 二分音符 X - 二拍 四分音符 X 一拍 八分音符 X(加1个_) 1/2拍 十六分音符 X(加2个_) 1/4拍 etc … … 练习一①②为C大调的音程关系练习图, ③④为C小调 大调和小调区别 从听觉上，通常来说 大调的音乐非常明亮，让人感到快乐，感到愉悦。 小调一般柔和暗淡。 详细参考: 五分钟知道什么是大调和小调 练习二用一首经典! 经典! 经典! 《送别》做练习吧 在扒谱中忘掉歌词, 只关注唱名","link":"/2019/04/10/扒谱系列—音程和音符/"},{"title":"微服务专题—开篇","text":"[TOC] 前言 2016年在公司有幸接触到微服务, 并参与了公司基于微服务架构的产品开发, 但是… 开发内容偏业务, 那时我甚至还没有搞清楚分布式, 集群, 微服务…几者之间的联系和区别 对微服务的认识仅限于Spring Cloud技术栈中某几个组件的简单使用和配置… 正如那句标语Coordinate Anything, Spring Cloud为什么简化了分布式开发, 那时完全说不清楚 后来几年里, 有时会关注Spring Boot, Spring Cloud的版本迭代, 尝试去做一些Demo例如xx整合xx 遗憾的是, 依旧记录的是一堆代码, 配置… 未把一些感受和思考及时记录下来 所以在清理电脑时, 把相关的学习代码全部删掉了. 只留下github上一个两年前提交的学习项目, 虽然版本很老旧, 但是组件整合比较完整 https://github.com/thank037/spring-cloud-study 记不太清, 当时好像是学习 周立 的Spring Cloud教程, 感谢他们的分享! 所以准备开一个专题, 来记录自己目前阶段对于微服务架构的思考与各开源组件的选型与实践 目的 微服务是一种架构风格, 所以专题内容不局限于代码实践, 不局限于Spring Cloud技术栈! 主要目的: 对微服务架构体系有较为全面的认识 对微服务技术栈中各组件的架构原理进行深入理解 理解和实践微服务技术栈中各组件的业务与运维场景 能够从源码中获得一些细节和深层次机制 目录(持续更新)开篇临时列出能想到的目录, 在补充阶段目录标题修改为文章链接 服务注册与发现 分布式系统中服务注册与服务发现的主流技术方案 服务发现的需求与模式 源码分析——服务发现组件Netflix Eureka 源码分析——客户端负载Netflix Ribbon Ribbon——与其它组件超时和重试 实践: Consul+Docker服务注册与发现 服务间的通讯 通讯模型 服务间的 IPC 机制及各方案对比: HTTP, RPC 实践: 服务间通讯gRPC 服务网关 服务网关的要素, 功能, 架构原理 业界流行的服务网关方案, 实践 核心功能重点源码阅读: Netflix Zuul, Zuul 2.0, Spring Cloud Gateway 配置中心 核心概念与架构原理 基础应用场景与高级特性 理解与实践: 携程Apollo 服务跟踪 分布式追踪系统架构 OpenTracing理论基础及其数据模型 Spring Cloud Sleuth, zipkin CAT的理解与实践 实践: 分布式链路追踪与展示: Sleuth + Zipkin + Kafka + Elasticsearch + Kibana 服务容错 场景实践: 服务调用熔断, 服务降级, 容错限流 理论基础: 断路器模式 Netflix Hystrix原理 实践: Hystrix Dashboard 服务监控解决方案 实践: Prometheus + Grafana 微服务安全 TODO 微服务持续集成与部署实践 TODO: 另开专题","link":"/2019/04/10/微服务专题—开篇/"},{"title":"润乾报表的使用","text":"[TOC] 前言前段时间项目用到了报表开发, 选择润乾报表来做. 是款纯Java的报表开发工具, 采用设计和集成分离的方式, 开发过程也较为简单. 介绍设计时需要安装一个报表设计器, 设计过程类似于Excel中的表格设计. 而且支持html, excel, pdf等多种展现方式. 支持导出doc, excel, pdf和打印. 内置一个数据库demo, 很多报表类型的例子, 和非常丰富的教学文档供我们学习, 而且内置Tomcat, 可供我们在设计过程中就在网页中发布报表来查看预览. 安装安装过程也较为简单, 不在讲述 需要注意的是: Jdk: 安装中有个选择jdk版本的, 可以选择默认的1.5版本, 也可以自定义选择自己安装的jdk版本. 考虑到设计过程中JRE的兼容问题. 这里可以选择默认的版本, 因为里面的html展现最终是靠jsp来完成, 可以尽量避免设计中的问题. 授权: 在设计器打开和在发布预览中设计器都需要授权, 授权lic文件都可以在网上找到. 安装完成打开设计器如图: 设计设计过程一般分为 绘制表图, 填写字段和表达式, 配置数据源, 定义数据源和参数, 集成项目等步骤. 1. 绘制表图为了使最后展现给用户的报表比较满意, 我们可能需要进行显示格式, 段落, 缩进, 字体的设置. 以及最后的打印, 分页都可以在报表属性中设置. 2. 填写字段和表达式前两个过程较为简单, 我用程序提供的小例子. 效果如下: 在A2单元格中, 相当于一个游标, 来纵向扩展数据. 当然也支持横向扩展, 支持自己选择. 在做分组报表和卡式报表时候, 有一个左/上主格和附属格(子格)的概念: 主格: 原来的扩展单元格. 附属格: 跟随主格扩展的单元格 左主格: 只有纵向扩展才有的概念, 右边的单元格跟随左边的单元格进行纵向扩展, 这个左边被跟随的单元格就是左主格. 其他跟随的单元格叫做附属格. 上主格: 和左主格概念类似, 只不过是在横向扩展中, 称下边附属格上面的即为上主格. 3. 配置数据源默认的数据源是润乾自带的demo数据库, 可供我们学习和参考. 在实际设计中, 我们通常需要配置自己的数据源, 在新建中选择驱动, 填写数据源URL, 口令和密码即可. 4. 定义数据源和参数这是比较关键的一步. 在配置中, 可以进行数据集的设置. 数据集支持SQL检索, 存储过程, 复杂SQL等多种类型. 其中SQL检索支持可以检索到你的数据库, 可以用选择的方式进行数据库中 表, 列, 条件, 排序, 参数的设置. 复杂SQL是比较常用的一种数据集创建方式. 直接在里面编写sql就好啦阿. 如果传参的话, 有普通传参和宏传参两种方式. 普通传参(数据集中):定义: select name, age form student where xxx = ?参数: 参数表达式: id , 结果类型: 字符串.配置-&gt;参数: 也要配置一下. 宏传参:定义: select name, age from student where ${maceventid}配置-&gt;宏定义: 也要配置一下. 以上步骤做完, 点击发布报表, 会生成一个raq文件. 这个就是最终要集成到项目的文件. 5. 集成项目关于报表在项目中的配置文件: reportConfig.xml 配置属性很多, 就不一一列举, 就举几个我配置过的属性: jdbc-ds-configs: 配置数据库信息 maxCellNum: 在我集成过程中遇到一个错误, cell num exceeds limit. 意思是单元格数超过限制, 但是我的单元格数据量并不大, 为什么会出现这个错误. 查阅资料, 发现xml文件中有个设置:maxCellNum. 它是一个并发控制的选项, 取决于内存大小. 内存越大可以设置的越大, 不超过2000000. 1234&lt;config&gt; &lt;name&gt;maxCellNum&lt;/name&gt; &lt;value&gt;100000&lt;/value&gt;&lt;/config&gt; 再设计器设计过程中-&gt;报表属性中有个’报表格数’ 属性, 可以设置它来规定单元格数的限制. 如果不设置. 默认会按defaultCellNum来. 如果没有defaultCellNum, 则会按20000来算, 我的xml中并没有设置它, 所以发生这个错误. 但是在后面做项目过程中, 其他人没有设置居然没有报这个错. 也很无解 1234&lt;config&gt; &lt;name&gt;defaultCellNum&lt;/name&gt; &lt;value&gt;9999&lt;/value&gt;&lt;/config&gt; 关于它, 还有更加详细的并发控制.附上链接:http://wenku.baidu.com/link?url=J6iqHMVKd2FDgnBAFsWfdUoHkaEdLeqFlhp4nOB0AumdWQ72-EInuncQ71jpMvjLOcCsZbt7muhxH6kj2AdS3PuF6LFIlfdxnbYWyTb7qsq errorPage 这个属性来指定异常页, 可以先把myErrorPage.jsp拷贝到项目里. 用来查看异常信息. 比直接看错误页面好多了. 1234&lt;config&gt; &lt;name&gt;errorPage&lt;/name&gt; &lt;value&gt;/myErrorPage.jsp&lt;/value&gt;&lt;/config&gt; 除此之外, 还需要拷贝一些必要的文件.web.xml中report的配置 1234567891011121314151617181920212223242526&lt;!-- report --&gt; &lt;servlet&gt; &lt;servlet-name&gt;SetContextServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;com.runqian.util.webutil.SetContextServlet&lt;/servlet-class&gt; &lt;load-on-startup&gt;2&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet&gt; &lt;servlet-name&gt;reportServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;com.runqian.report4.view.ReportServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;configFile&lt;/param-name&gt; &lt;param-value&gt;/WEB-INF/report/reportConfig.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;reportServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/reportServlet&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;jsp-config&gt; &lt;taglib&gt; &lt;taglib-uri&gt;/WEB-INF/report/runqianReport4.tld&lt;/taglib-uri&gt; &lt;taglib-location&gt;/WEB-INF/report/runqianReport4.tld&lt;/taglib-location&gt; &lt;/taglib&gt; &lt;/jsp-config&gt; 下面看看最主要的文件吧 首先, 我们设计出来的raq文件怎么在html中展现呢? 只需要在你的请求url上, 映射到showReport.jsp上, url上一般还附带着请求的传参, 和raq文件名. 例如: showReport.jsp?params=”+param+”&amp;reqName=”+raqName; 在showReport.jsp中: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586&lt;%@ page contentType=\"text/html;charset=GBK\" %&gt;&lt;%@ taglib uri=\"/WEB-INF/runqianReport4.tld\" prefix=\"report\" %&gt;&lt;%@ page import=\"java.io.*\"%&gt;&lt;%@ page import=\"java.util.*\"%&gt;&lt;%@ page import=\"com.runqian.report4.usermodel.Context\"%&gt;&lt;html&gt;&lt;head&gt;&lt;link type=\"text/css\" href=\"css/style.css\" rel=\"stylesheet\"/&gt;&lt;/head&gt;&lt;body topmargin=0 leftmargin=0 rightmargin=0 bottomMargin=0 style=\"overflow:hidden;\"&gt;&lt;% request.setCharacterEncoding( \"GBK\" ); String report = request.getParameter( \"raq\" ); String reportFileHome=Context.getInitCtx().getMainDir(); StringBuffer param=new StringBuffer(); //保证报表名称的完整性 int iTmp = 0; if( (iTmp = report.lastIndexOf(\".raq\")) &lt;= 0 ){ report = report + \".raq\"; iTmp = 0; } Enumeration paramNames = request.getParameterNames(); if(paramNames!=null){ while(paramNames.hasMoreElements()){ String paramName = (String) paramNames.nextElement(); String paramValue=request.getParameter(paramName); if(paramValue!=null){ //把参数拼成name=value;name2=value2;.....的形式 param.append(paramName).append(\"=\").append(paramValue).append(\";\"); } } } //以下代码是检测这个报表是否有相应的参数模板 String paramFile = report.substring(0,iTmp)+\"_arg.raq\"; File f=new File(application.getRealPath(reportFileHome+ File.separator +paramFile));%&gt;&lt;jsp:include page=\"toolbar.jsp\" flush=\"false\" /&gt;&lt;div style=\"overflow:auto;height:expression(parent.document.body.clientHeight-33+'px');\"&gt;&lt;table id=\"rpt\" align=\"left\" &gt;&lt;tr&gt;&lt;td&gt;&lt;% //如果参数模板存在，则显示参数模板 if( f.exists() ) { %&gt; &lt;table id=\"param_tbl\" width=\"100%\" height=\"100%\"&gt;&lt;tr&gt;&lt;td&gt; &lt;report:param name=\"form1\" paramFileName=\"&lt;%=paramFile%&gt;\" needSubmit=\"no\" params=\"&lt;%=param.toString()%&gt;\" /&gt; &lt;/td&gt; &lt;td&gt;&lt;a href=\"javascript:_submit( form1 )\"&gt;&lt;img src=\"../images/search.gif\" border=no style=\"vertical-align:middle\"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt; &lt;% }%&gt;&lt;table align=\"center\" width=\"100%\" height=\"100%\" &gt; &lt;tr&gt;&lt;td&gt; &lt;report:html name=\"report1\" reportFileName=\"&lt;%=report%&gt;\" funcBarLocation=\"top\" needPageMark=\"yes\" generateParamForm=\"no\" params=\"&lt;%=param.toString()%&gt;\" needPivot=\"yes\" pivotLabel=\"\" exceptionPage=\"/reportJsp/myError2.jsp\" appletJarName=\"runqianReport4Applet.jar,dmGraphApplet.jar\" pageMarkLabel=\"\" firstPageLabel=\"\" prevPageLabel=\"\" nextPageLabel=\"\" lastPageLabel=\"\" /&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;&lt;script language=\"javascript\"&gt; //设置分页显示值 document.getElementById( \"t_page_span\" ).innerHTML=report1_getTotalPage(); document.getElementById( \"c_page_span\" ).innerHTML=report1_getCurrPage();&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 其中最关键的两个参数是: raqName: 接收的是raq的全文件名. param: 接收的是:“eventid=” + eventid 除此之外, 还需要把该jsp需要引入的jsp和静态资源加进来. 如toolbar.jsp, style.css等. 总结润乾报表是一个国产收费的报表工具, 工作中遇到了, 承认东西很强 但是不喜欢, 所以不想做过多研究","link":"/2016/09/15/报表开发(润乾报表)/"},{"title":"探秘Java中String、StringBuilder以及StringBuffer","text":"相信String这个类是Java中使用得最频繁的类之一，并且又是各大公司面试喜欢问到的地方，今天就来和大家一起学习一下String、StringBuilder和StringBuffer这几个类，分析它们的异同点以及了解各个类适用的场景。 原文出处：海子","link":"/2013/02/15/探秘Java中String、StringBuilder以及StringBuffer/"},{"title":"更换icarus主题记录","text":"[TOC] 前言终于把用了两年多的NexT主题换掉了, 之前喜欢这个主题是因为黑白两色显得特别简洁! 但是已经看腻了哈哈哈哈哈哈哈哈哈哈哈哈, 换成了稍微小众点的icarus NexT: https://github.com/iissnan/hexo-theme-next icarus: https://github.com/ppoffice/hexo-theme-icarus 两者效果图如下: NexT icarus 相较NexT, icarus使用的人数没有很多, 想要改什么在网上搜到的基本都是关于NexT的 虽然icarus也提供了很多配置, 但还是有些地方想按照自己意思做些修改, 强迫症~ 有些无法通过配置完成的, 只能改源码了 作为后端开发前端知识匮乏, 还好icarus使用的是ejs, 一种模版语言, 类似以前用过的FreeMarker和Thymeleaf 变动日志主题配置文件_config.yml的更改不记录了, 可参考文档: Documentation 主要记录源码部分的变动如下: 1. 修改navbar导航栏左边的logo配置方式因为不会设计Logo, 就改成”icon+文字”的方式, 并加入logo.img配置项 themes/hexo-theme-icarus/layout/common/navbar.ejs 2. 修改navbar导航栏右边的搜索功能原版2.3.0只有一个小的搜索icon, 加入搜索输入框并嵌入搜索icon themes/hexo-theme-icarus/layout/common/navbar.ejs themes/hexo-theme-icarus/source/css/style.styl 3. 修改个人信息页中的几个links原版是通过socialLinks动态配置的, 不支持微信, 码云, 微博这几个常用, 这里为了方便我使用&lt;a&gt;+&lt;img&gt;标签写死 themes/hexo-theme-icarus/layout/widget/profile.ejs 4. 友情链接标题前加入icon, 为了好看 themes/hexo-theme-icarus/layout/widget/links.ejs 5. 修改文章页(index页和post页)的文章时间加入判断, 如果是列表页显示例如几月前, 文章页显示具体日期, 例如2018-12-22 themes/hexo-theme-icarus/layout/common/article.ejs 6. 修改文章详情页面不显示文章图片thumbnail在阅读文章时感觉有点花, 默认是index页和post页都会显示 themes/hexo-theme-icarus/layout/common/article.ejs 7. 修改首页文章列表摘要信息不显示样式去掉Markdown生成的html标签, 类似简书上的文章排版, 整洁一点 themes/hexo-theme-icarus/layout/common/article.ejs 8. 修改文章页面布局原版的主页和文章页都使用三栏布局, 在文章页阅读会显得内容很窄, 尤其是代码部分, 需要左右滚动, 故修改文章页为两栏布局 themes/hexo-theme-icarus/includes/helpers/layout.js themes/hexo-theme-icarus/layout/common/widget.ejs themes/hexo-theme-icarus/layout/layout.ejs themes/hexo-theme-icarus/source/css/style.styl 9. 目录的开启方式改为默认就开启文章目录这样可以不用每个md文件都去写toc: true themes/hexo-theme-icarus/includes/helpers/config.js 10. 修改开启目录后的显示问题默认目录在滚动文章时如果太长会显示不全, 所以增加目录粘性布局 themes/hexo-theme-icarus/layout/widget/toc.ejs 11. 文章页增加版权声明 themes/hexo-theme-icarus/layout/common/article.ejs themes/hexo-theme-icarus/source/css/style.styl 12. 修改底部footer的显示信息 themes/hexo-theme-icarus/layout/common/footer.ejs 配合gulp压缩主要是为了在hexo generate到public目录后, 压缩html, css, js等资源 经过压缩, 我的public目录大小从8MB降到5MB, 还是可以的 第一次用压缩工具, 记录下gulp的安装和使用, 及配合hexo icarus主题进行压缩时的几个问题 安装12npm install gulp --savenpm install gulp -g 还需要以下模块 gulp-htmlclean: 清理html gulp-htmlmin: 压缩html gulp-minify-css: 压缩css gulp-uglify: 混淆js gulp-imagemin: 压缩图片 执行安装命令 1npm install gulp-htmlclean gulp-htmlmin gulp-minify-css gulp-uglify gulp-imagemin --save 最好在安装一个可以打印错误日志的工具, 之后会用到: 1npm install --save-dev gulp-util 建立任务在hexo根目录建立文件gulpfile.js, 内容如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970var gulp = require('gulp');var minifycss = require('gulp-minify-css');var uglify = require('gulp-uglify');var htmlmin = require('gulp-htmlmin');var htmlclean = require('gulp-htmlclean');var imagemin = require('gulp-imagemin');var gutil = require('gulp-util');// 压缩htmlgulp.task('minify-html', function() { return gulp.src('./public/**/*.html') .pipe(htmlclean()) .pipe(htmlmin({ removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, })) .pipe(gulp.dest('./public'))});// 压缩htmlgulp.task('minify-xml', function() { return gulp.src('./public/**/*.xml') .pipe(htmlclean()) .pipe(htmlmin({ removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, })) .pipe(gulp.dest('./public'))});// 压缩cssgulp.task('minify-css', function() { return gulp.src('./public/**/*.css') .pipe(minifycss({ compatibility: 'ie8' })) .pipe(gulp.dest('./public'));});// 压缩jsgulp.task('minify-js', function() { return gulp.src('./public/js/**/*.js') .pipe(uglify()) .pipe(gulp.dest('./public'));});// 压缩jsgulp.task('minify-js', function() { return gulp.src('./public/js/**/*.js') .pipe(uglify()) .on('error', function (err) { gutil.log(gutil.colors.red('[Error]'), err.toString()); }) .pipe(gulp.dest('./public'));});// 压缩图片gulp.task('minify-images', function() { return gulp.src('./public/img/**/*.*') .pipe(imagemin( [imagemin.gifsicle({'optimizationLevel': 3}), imagemin.jpegtran({'progressive': true}), imagemin.optipng({'optimizationLevel': 7}), imagemin.svgo()], {'verbose': true})) .pipe(gulp.dest('./public/img'))});// 默认任务gulp.task('default', [ 'minify-html','minify-xml','minify-css','minify-js','minify-images']); 问题一: gulp版本在Hexo根目录执行gulp, 错误如下: 1AssertionError: Task function must be specified。 版本问题导致的, 可以查看下gulp版本: gulp -v 修改package.json中的gulp版本为3.x, 例如: 1234\"dependencies\": { \"gulp\": \"^3.9.1\", // ...} 然后重新安装gulp: npm install gulp 问题二: icarus主题中的js语法问题接下来gulp可能会发生如下错误: 1GulpUglifyError: unable to minify JavaScript 原因是javascirpt语法问题，在es5环境里使用了es6、es7语法 因为上面安装部分和gulpfile.js中已经添加了错误打印, 可以看到具体的错误信息 我修改了如下js文件: themes/hexo-theme-icarus/source/js/back-to-top.js themes/hexo-theme-icarus/source/js/clipboard.js themes/hexo-theme-icarus/source/js/main.js 然后就好啦","link":"/2019/04/02/更换主题icarus/"},{"title":"源码分析——客户端负载Netflix Ribbon","text":"[TOC] 前言Ribbon是由Netflix OSS开源的负载均衡组件 Spring Cloud将其整合作为客户端侧的(client-side)负载均衡组件, 以类库的形式集成于消费者客户端内 你在Spring Cloud整合Eureka, Feign, Zuul的组件中都可以见到它的影子 关于负载的作用与分类模式可以参考: 服务发现——需求与模式 在Netflix的场景中, Ribbon与Eureka都是作用在中间层的服务, 在终端接入侧的Edge Service负载仍然由亚马逊ELB服务提供 AWS 弹性负载均衡服务是边界服务的负载均衡解决方案，边界服务是向终端用户访问 Web 而开放的。Eureka 填补了中间层负载均衡的空缺。虽然，理论上可以将中间层服务直接挂在 AWS 弹性负载均衡器后面，但这样会将它们直接开放给外部世界，从而失去了 AWS 安全组的所有好处。—— 摘自Netflix 目的阅读源码的目为了解决以下几个疑问 Ribbon获取到服务列表后是如何负载? 如何路由到目标服务的? Ribbon中的负载均衡策略有哪些? —— 网上很多说明 Ribbon是怎么获取/更新服务列表的? Ribbon的工作流程 Spring RestTemplate是如何具备LB能力的, 它与Ribbon有什么联系 说明版本: Netflix Ribbon的源码依然是Servlet应用, 为了调试方便我还是在Spring Cloud的集成工程中, 源码仅作为参照方便查找 所以准确讲, 并不是单独分析, 而是Spring Cloud Netflix Ribbon Spring Cloud: Finchley.SR2 对应Netflix Ribbon: v2.2.5 1$ git checkout -b v2.2.5 v2.2.5 名词: NIWS: Netflix Internal Web Service Framework 核心组件 the beans that Spring Cloud Netflix provides by default for Ribbon: Bean Type Bean Name Class Name (default) IClientConfig ribbonClientConfig DefaultClientConfigImpl IRule ribbonRule ZoneAvoidanceRule IPing ribbonPing DummyPing ServerList&lt;Server&gt; ribbonServerList ConfigurationBasedServerList ServerListFilter&lt;Server&gt; ribbonServerListFilter ZonePreferenceServerListFilter ILoadBalancer ribbonLoadBalancer ZoneAwareLoadBalancer ServerListUpdater ribbonServerListUpdater PollingServerListUpdater 以上几个组件在源码分析过程会提到 Ribbon核心源码调试代码为了在方便调试源码, 我准备一个很简单的代码123456789@Autowiredprivate LoadBalancerClient loadBalancerClient;public String getHello() { RestTemplate restTemplate = new RestTemplate(); ServiceInstance serviceInstance = loadBalancerClient.choose(\"HELLO-SERVICE\"); String url = \"http://\" + serviceInstance.getHost() + \":\" + serviceInstance.getPort(); return restTemplate.getForObject(url, String.class);} 路由 &amp; 负载LoadBalancerClientRibbonLoadBalancerClient作为Ribbon负载逻辑的重要实现类, 看它之前, 先来看看它的接口声明 继承结构 接口定义 12345678910111213public interface LoadBalancerClient extends ServiceInstanceChooser { &lt;T&gt; T execute(String serviceId, LoadBalancerRequest&lt;T&gt; request) throws IOException; &lt;T&gt; T execute(String serviceId, ServiceInstance serviceInstance, LoadBalancerRequest&lt;T&gt; request) throws IOException; URI reconstructURI(ServiceInstance instance, URI original);}public interface ServiceInstanceChooser { ServiceInstance choose(String serviceId);} 解释: LoadBalancerClient execute(): 两个重载方法根据服务实例(serviceInstance)执行请求 reconstructURI(): 重构URI 例如http://cloudlink-user/priUser/getByIds重构为http://SC-201707142304:8802/priUser/getByIds ServiceInstanceChooser choose(String serviceId): 即依据LoadBalancer选择并返回服务ID对应的服务实例 源码中有比较详细的注释 显而易见RibbonLoadBalancerClient#choose是应该重点分析的方法 它调用了getServer()方法, 经过几个函数重载, 最终调用了ILoadBalancer中的实现 默认的实现其实是com.netflix.loadbalancer.ZoneAwareLoadBalancer#chooseServer —— 参考配置类: RibbonClientConfiguration#ribbonLoadBalancer 它是BaseLoadBalancer的子类, 因为我的环境只有一个可用区zone, 所以不会走区域相关的负载逻辑, 直接调用父类super.chooseServer(key), 即BaseLoadBalancer#chooseServer 12345678910111213141516public Server chooseServer(Object key) { if (counter == null) { counter = createCounter(); } counter.increment(); if (rule == null) { return null; } else { try { return rule.choose(key); // look me ! } catch (Exception e) { logger.warn(\"LoadBalancer [{}]: Error choosing server for key {}\", name, key, e); return null; } }} IRule接着上面的, 略过非重点代码, 看rule.choose(key) 其中的rule就是Ribbon中负载均衡策略的核心接口IRule , 对应前面核心组件中提到的Rule—负载均衡策略 接口定义如下: 12345678public interface IRule{ public Server choose(Object key); public void setLoadBalancer(ILoadBalancer lb); public ILoadBalancer getLoadBalancer(); } 除了choose(), 接口ILoadBalancer中定义了负载均衡器中的操作方法 123456789101112131415public interface ILoadBalancer { public void addServers(List&lt;Server&gt; newServers); public Server chooseServer(Object key); public void markServerDown(Server server); @Deprecated public List&lt;Server&gt; getServerList(boolean availableOnly); public List&lt;Server&gt; getReachableServers(); public List&lt;Server&gt; getAllServers();} IRule的抽象子类AbstractLoadBalancerRule中完成了ILoadBalancer的setter&amp;getter 回到chooseServer()中, 作为BaseLoadBalancer类的成员变量, 默认值定义如下:12private final static IRule DEFAULT_RULE = new RoundRobinRule();protected IRule rule = DEFAULT_RULE; 也就是默认的策略为轮询(RoundRobin) 看看其实现: com.netflix.loadbalancer.RoundRobinRule#choose(ILoadBalancer, Object), 代码省略… 可以看到在该方法中调用了ILoadBalancer#getAllServers获取到服务列表, 通过一个线程安全的计数算法从中取出一个活着的Server返回 继承结构 从源码中可以看到IRule的实现有很多很多, 对应多种负载均衡策略的种类 网上有针对每种策略的解释, 不说了 至此, 已经分析了Ribbon从多个服务中依据某种负载策略选择一个进行调用 回答了开头问题1: Ribbon获取到服务列表后是如何负载? 如何路由到目标服务的? 获取 &amp; 更新服务下面来分析问题2: Ribbon是怎么获取/更新服务列表的? 在consumer客户端进行第一次调用provider服务时, 能看到这样一行日志: 1234INFO c.n.l.DynamicServerListLoadBalancer : DynamicServerListLoadBalancer for client cloudlink-user initialized: DynamicServerListLoadBalancer:{ NFLoadBalancer:name=cloudlink-user,current list of Servers=[SC-201707142304:8801, SC-201707142304:8802], ...} 日志中打印了服务提供者的信息 在源码中查看, 来自于com.netflix.loadbalancer.DynamicServerListLoadBalancer#restOfInit, 调用栈如下: 1234567org.springframework.cloud.netflix.ribbon.RibbonClientConfiguration#ribbonLoadBalancer ↓com.netflix.loadbalancer.ZoneAwareLoadBalancer#ZoneAwareLoadBalancer ↓com.netflix.loadbalancer.DynamicServerListLoadBalancer#DynamicServerListLoadBalancer ↓com.netflix.loadbalancer.DynamicServerListLoadBalancer#restOfInit 最后在restOfInit()中调用了updateListOfServers(), 从字面意思看, 它与获取服务实例有关 com.netflix.loadbalancer.DynamicServerListLoadBalancer#updateListOfServers 12345678910111213141516@VisibleForTestingpublic void updateListOfServers() { List&lt;T&gt; servers = new ArrayList&lt;T&gt;(); if (serverListImpl != null) { servers = serverListImpl.getUpdatedListOfServers(); // look me ! LOGGER.debug(\"List of Servers for {} obtained from Discovery client: {}\", getIdentifier(), servers); if (filter != null) { servers = filter.getFilteredListOfServers(servers); LOGGER.debug(\"Filtered List of Servers for {} obtained from Discovery client: {}\", getIdentifier(), servers); } } updateAllServerList(servers);} 从这段代码能够推断出两点: 获取到的服务集合信息servers来自于ServerList的实现类 ServerList接口中声明了获取服务列表的方法以及更新服务列表(30秒)的方法 —— 下面 其实从上面那行debug日志已经能够看出服务列表信息来自于服务发现客户端 我们只需要进入ServerList的实现类证明即可 ServerList这里看到了Ribbon中的另一个核心组件: ServerList 即服务列表, 用于获取地址列表 在与Spring Cloud Eureka集成的服务中, 它是从注册中心拉取的并能够动态更新的服务列表清单 当然, 在捕鱼Eureka集成的情况下可以配制成静态的地址列表 123456789101112public interface ServerList&lt;T extends Server&gt; { public List&lt;T&gt; getInitialListOfServers(); /** * Return updated list of servers. This is called say every 30 secs * (configurable) by the Loadbalancer's Ping cycle * */ public List&lt;T&gt; getUpdatedListOfServers(); } 找到其实现: com.netflix.niws.loadbalancer.DiscoveryEnabledNIWSServerList 可以看到以上两个方法的实现都调用了同一个方法obtainServersViaDiscovery() 代码很长, 简化一下大致如下:1234List&lt;DiscoveryEnabledServer&gt; serverList = new ArrayList&lt;DiscoveryEnabledServer&gt;();EurekaClient eurekaClient = eurekaClientProvider.get();List&lt;InstanceInfo&gt; instanceInfo = eurekaClient.getInstancesByVipAddress(vipAddress, isSecure, targetRegion);serverList.add(new DiscoveryEnabledServer(instanceInfo);); 还能看到在调用getUpdatedListOfServers()得到servers后, 并没有立即返回 而是又去执行了filter.getFilteredListOfServers(servers);过滤后返回 至此, 已经能够证明Ribbon获取服务提供者的信息来自于Eureka Server ServerListFilterRibbon中另一个组件: ServerListFilter, 负责服务列表过滤 仅当使用动态ServerList时使用, 用于在原始的服务列表中使用一定策略过虑掉一部分地址 12345public interface ServerListFilter&lt;T extends Server&gt; { public List&lt;T&gt; getFilteredListOfServers(List&lt;T&gt; servers);} 默认的过滤器为ZonePreferenceServerListFilter, 会过滤出同区域的服务实例, 也就是区域优先 从ServerListFilter的实现类可以找到, Netflix Ribbon中还提供了其它的过滤规则, 不说了 ServerListUpdater在DynamicServerListLoadBalancer中可以看到ServerListUpdater的定义 123456789protected final ServerListUpdater.UpdateAction updateAction = new ServerListUpdater.UpdateAction() { @Override public void doUpdate() { updateListOfServers(); }};protected volatile ServerListUpdater serverListUpdater; 它定义了服务列表ServerList的动态更新, 相当于一个服务更新器 Implement 1(default): PollingServerListUpdater Implement 2: EurekaNotificationServerListUpdater 默认的动态更新策略为PollingServerListUpdater, 会执行一个定时任务, 源码中定义了默认执行周期为30s IPing接口定义中只有一个方法, 检测服务是否活着 1234public interface IPing { public boolean isAlive(Server server);} 在Ribbon中的默认实现为DummyPing, 也就是假Ping, 一直返回True 在与Eureka集成使用中, 默认实现为NIWSDiscoveryPing, 会根据服务实例的状态判断该实例是否活着: 123456// ...InstanceStatus status = instanceInfo.getStatus();if (status!=null){ isAlive = status.equals(InstanceStatus.UP);}return isAlive; 也是以定时任务周期执行, 源码可参考: com.netflix.loadbalancer.BaseLoadBalancer.PingTask RestTemplate &amp; LB介绍在Spring Cloud中的服务间通信场景, 除了可以使用声明式的REST客户端Feign, 也可以使用从Spring 3.0引入的RestTemplate RestTemplate中封装了简单易用的API, 其实现默认是封装JDK原生的HTTP客户端URLConnection 如果你想替换实现为HttpClient, OkHttpClient, 加入对应的依赖和显示的配置即可, Spring Cloud已对其做好了自动配置 参考: org.springframework.cloud.commons.httpclient.HttpClientConfiguration 例如替换为OkHttp3Client pom依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.squareup.okhttp3&lt;/groupId&gt; &lt;artifactId&gt;okhttp&lt;/artifactId&gt; &lt;version&gt;3.9.1&lt;/version&gt;&lt;/dependency&gt; 显示配置 1234567@Bean@LoadBalancedpublic RestTemplate restTemplate() { RestTemplate restTemplate = new RestTemplate(); restTemplate.setRequestFactory(new OkHttp3ClientHttpRequestFactory()); return restTemplate;} 那么问题来了, 服务间的调用和正常HttpClient远程调用还是有区别的 RestTemplate是如何具备从多个服务中挑选一个进行路由的能力? 它和Ribbon有什么关系? 我们知道, 有了@LoadBalanced注释, RestTemplate才能通过逻辑服务名的方式进行调用, 否则会是UnknowHostException 所以猜想RestTemplate在某一步会被赋予客户端负载的能力 源码分析在LoadBalanced同路径下, 可以看到如下两个类: LoadBalancerAutoConfiguration LoadBalancerInterceptor 关注如下代码:12345678910111213141516171819202122232425262728293031323334public class LoadBalancerAutoConfiguration { @LoadBalanced @Autowired(required = false) private List&lt;RestTemplate&gt; restTemplates = Collections.emptyList(); @Bean public SmartInitializingSingleton loadBalancedRestTemplateInitializerDeprecated( final ObjectProvider&lt;List&lt;RestTemplateCustomizer&gt;&gt; restTemplateCustomizers) { return () -&gt; restTemplateCustomizers.ifAvailable(customizers -&gt; { for (RestTemplate restTemplate : LoadBalancerAutoConfiguration.this.restTemplates) { for (RestTemplateCustomizer customizer : customizers) { customizer.customize(restTemplate); // look me ! } } }); } //... static class LoadBalancerInterceptorConfig { //... @Bean @ConditionalOnMissingBean public RestTemplateCustomizer restTemplateCustomizer(final LoadBalancerInterceptor loadBalancerInterceptor) { return restTemplate -&gt; { List&lt;ClientHttpRequestInterceptor&gt; list = new ArrayList&lt;&gt;(restTemplate.getInterceptors()); list.add(loadBalancerInterceptor); // look me ! restTemplate.setInterceptors(list); }; } }} 其中最关键的就是restTemplate.setInterceptors(list)了, 为restTemplate添加了一个拦截器, 在发起HTTP请求时进行拦截 而拦截后要实现的功能肯定就是完成客户端负载, 进入该拦截器:1234567891011121314public class LoadBalancerInterceptor implements ClientHttpRequestInterceptor { private LoadBalancerClient loadBalancer; //... @Override public ClientHttpResponse intercept(final HttpRequest request, final byte[] body, final ClientHttpRequestExecution execution) throws IOException { final URI originalUri = request.getURI(); String serviceName = originalUri.getHost(); Assert.state(serviceName != null, \"Request URI does not contain a valid hostname: \" + originalUri); return this.loadBalancer.execute(serviceName, requestFactory.createRequest(request, body, execution)); }} 可以看到intercept方法内传入了serviceName, request, 最终调用了LoadBalancerClient#execute() 其中host取到的是URL中填写的服务名 e.g. http://cloudlink-user/priUser/getByIds中服务名为cloudlink-user 服务名serviceName取的是host 那接下来就是根据serviceName, 根据负载找到一个服务实例进行路由, 又回到上面的 路由&amp;负载 过程啦 简化一下用一段代码来手动模拟, 就是这个样子12345678910@Autowiredprivate LoadBalancerClient loadBalancerClient;public List method() { RestTemplate restTemplate = new RestTemplate(); List&lt;ClientHttpRequestInterceptor&gt; interceptors = restTemplate.getInterceptors(); interceptors.add(new LoadBalancerInterceptor(loadBalancerClient)); restTemplate.setInterceptors(interceptors); return restTemplate.postForObject(\"http://cloudlink-user/priUser/getByIds\", userIds, List.class);} 总结源码没有很深入, 仅仅为了了解Ribbon工作流程和其中一些细节 整个流程还是比较清晰, 按照顺序大致分为: 获取服务列表信息 执行定时任务动态更新/检查/剔除服务 对获取到的服务列表进行过滤 按照负载均衡策略在多个服务中选择一个服务进行调用 从中也能看出, 在服务实例发生变化时, Consumer端并不能立刻感知到, 而是有一定的延迟, 可能会继续调用而报错 与Eureka相同, 再次看出CAP中保证了AP而适当牺牲C, 所以无论是服务网关路由到我们的服务, 还是服务消费者调用提供者. 作为Eureka Client调用侧, 尽可能的加入一定重试. 通过配置可以实现这一点","link":"/2019/04/23/源码分析——软负载实现Netflix Ribbon/"},{"title":"蓝桥杯算法练习 - 分解质因数","text":"问题描述： 求出区间[a,b]中所有整数的质因数分解。 输入格式： 输入两个整数a，b。 输出格式： 每行输出一个数的分解，形如k=a1a2a3…(a1&lt;=a2&lt;=a3…，k也是从小到大的)(具体可看样例) 样例输入： 3 10 样例输出： 3=34=225=56=237=78=2229=3310=25 提示： 先筛出所有素数，然后再分解。 数据规模和约定： 2&lt;=a&lt;=b&lt;=10000 实现：主函数Main12345678910111213141516public class ResolvePrimeFactor { public static void main(String[] args) { System.out.println(\"Please input startNum endNum:\"); Scanner scanner = new Scanner(System.in); int start = scanner.nextInt(); int end = scanner.nextInt(); for (int i = start; i &lt;= end; ++i) { System.out.print(i + \"=\"); fun(i); System.out.println(); } }} 普通方式-循环 普通方式, 循环 1234567891011121314151617public static void fun(int n) { int k = 2; // --定义一个标 k while (k &lt;= n) { if (n % k == 0) { System.out.print(k); // 若后面还有 项, 输出\"*\" 后继续判断 n = n / k; if (k &lt;= n) { System.out.print(\"*\"); } } else { k++; } }} 递归方式 一 递归方法 一: (while ..) 自己写的递归， 略繁琐 12345678910111213141516public static void recfun(int n) { int k = 2; while (k &lt;= n) { if (n % k == 0) { System.out.print(k); if (k &lt;= n / k) { System.out.print(\"*\"); recfun(n / k); return; } n = n / k; } else { k++; } }} 递归方式 二 递归方法二: (for…) 4行代码 12345678public static String recfun2(int n) { for (int i = 2; i &lt; n; ++i) { if (n % i == 0) { return i + \"*\" + recfun2(n / i); } } return \"\" + n;}","link":"/2014/11/14/蓝桥杯算法练习-分解质因数/"},{"title":"服务发现——需求与模式","text":"[TOC] 前言从自成一体的单体应用到分布式应用, 演进出了面向服务架构 服务发现的本质在于让服务之间发现彼此, 这是服务提供方与服务消费方完成消费的前提 在微服务架构下, 无论是使用Dubbo, Thrift, gRPC这类标准的PRC实现进行通讯, 还是微服务所倡导的RESTful风格API, 服务发现都是一个避不开的话题 需求来举例一个简单的场景: 在固定数量, 固定地址的情况下, Consumer当然可以将服务提供者的地址写死到自己的应用内, 按照随机或轮询的策略去访问服务提供者 但在云时代, 微服务的架构下作为服务提供者的实例不一定运行在传统的物理机/虚拟机上, 网络地址动态变化, 实例数量还可能依据访问流量进行动态伸缩 其中有两个最核心的问题: 消费者怎么找到服务提供者? ——服务发现 在找到多个提供者实例后, 选择哪个实例来消费服务? ——负载均衡 可见, 服务发现和负载均衡是分布式架构最根本的问题, 同样也应该作为微服务的基础框架/组件 方案目前主要分为两类模式: 客户端发现和服务端发现, 也就是client-side discovery pattern and server-side discovery pattern 下面来介绍几种典型模式 集中式(传统) 客户端嵌入式(进程内) 主机独立进程 这几种模式都有业界较好的实践方案及开源组件 一: 集中式 常用的独立LB可以使用硬件: F5 或软件: Nginx 等 独立LB中需要手工配置服务列表的地址, 例如在Nginx中配置服务列表:1234upstream service_provider { server 192.168.100.100:8080...; server 192.168.100.101:8080...;} 软负载的特点是配置灵活且易于扩展, 还可以与硬件负载均衡器性能稳定, 负载能力强的特点结合使用. 例如F5下配多个Nginx, 对其进行负载均衡, 多个Nginx再对后台服务进行负载均衡 这种方式重点提供的是LB能力, 而”服务发现”通常是在F5或Nginx上配置来实现的, 其功能对客户端来说是透明的 这种模式相对简单, 在中小型公司中是很主流的方案 二: 客户端嵌入式也可以称为进程内代理 这种模式下, 多了一个注册中心, 也是服务注册表的概念, 并且在客户端会集成一个类似SDK的东西: 对于服务提供者(Provider)来讲: 它需要注册到注册中心, 定期进行heartbeat检测 对于消费者(Consumer)来讲: 首先它能够从注册中心获取服务的实例, 也就是具备服务发现的能力其次获取到服务列表后选择一个进行调用, 也就是具备负载均衡的能力 通常情况下, 整个过程可以做到自发现和自注册, 无需其它代理的介入 三: 主机独立进程式 这种模式类似于上面一种, 区别在于提供服务发现和负载均衡能力的组件没有嵌入每个客户端内, 而是放在了同一主机下, 可以理解为同一主机下的不同进程 四: Other实际中除了上述三种模式外, 还有很多变种和折中方案, 例如集中式的代理结合服务注册中心实现服务的注册于发现 以注册中心Eureka Server为例, 可以提供UI/API对接发布系统或是手工注册, 以及定期的健康检查, 而无需在客户端集成Eureka Client, 集中式的Proxy需要同步注册中心中的服务地址 总结可以看到, 以上三种最典型的方案中, 相同点都是把具备服务发现和负载均衡能力的组件交给一个类似Proxy的角色中 而区别在于该角色所处的位置, 放入客户端内还是客户端外, 这也对应了两种分类: 客户端发现与服务端发现, 客户端负载与服务端负载 通过以上几种方案的说明, 我们可以理清它的分类: 优劣性每种方式都各有优略, 没有绝对, 主要从这几方面来对比 可用性 &amp; 性能 集中式方案中如果LB Proxy角色挂了, 那么影响的是所有客户端, 由于这种单点问题, 为了保证HA软负载通常需要部署多个, 好在Nginx无状态可水平扩展 进程内的方案如果客户端/Proxy挂了只对自己产生影响, 而主机独立进程的方案相对折中, 影响面仅为同一主机下的客户端 从图中能看出集中式的方案在服务的调用上会多一跳, 有一定的性能开销 语言栈 相较其它两种, 嵌入式的客户端需要对针对客户端做集成和改造才能实现自注册和自发现, 说的难听点就是需要对服务有一定的侵入 并且这种改造需针对每种语言栈的服务定制 Netflix Eureka &amp; Ribbon是该模式最典型的实现, 对于Java语言栈, 这一切都变得轻松. 微服务的异构旨在不同的数据库和语言栈, Eureka服务端提供了完善的REST API, 对于其它非Java语言栈只需实现自己语言对应的Eureka客户端程序, 就可以将非Java实现的服务纳入自己的服务治理体系 统一管控 从图中也能看出, 集中式方案可以做到集中式的访问控制, 而客户端进程内的模式则不易做到统一的管控, 主机独立进程的方式在两者之中 对比相信通过以上三种典型模式的了解, 对于业界常见的一些服务发现组件对应哪种模式应该很清楚了, 例如: 客户端发现: Netflix Eureka, Apache Dubbo, Motan 服务端发现: Nginx, Zookeeper, Kubernetes 而独立于主机进程的代理更像是Service Mesh的风格 模式 优点 缺点 适用场景 集中式代理 运维简单, 集中治理, 语言栈无关 配置麻烦, 单点问题, 多一跳有性能开销 具备一定运维能力 客户端嵌入式代理 无单点问题, 性能较好 客户端复杂, 不易集中治理, 语言栈相关 语言栈统一 主机独立进程代理 以上两者折中 运维部署较复杂 具备一定运维能力","link":"/2019/04/19/服务发现——需求与模式/"},{"title":"源码分析——服务发现组件Netflix Eureka","text":"[TOC] 前言结合Netflix Eureka 架构图, 简单分析这一服务注册和服务发现组件的源码 目的 结合Netflix Eureka架构图 从源码解读Eureka架构图中服务生命周期 Register: 服务注册 Renew: 服务续约 Cancel: 服务下线 Evict: 服务剔除(Servcer端) 从源码解读Eureka Peer Replicate过程 总结其一致性问题 说明环境准备源码获取: https://github.com/Netflix/eureka 项目结构如下:1234567891011eureka-clienteureka-client-archaius2eureka-client-jersey2eureka-coreeureka-core-jersey2eureka-exampleseureka-resourceseureka-servereureka-server-governatoreureka-test-utils... 本章节中只需要关注eureka-core和eureka-client两个模块 项目是纯Servlet应用, 使用Gradle构建, 这里我只是作为参照并没有构建, 在集成了Spring Cloud Netflix Eureka的项目中进行源码调试 Spring Cloud: Finchley.SR2 Spring Boot: 2.0.8.RELEASE 对应的Netflix Eureka版本为: v1.9.3 1$ git checkout -b v1.9.3 v1.9.3 在Spring Cloud集成Netflix Eureka的代码中, 还能看到两个与Eureka相关的包 spring-cloud-netflix-eureka-server spring-cloud-netflix-eureka-client 这部分代码被称为胶水代码, 包含一些原生代码针对Spring项目的支持, 一些默认配置等 以及我们最熟悉的 Eureka Dashboard 界面, 都是由这部分代码完成 架构图引用官方wiki中的架构图 所以源码分析的角度就是上图中画直线的部分, 可以总结为两部分 Eureka Server之间的Replicate Eureka Client 与 Eureka Server之间的Register, Renew, Cancel… 对于Eureka Client, 我们还应该区分这样两个逻辑角色 Service Provider: 服务提供者 Service Consumer: 服务消费者 无论客户端发现还是服务端发现, 服务发现的本质在于能够让服务之间发现彼此, 这是服务提供方与服务消费方完成消费的前提, 对应架构图中的Client端之间的Remote Call 源码解析 Eureka Server启动入口随着Eureka Server启动时的一行INFO日志Started Eureka Server 来到第一个入口点: EurekaServerInitializerConfiguration#start 123456789101112131415161718192021222324252627@Configurationpublic class EurekaServerInitializerConfiguration implements ServletContextAware, SmartLifecycle, Ordered { // ... @Override public void start() { new Thread(new Runnable() { @Override public void run() { try { //TODO: is this class even needed now? eurekaServerBootstrap.contextInitialized(EurekaServerInitializerConfiguration.this.servletContext); log.info(\"Started Eureka Server\"); publish(new EurekaRegistryAvailableEvent(getEurekaServerConfig())); EurekaServerInitializerConfiguration.this.running = true; publish(new EurekaServerStartedEvent(getEurekaServerConfig())); } catch (Exception ex) { // Help! log.error(\"Could not initialize Eureka servlet context\", ex); } } }).start(); } // ...} ^_^注释很有意思 这里看到了一个启动引导类EurekaServerBootstrap 123456789101112public void contextInitialized(ServletContext context) { try { initEurekaEnvironment(); initEurekaServerContext(); context.setAttribute(EurekaServerContext.class.getName(), this.serverContext); } catch (Throwable e) { log.error(\"Cannot bootstrap eureka server :\", e); throw new RuntimeException(\"Cannot bootstrap eureka server :\", e); }} 同样在eureka-core中也存在一个引导类EurekaBootStrap 这是Eureka Server的启动类, 代码与EurekaServerBootstrap基本类似 作用也基本相似, 都是完成eureka的初始化工作 不同的是EurekaBootStrap实现了Servlet API中的ServletContextListener接口 所以在容器启动时就会调用contextInitialized()方法完成init过程, 以及在终止时的contextDestroyed() 在这个引导类中, 还能够看到两个与配置相关的接口: EurekaServerConfig eurekaServerConfig; EurekaClientConfig eurekaClientConfig; 没错, Eureka Server 和 Eureka Client的配置都在这里, 他们分别对应两个实现类: DefaultEurekaServerConfig implements EurekaServerConfig EurekaServerConfigBean implements EurekaServerConfig 一个是提供默认配置的类, 在netflix eureka源码中, 另一个在Spring Cloud Netflix的整合代码中, 是添加了@ConfigurationProperties的属性配置类, 前缀为eureka.server 很熟悉吧? 对应于我们平时在配置文件中配置的eureka.server.xxx. Eureka Client端的EurekaClientConfig与之类似, 不说了 Resources API从上面的架构图可以看到, Eureka Client需要向Eureka Server进行注册, 发送心跳更新租约, 获取注册信息等; 同时Eureka Server之间也要进行注册信息的同步 所以在Eureka Server中, 一定有对外提供REST API的入口, 这就是Resources中的内容, 你可以认为是控制器~ 这部分代码在eureka-core中的com.netflix.eureka.resources中: ApplicationResource InstanceResource PeerReplicationResource etc… 通过类名上的@Path注解, 或是类名. 不难分辨他们分别代表什么资源 Register&amp;Renew&amp;Cancel来看一个服务注册-register的API, 在ApplicationResource中123456789101112@POST@Consumes({\"application/json\", \"application/xml\"})public Response addInstance(InstanceInfo info, @HeaderParam(PeerEurekaNode.HEADER_REPLICATION) String isReplication) { // validate required fields... // handle cases where clients may be registering with bad DataCenterInfo with missing data // 省略... registry.register(info, \"true\".equals(isReplication)); return Response.status(204).build(); // 204 to be backwards compatible} 我们可以模拟一个客户端调用看看: com.netflix.eureka.resources.ApplicationResource#getApplication GET http://localhost:8761/eureka/apps/CLOUDLINK-USER 返回结果如下: 12345678910111213141516171819202122232425262728293031323334353637&lt;application&gt; &lt;name&gt;CLOUDLINK-USER&lt;/name&gt; &lt;instance&gt; &lt;instanceId&gt;192.168.153.1:cloudlink-user:8801&lt;/instanceId&gt; &lt;hostName&gt;localhost&lt;/hostName&gt; &lt;app&gt;CLOUDLINK-USER&lt;/app&gt; &lt;ipAddr&gt;192.168.153.1&lt;/ipAddr&gt; &lt;status&gt;UP&lt;/status&gt; &lt;overriddenstatus&gt;UNKNOWN&lt;/overriddenstatus&gt; &lt;port enabled=\"true\"&gt;8801&lt;/port&gt; &lt;securePort enabled=\"false\"&gt;443&lt;/securePort&gt; &lt;countryId&gt;1&lt;/countryId&gt; &lt;dataCenterInfo class=\"com.netflix.appinfo.InstanceInfo$DefaultDataCenterInfo\"&gt; &lt;name&gt;MyOwn&lt;/name&gt; &lt;/dataCenterInfo&gt; &lt;leaseInfo&gt; &lt;renewalIntervalInSecs&gt;30&lt;/renewalIntervalInSecs&gt; &lt;durationInSecs&gt;90&lt;/durationInSecs&gt; &lt;registrationTimestamp&gt;1555462894529&lt;/registrationTimestamp&gt; &lt;lastRenewalTimestamp&gt;1555463085849&lt;/lastRenewalTimestamp&gt; &lt;evictionTimestamp&gt;0&lt;/evictionTimestamp&gt; &lt;serviceUpTimestamp&gt;1555462894529&lt;/serviceUpTimestamp&gt; &lt;/leaseInfo&gt; &lt;metadata&gt; &lt;management.port&gt;8801&lt;/management.port&gt; &lt;/metadata&gt; &lt;homePageUrl&gt;http://localhost:8801/&lt;/homePageUrl&gt; &lt;statusPageUrl&gt;http://localhost:8801/actuator/info&lt;/statusPageUrl&gt; &lt;healthCheckUrl&gt;http://localhost:8801/actuator/health&lt;/healthCheckUrl&gt; &lt;vipAddress&gt;cloudlink-user&lt;/vipAddress&gt; &lt;secureVipAddress&gt;cloudlink-user&lt;/secureVipAddress&gt; &lt;isCoordinatingDiscoveryServer&gt;false&lt;/isCoordinatingDiscoveryServer&gt; &lt;lastUpdatedTimestamp&gt;1555462894530&lt;/lastUpdatedTimestamp&gt; &lt;lastDirtyTimestamp&gt;1555462875800&lt;/lastDirtyTimestamp&gt; &lt;actionType&gt;ADDED&lt;/actionType&gt; &lt;/instance&gt;&lt;/application&gt; 通过查看这几个Resource类的API不难发现, 真正的逻辑都在一个PeerAwareInstanceRegistry registry中完成 注意区分registry和register 来看下它的继承和实现关系 其中LeaseManager中定义了实例注册(register), 续约(renew), 取消(cancel), 剔除(evict)的声明 12345678910public interface LeaseManager&lt;T&gt; { void register(T r, int leaseDuration, boolean isReplication); boolean cancel(String appName, String id, boolean isReplication); boolean renew(String appName, String id, boolean isReplication); void evict();} 进入其中一个实现类, 样子是这样 123456789@Overridepublic void register(final InstanceInfo info, final boolean isReplication) { int leaseDuration = Lease.DEFAULT_DURATION_IN_SECS; // 默认90s if (info.getLeaseInfo() != null &amp;&amp; info.getLeaseInfo().getDurationInSecs() &gt; 0) { leaseDuration = info.getLeaseInfo().getDurationInSecs(); } super.register(info, leaseDuration, isReplication); replicateToPeers(Action.Register, info.getAppName(), info.getId(), info, null, isReplication);} 这里做了两件事: 调用父类register方法, 传入服务注册所需要的信息(例如服务名, 实例ID等) 调用replicateToPeers方法, 拿到所有对等的Eureka Server节点的信息, 把实例的更改信息复制到各节点中, 也就是Peer Replicate过程 这里的leaseDuration对应配置项eureka.instance.lease-expiration-duration-in-seconds, 默认90s Registry的流程如下: 看到这里, 剩下的就是进入父类AbstractInstanceRegistry去查看细节了, 而Renew, Cancel的过程基本都是如此, 不说了 下面对其中一些重点概念进行分析 说明Registry在LeaseManager中的几个实现中, 都能看到这样一个数据结构 1ConcurrentHashMap&lt;String, Map&lt;String, Lease&lt;InstanceInfo&gt;&gt;&gt; registry; 如果能区分应用和实例的概念, 那就很好理解这个结构啦 翻译过来就是这样的: Map&lt;应用名, Map&lt;实例名, 实例的信息以及该实例续约相关的时间戳&gt;&gt; 举个例子, 向Eureka Server注册了两个服务server-a, server-b, 其中server-a有两个实例, 端口为8801, 8802 那么registry的结构为123456789{ \"SERVER-A\": { \"192.168.153.1:server-a:8802\": Lease&lt;InstanceInfo&gt;, \"192.168.153.1:server-a:8801\": Lease&lt;InstanceInfo&gt; }, \"SERVER-B\": { \"192.168.153.1:server-b:8901\": Lease&lt;InstanceInfo&gt; }} 说明RecentlyChangedQueue1ConcurrentLinkedQueue&lt;RecentlyChangedItem&gt; recentlyChangedQueue; 字面翻译是最近改变的队列, 在register, cancel, statusUpdate中, 都能看到对该队列添加了一个租约的变更记录 RecentlyChangedItem中存放了两个属性: leaseInfo: 租约 lastUpdateTime: 上次更新时间 在register中, 还有一个定时任务, 会对该队列定期进行移除12345Timer deltaRetentionTimer = new Timer(\"Eureka-DeltaRetentionTimer\", true)this.deltaRetentionTimer.schedule(getDeltaRetentionTask(), serverConfig.getDeltaRetentionTimerIntervalInMs(), serverConfig.getDeltaRetentionTimerIntervalInMs()); 来看下具体要执行的任务是什么123456789101112131415161718private TimerTask getDeltaRetentionTask() { return new TimerTask() { @Override public void run() { Iterator&lt;RecentlyChangedItem&gt; it = recentlyChangedQueue.iterator(); while (it.hasNext()) { if (it.next().getLastUpdateTime() &lt; System.currentTimeMillis() - serverConfig.getRetentionTimeInMSInDeltaQueue()) { it.remove(); } else { break; } } } };} 在EurekaServer的配置bean中可以找到上述代码中的两个配置的默认值1234// 租约变更的过期时间private long retentionTimeInMSInDeltaQueue = 3 * MINUTES;// 移除租约变更记录的定时器的执行间隔时间private long deltaRetentionTimerIntervalInMs = 30 * 1000; 关注if块中的移除条件就好, 也就是说会该任务每30秒执行一次, 移除最后更新时间距离现在超过180秒的租约记录 像不像一个距离为180s的滑动窗口? Evict与实例注册, 发送心跳不同的是实例的剔除是Eureka Server主动来做的, 定期剔除无效的服务 Server端定期执行剔除任务的默认周期为60s 配置项: eureka.server.eviction-interval-timer-in-ms 无效的服务是指未在指定时间内收到心跳(也就是未进行renew汇报的服务) 默认的时间上限为90s, 配置项: eureka.instance.lease-expiration-duration-in-seconds 代码在这里AbstractInstanceRegistry#evict(), 又是一个定时任务 这点从Eureka Server不停打出的INFO日志也能看出来1[a-EvictionTimer] c.n.e.registry.AbstractInstanceRegistry : Running the evict task with compensationTime 0ms 定义如下:1private Timer evictionTimer = new Timer(\"Eureka-EvictionTimer\", true); 缓存ResponseCacheImpl是Eureka Server缓存的实现, 通过读写缓存来降低读写竞争, 增大并发 在服务注册, 下线, 状态改变和剔除失效服务中, 都能看到这样一个方法invalidateCache(), 让什么缓存失效? 涉及到Eureka Server中两个很重要的缓存: ResponseCacheImpl#readOnlyCacheMap ResponseCacheImpl#readWriteCacheMap 从名字也能看出一个只读锁, 一个读写锁. 前者是ConcurrentMap, 后者是Guava Cache 它的写失效时间, Load方法都在ResponseCacheImpl的构造函数中定义了 而在invalidateCache()中, 最终操作的都是readWriteCacheMap, readOnlyCacheMap负责所有客户端读取实例信息的请求, 那么它的值从哪来, 我看到两种方式: 方式一: getIfNotExist: 在ResponseCacheImpl#getValue中可以看到首先会从readOnlyCacheMap读, 如果不存在就从readWriteCacheMap拿, 然后放到readOnlyCacheMap中 方式二: Timer 在ResponseCacheImpl#timer中定义了一个缓存填充的定时器, 在定时器的TaskResponseCacheImpl#getCacheUpdateTask中可以看到会将readWriteCacheMap的内容copy到readOnlyCacheMap 注意这两部分源码中都有一个if判断shouldUseReadOnlyResponseCache, 对应配置项eureka.server.use-read-only-response-cache, 默认为true, 字面翻译很清楚, 相当于开启只读缓存, 不开的话也就没有定时复制的任务了, 直接从读写缓存中拿了 Peer Replicate对应架构图中Eureka Server之间的Replicate isReplication 需要注意的是通过查看几个resource API, 都能发现一个放在Header的参数isReplication. 例如 123@POST@Consumes({\"application/json\", \"application/xml\"})public Response addInstance(InstanceInfo info, @HeaderParam(PeerEurekaNode.HEADER_REPLICATION) String isReplication) {} 这是因为Eureka Server依赖Eureka Client, 因为Eureka Server也要作为其它Eureka Server的Client, 所以通过isReplication来区分是来自于其它peer的复制请求还是来自与普通的client实例的请求, 如果Eureka Server收到属于复制请求的, 就不会再复制给其它Peer, 防止死循环 还需要注意的是在InstanceInfo有一个lastDirtyTimestamp字段, 类似于版本号的概念, 在Peer Replication的过程中会对其进行比较, 在判断数据冲突的情况下, 返回4xx, 让应用实例重新register或同步信息, 来避免复制的冲突问题 前面说过replicateToPeers()的入口点, 这个过程实际是将instance修改信息添加到一个批量任务中打包发送给其他peer 源码参考: com.netflix.eureka.cluster.PeerEurekaNode 里面可以看到创建Batch Task的过程, 由于是异步, 所以并不能保证在服务实例状态发生梗概时, 所有Peer上的信息都一致. Eureka Server端采用的是P2P的复制模式, 但是它不保证复制一定成功, 因此还通过与Eureka Client定期进行hearbeat, Server端内部的矫正机制来做应用实例信息的数据修复, 尽力提供一个最终一致性的服务实例视图 Eureka Client通过以上Eureka Server中的, 基本对架构图中的Register, Renew, Cancel过程有了基本的认识 再来简单说说Eureka Client provider对于Client端的Provider来说, 在启动和实例状态变化是, 需要通知Server端 源码参考(有序): com.netflix.discovery.InstanceInfoReplicator#onDemandUpdate com.netflix.discovery.DiscoveryClient#register com.netflix.discovery.shared.transport.decorator.EurekaHttpClientDecorator#register com.netflix.discovery.shared.transport.jersey.AbstractJerseyEurekaHttpClient#register 流程图如下: 同样, Renew, Cancel的过程也参照这个顺序 Consumer对于Client端的Consumer来说, 需要拉取服务实例的列表并缓存, 还需要定期更新 流程如下图 总结Netflix Eureka是典型的注册中心+嵌入式客户端架构, 并且各节点之间对等 通过上面的分析, 对Netflix Eureka架构图中的各个过程都有了一定的了解, 以及一些配置项的细节 能够发现的一点是, 无论Eureka Server之间Peer to Peer的Replicate过程, 还是Eureka Client向Eureka Server发起Get Service Registries的过程 实现中看到了大量的schedule, cache, 异步过程, 以及Eureka的自我保护机制, 这种模型简化了集群管理的复杂度, 易于实现高可扩展性. 但是并不能完全保证强一致, 而是最终一致性. 刚刚接触Eureka的新手肯定能够发现, 为什么在服务上线/下线后, 注册中心并没有立刻感知到, 而是间隔了若干时间. 这在很多业务场景下是能够满足的, 因为作为Eureka客户端来说, 通常都会配置Ribbon提供失败重试, 尤其对于服务发现这一场景, 即使返回了非最新的服务供消费者调用, 也比什么都不返回好 也就是Eureka! Why You Shouldn’t Use ZooKeeper for Service Discovery 可参考中文翻译: https://blog.csdn.net/jenny8080/article/details/52448403 这也是Eureka与其它几个服务发现组件(Zookeeper, Etcd, Consul)显著的区别. 在CAP理论中, Eureka保证AP, 其它几个保证CP 参考资料 Eureka at a glance Dive into Eureka","link":"/2019/04/16/源码分析—服务发现组件Netflix Eureka/"},{"title":"蓝桥杯算法练习 - 十六进制转八进制","text":"问题描述： 给定n个十六进制正整数，输出它们对应的八进制数。 输入格式： 输入的第一行为一个正整数n （1&lt;=n&lt;=10）。接下来n行，每行一个由0~9、大写字母A~F组成的字符串，表示要转换的十六进制正整数，每个十六进制数长度不超过100000。 输出格式： 输出n行，每行为输入对应的八进制正整数。 实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123import java.io.BufferedReader;import java.io.InputStreamReader;public class Main { public static void main(String[] args) throws Exception { BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); int len = Integer.parseInt(br.readLine()); String[] arr = new String[len]; for(int i=0; i&lt;arr.length; ++i){ arr[i] = br.readLine(); } for(int i=0; i&lt;arr.length; ++i){ transform1(arr[i]); } } //16--&gt;2 public static void transform1(String str){ char[] c = new char[str.length()]; c = str.toCharArray(); StringBuffer sb = new StringBuffer(); for(int i=0; i&lt;str.length(); ++i){ switch(c[i]){ case '0': sb.append(\"0000\"); break; case '1': sb.append(\"0001\"); break; case '2': sb.append(\"0010\"); break; case '3': sb.append(\"0011\"); break; case '4': sb.append(\"0100\"); break; case '5': sb.append(\"0101\"); break; case '6': sb.append(\"0110\"); break; case '7': sb.append(\"0111\"); break; case '8': sb.append(\"1000\"); break; case '9': sb.append(\"1001\"); break; case 'A': sb.append(\"1010\"); break; case 'B': sb.append(\"1011\"); break; case 'C': sb.append(\"1100\"); break; case 'D': sb.append(\"1101\"); break; case 'E': sb.append(\"1110\"); break; case 'F': sb.append(\"1111\"); break; } } transform2(sb); } //2--&gt;8 public static void transform2(StringBuffer sb){ int len = sb.length(); if(len%3 == 0){ if(\"000\".equals(sb.substring(0, 3))) sb.delete(0, 3); }else if(len%3 == 1){ if(\"0\".equals(sb.substring(0, 1))) sb.delete(0, 1); else sb.insert(0, \"00\"); }else if(len%3 == 2){ if(\"00\".equals(sb.substring(0, 2))) sb.delete(0, 2); else sb.insert(0, \"0\"); } StringBuffer sb2 = new StringBuffer(); int len2 = sb.length()/3; for(int i=0; i&lt;len2; ++i){ if(\"000\".equals(sb.substring(3*i, 3*i+3))) sb2.append(\"0\"); else if(\"001\".equals(sb.substring(3*i, 3*i+3))) sb2.append(\"1\"); else if(\"010\".equals(sb.substring(3*i, 3*i+3))) sb2.append(\"2\"); else if(\"011\".equals(sb.substring(3*i, 3*i+3))) sb2.append(\"3\"); else if(\"100\".equals(sb.substring(3*i, 3*i+3))) sb2.append(\"4\"); else if(\"101\".equals(sb.substring(3*i, 3*i+3))) sb2.append(\"5\"); else if(\"110\".equals(sb.substring(3*i, 3*i+3))) sb2.append(\"6\"); else if(\"111\".equals(sb.substring(3*i, 3*i+3))) sb2.append(\"7\"); } System.out.println(sb2); }} 输入的十六进制数不会有前导0，比如012A。输出的八进制数也不能有前导0。 样例输入： 239123ABC 样例输出： 714435274 提示： 先将十六进制数转换成某进制数，再由某进制数转换成八进制。","link":"/2014/11/15/蓝桥杯算法练习-十六进制转八进制/"},{"title":"蓝桥杯算法练习 - 报时助手","text":"问题描述： 给定当前的时间，请用英文的读法将它读出来。 时间用时h和分m表示，在英文的读法中，读一个时间的方法是： 如果m为0，则将时读出来，然后加上“o’clock”，如3:00读作“three o’clock”。如果m不为0，则将时读出来，然后将分读出来，如5:30读作“five thirty”。 时和分的读法使用的是英文数字的读法，其中0~20读作：0:zero, 1: one, 2:two, 3:three, 4:four, 5:five, 6:six, 7:seven, 8:eight, 9:nine, 10:ten, 11:eleven, 12:twelve, 13:thirteen, 14:fourteen, 15:fifteen, 16:sixteen, 17:seventeen, 18:eighteen, 19:nineteen, 20:twenty。30读作thirty，40读作forty，50读作fifty。 按上面的规则21:54读作“twenty one fifty four”，9:07读作“nine seven”，0:15读作“zero fifteen”。 输入格式： 输入包含两个非负整数h和m，表示时间的时和分。非零的数字前没有前导0。h小于24，m小于60。 输出格式： 输出时间时刻的英文。 样例输入： 0 15 样例输出： zero fifteen 实现：12345678910111213141516171819202122232425262728293031323334353637383940414243import java.util.Scanner;public class Timer { public static void main(String[] args){ System.out.println(\"Please input hour min:\"); Scanner scanner = new Scanner(System.in); int hour = 0; int min = 0; while(scanner.hasNext()){ hour = scanner.nextInt(); min = scanner.nextInt(); String[] time1 = { \"zero\",\"one\",\"two\",\"three\",\"four\",\"five\",\"six\",\"seven\", \"eight\",\"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\", \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\",\"twenty\" }; String[] time2 = {\"twenty\",\"thirty\",\"forty\",\"fifty\"}; if(hour &lt; 20){ System.out.print(time1[hour]); if(min == 0){ System.out.print(\" \" + \"o'clock\"); }else if(min &lt; 20){ System.out.print(\" \" + time1[min]); }else{ System.out.print(\" \" + time2[min/10 - 2] + \" \" + time1[min%10]); } }else{ System.out.print(time2[0] + \" \" + time1[hour-20]); if(min == 0){ System.out.print(\"o'clock\"); }else if(min &lt; 20){ System.out.print(\" \" + time1[min]); }else{ System.out.print(\" \" + time2[min/10 - 2] + \" \" + time1[min%10]); } } } }}","link":"/2014/11/12/蓝桥杯算法练习-报时助手/"},{"title":"蓝桥杯算法练习 - 大数字的读法","text":"问题描述： Tom教授正在给研究生讲授一门关于基因的课程，有一件事情让他颇为头疼：一条染色体上有成千上万个碱基对，它们从0开始编号，到几百万，几千万，甚至上亿。 比如说，在对学生讲解第1234567009号位置上的碱基时，光看着数字是很难准确的念出来的。 所以，他迫切地需要一个系统，然后当他输入12 3456 7009时，会给出相应的念法：十二亿三千四百五十六万七千零九 用汉语拼音表示为shi er yi san qian si bai wu shi liu wan qi qian ling jiu 这样他只需要照着念就可以了。 你的任务是帮他设计这样一个系统：给定一个阿拉伯数字串，你帮他按照中文读写的规范转为汉语拼音字串，相邻的两个音节用一个空格符格开。 注意必须严格按照规范，比如说“10010”读作“yi wan ling yi shi”而不是“yi wan ling shi”，“100000”读作“shi wan”而不是“yi shi wan”，“2000”读作“er qian”而不是“liang qian”。 输入格式： 有一个数字串，数值大小不超过2,000,000,000。 输出格式： 是一个由小写英文字母，逗号和空格组成的字符串，表示该数的英文读法。 样例输入： 1234567009 样例输出： shi er yi san qian si bai wu shi liu wan qi qian ling jiu 实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164import java.util.Scanner;public class ReadNumber { public static void main(String[] args){ Scanner scanner = new Scanner(System.in); String str = scanner.next(); int len = str.length(); int n = len; int m = n; int[] arr = new int[n]; //该数组保存输入的数字 StringBuffer sb = new StringBuffer(); //该缓冲区存放 数字的输出 /* 将输入的字符串数字 截取 放到整数类型arr数组中 方便操作 */ for(int i=0; i&lt;len; ++i){ if(i &lt; n-1) arr[i] = Integer.parseInt(str.substring(i, i+1)); else arr[i] = Integer.parseInt(str.substring(n-1)); } for(int i=0; i&lt;len; ++i){ if(m == 10){ //十亿 if(arr[i] == 1) sb.append(\"shi \"); else sb.append(\"er shi \"); } if(m == 9){ //亿 if(arr[i] &gt; 0){ sb.append(check(arr[i]) + \" yi \"); }else{ if(arr[i]==0){ sb.append(\" yi \"); } } } if(m == 8){//千万 if(arr[i] &gt; 0){ sb.append(check(arr[i]) + \" qian \"); }else{ if(arr[i]==0 &amp;&amp; arr[i+1]!=0){ sb.append(\"ling \"); } } } if(m == 7){//百万 if(arr[i] &gt; 0){ sb.append(check(arr[i]) + \" bai \"); }else{ if(arr[i]==0 &amp;&amp; arr[i+1]!=0){ sb.append(\"ling \"); } } } if(m == 6){//十万 if(arr[i] &gt; 0){ sb.append(check(arr[i]) + \" shi \"); }else{ if(arr[i]==0 &amp;&amp; arr[i+1]!=0){ sb.append(\"ling \"); } } } if(m == 5){//万 if(arr[i] &gt; 0){ sb.append(check(arr[i]) + \" wan \"); }else{ if(arr[i]==0 &amp;&amp; arr[i+1]!=0){ sb.append(\"ling \"); }else{ //\"注意:当千万,百万,十万位都为0且万位不为0时候 不会输出万\" 例如:十三亿,二十亿 if(arr[i-1]!=0 || arr[i-2]!=0 || arr[i-3]!=0) sb.append(\"wan \"); } } } if(m == 4){//千 if(arr[i] &gt; 0){ sb.append(check(arr[i]) + \" qian \"); }else{ if(arr[i]==0 &amp;&amp; arr[i+1]!=0){ sb.append(\"ling \"); } } } if(m == 3){//百 if(arr[i] &gt; 0){ sb.append(check(arr[i]) + \" bai \"); }else{ if(arr[i]==0 &amp;&amp; arr[i+1]!=0){ sb.append(\"ling \"); } } } if(m == 2){//十 if(arr[i] &gt; 0){ sb.append(check(arr[i]) + \" shi \"); }else{ if(arr[i]==0 &amp;&amp; arr[i+1]!=0){ sb.append(\"ling \"); } } } if(m == 1){//个 if(arr[i] &gt; 0){ sb.append(check(arr[i])); } } m--; }//end for System.out.println(sb); } public static String check(int i){ String s = \"\"; switch (i){ case 1: s = \"yi\"; break; case 2: s = \"er\"; break; case 3: s = \"san\"; break; case 4: s = \"si\"; break; case 5: s = \"wu\"; break; case 6: s = \"liu\"; break; case 7: s = \"qi\"; break; case 8: s = \"ba\"; break; case 9: s = \"jiu\"; break; } return s; }}","link":"/2014/11/13/蓝桥杯算法练习-大数字的读法/"},{"title":"设计模式—单例模式","text":"[TOC] 序言: 计算机老师让每个人准备一个PPT对其演讲, 内容不限 看到同学都在讲Java实现xxx系统, 感觉很厉害. 所以我想来看下设计模式. 从最简单的看起 介绍设计模式(Design pattern): 是一套被反复使用, 多数人知晓的, 经过分类遍目的,代码设计经验的总结. 目的: 使用设计模式是为了可重用代码, 让代码更容易被他人理解, 保证代码的可靠性 单例模式(Singleton): 保证整个应用中某个实例有且只有一个 有些对象我们只需要一个: 配置文件, 工具类, 线程池, 缓存, 日志对象等. 如果创造出多个实例就会导致训多问题: 比如占用过多资源,不一致的结果等 单例模式三要素 静态的私有的本类型的变量 构造方法私有化 提供一个公共的静态的入口点方法 饿汉式第一种: 饿汉式 1234567891011121314151617public class SingletonParrent01 { public static void main(String[] args){ Singleton s1 = Singleton.getInstance(); Singleton s2 = Singleton.getInstance(); System.out.println(s1 == s2); } class Singleton{ private Singleton(){} private static Singleton instance = new Singleton();; public static Singleton getInstance(){ return instance; } } 类加载阶段创建对象, 并且只创建一次 不好处: 无论这个类是否被使用,都会创建. 所以很多创建过程是无用的. 懒汉式第二种: 懒汉式, 或者说lazy loaded 1234567891011121314151617181920 public class SingletonParrent2 { public static void main(String[] args){ Singleton s1 = Singleton.getInstance(); Singleton s2 = Singleton.getInstance(); System.out.println(s1 == s2); System.out.println(s1); System.out.println(s2); } } class Singleton{ private Singleton(){} private static Singleton instance; public static Singleton getInstance(){ if(instance == null) instance = new Singleton(); return instance; } } 线程不安全上面懒汉式的单例模式代码很清楚，也很简单。 然而就像那句名言：“80%的错误都是由20%代码优化引起的”。 单线程下，这段代码没有什么问题，可是如果是多线程，麻烦就来了。 我们来分析一下：线程A希望使用Singleton，调用getInstance()方法。 因为是第一次调用，A就发现instance是null的，于是它开始创建实例，就在这个时候，CPU发生时间片切换，线程B开始执行，它要使用Singleton，调用getInstance()方法，同样检测到instance是null——注意，这是在A检测完之后切换的，也就是说A并没有来得及创建对象——因此B开始创建。B创建完成后，切换到A继续执行，因为它已经检测完了，所以A不会再检测一遍，它会直接创建对象。 这样，线程A和B各自拥有一个Singleton的对象——单例失败！ 所以懒汉式是线程不安全的! 尝试加锁所以,我们用加锁来试一下. 1234567891011121314public class Singleton { private static Singleton instance = null; public synchronized static Singleton getInstance() { if (instance == null) { instance = new Singleton(); } return instance; } private Singleton() { }} 这样的用法，在性能上会有所下降，因为每次调用getInstance()，都要对对象上锁，事实上，只有在第一次创建对象的时候需要加锁，之后就不需要了 但是不管是否为Null, 只要调用getInstance()方法, 都会进入synchronized同步代码块, 速度会很慢. 性能很低. 让我们来分析一下，究竟是整个方法都必须加锁，还是仅仅其中某一句加锁就足够了？ 我们为什么要加锁呢？分析一下出现lazy loaded的那种情形的原因。 原因就是检测null的操作和创建对象的操作分离了。如果这两个操作能够原子地进行，那么单例就已经保证了。 Double checked于是，我们开始修改代码： 1234567891011121314151617181920//double-checked locking public class Singleton { private static Singleton instance = null; public static Singleton getInstance() { if (instance == null) { synchronized (Singleton.class) { if (instance == null) { instance = new Singleton(); } } } return instance; } private Singleton() { } } 首先判断instance是不是为null，如果为null，加锁初始化；如果不为null，直接返回instance。 从源头检查: 下面我们开始说编译原理。所谓编译，就是把源代码“翻译”成目标代码——大多数是指机器代码——的过程。 针对Java，它的目标代码不是本地机器代码，而是虚拟机代码。 编译原理里面有一个很重要的内容是编译器优化。所谓编译器优化是指，在不改变原来语义的情况下，通过调整语句顺序，来让程序运行的更快。 这个过程成为reorder。 要知道，JVM只是一个标准，并不是实现。JVM中并没有规定有关编译器优化的内容，也就是说，JVM实现可以自由的进行编译器优化。 下面来想一下，创建一个变量需要哪些步骤呢？ 一个是申请一块内存，调用构造方法进行初始化操作 另一个是分配一个指针指向这块内存。 这两个操作谁在前谁在后呢？JVM规范并没有规定。 那么就存在这么一种情况，JVM是先开辟出一块内存，然后把指针指向这块内存，最后调用构造方法进行初始化。 下面我们来考虑这么一种情况：线程A开始创建SingletonClass的实例，此时线程B调用了getInstance()方法，首先判断instance是否为null。 按照我们上面所说的内存模型，A已经把instance指向了那块内存，只是还没有调用构造方法，因此B检测到instance不为null，于是直接把instance返回了——问题出现了，尽管instance不为null，但它并没有构造完成，就像一套房子已经给了你钥匙，但你并不能住进去，因为里面还没有收拾。 此时，如果B在A将instance构造完成之前就是用了这个实例，程序就会出现错误了！ 解决方案: volatile说了这么多，难道单例没有办法在Java中实现吗？其实不然！在JDK 5之后，Java使用了新的内存模型。 volatile关键字有了明确的语义——在JDK1.5之前，volatile是个关键字，但是并没有明确的规定其用途——被volatile修饰的写变量不能和之前的读写代码调整，读变量不能和之后的读写代码调整！ 因此，只要我们简单的把instance加上volatile关键字就可以了。 1234567891011121314151617181920212223/* * volatile关键字 */public class Singleton { private volatile static Singleton instance = null; public static Singleton getInstance() { if (instance == null) { synchronized (Singleton.class) { if (instance == null) { instance = new Singleton(); } } } return instance; } private Singleton() { }} 而，这只是JDK1.5之后的Java的解决方案，那之前版本呢？其实，还有另外的一种解决方案，并不会受到Java版本的影响： 123456789101112131415161718/* * 静态内部类方式 */public class Singleton { private static class SingletonInstance { private static final Singleton instance = new Singleton(); } public static Singleton getInstance() { return SingletonInstance.instance; } private Singleton() { }} 在这一版本的单例模式实现代码中，我们使用了Java的静态内部类。 这一技术是被JVM明确说明了的，因此不存在任何二义性。 在这段代码中，因为SingletonClass没有static的属性，因此并不会被初始化。直到调用getInstance()的时候，会首先加载SingletonClassInstance类，这个类有一个static的SingletonClass实例，因此需要调用SingletonClass的构造方法，然后getInstance()将把这个内部类的instance返回给使用者。由于这个instance是static的，因此并不会构造多次。 由于SingletonClassInstance是私有静态内部类，所以不会被其他类知道，同样，static语义也要求不会有多个实例存在。并且，JSL规范定义，类的构造必须是原子性的，非并发的，因此不需要加同步块。同样，由于这个构造是并发的，所以getInstance()也并不需要加同步。 至此，我们完整的了解了单例模式在Java语言中的时候，提出了两种解决方案。个人偏向于第二种，并且Effiective Java也推荐的这种方式。","link":"/2015/06/26/设计模式之单例模式/"},{"title":"通关Githug(二)","text":"[TOC] 继续上一篇通关Githug(一)) 关卡列表再附上30-55关目录 关卡名称 学习内容 Git 命令 第30关 blame 查询每一行代码被谁编辑过 git blame 第31关 branch 创建分支 git branch 第32关 checkout 切换分支 git checkout 第33关 checkout_tag 切换到标签 git checkout 第34关 checkout_tag_over_branch 切换到标签 git checkout 第35关 branch_at 在指定的提交处创建分支 git branch 第36关 delete_branch 删除分支 git branch -d 第37关 push_branch 推送分支到远程仓库 git push 第38关 merge 合并分支 git merge 第39关 fetch 从远程仓库抓取数据 git fetch 第40关 rebase 变基合并 git rebase 第41关 repack 重新打包 git repack 第42关 cherry-pick 合并分支上指定的提交 git cherry-pick 第43关 grep 搜索文本 git grep 第44关 rename_commit 修改历史提交的说明 git rebase -i 第45关 squash 把多次提交合并成一次提交 git rebase -i 第46关 merge_squash 合并分支时把多次提交合并成一次提交 git merge –squash 第47关 reorder 调整提交顺序 git rebase -i 第48关 bisect 用二分法定位 bug git bisect 第49关 stage_lines 添加文件的部分行到暂存区 git add –edit 第50关 file_old_branch 查看 Git 上的操作历史 git reflog 第51关 revert 取消已推送到远程仓库的提交 git revert 第52关 restore 恢复被删除的提交 git reset –hard 第53关 conflict 解决冲突 第54关 submodule 把第三方库当作子模块 git submodule 第55关 contribute 捐献 Level 30: blame Someone has put a password inside the file ‘config.rb’ find out who it was. 有人在config.rb文件里放了个password, 找出来谁搞的? 1234567891011121314151617181920212223[root@pipeline-cloud-test02 git_hug]# git blame config.rb^5e8863d (Gary Rennie 2012-03-08 23:05:24 +0000 1) class Config70d00535 (Bruce Banner 2012-03-08 23:07:41 +0000 2) attr_accessor :name, :password97bdd0cc (Spider Man 2012-03-08 23:08:15 +0000 3) def initialize(name, password = nil, options = {})^5e8863d (Gary Rennie 2012-03-08 23:05:24 +0000 4) @name = name97bdd0cc (Spider Man 2012-03-08 23:08:15 +0000 5) @password = password || &quot;i&lt;3evil&quot;00000000 (Not Committed Yet 2017-12-22 16:23:59 +0800 6)09409480 (Spider Man 2012-03-08 23:06:18 +0000 7) if options[:downcase]09409480 (Spider Man 2012-03-08 23:06:18 +0000 8) @name.downcase!09409480 (Spider Man 2012-03-08 23:06:18 +0000 9) end70d00535 (Bruce Banner 2012-03-08 23:07:41 +0000 10)ffd39c2d (Gary Rennie 2012-03-08 23:08:58 +0000 11) if options[:upcase]ffd39c2d (Gary Rennie 2012-03-08 23:08:58 +0000 12) @name.upcase!ffd39c2d (Gary Rennie 2012-03-08 23:08:58 +0000 13) endffd39c2d (Gary Rennie 2012-03-08 23:08:58 +0000 14)^5e8863d (Gary Rennie 2012-03-08 23:05:24 +0000 15) end^5e8863d (Gary Rennie 2012-03-08 23:05:24 +0000 16) end[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Who made the commit with the password? Spider ManCongratulations, you have solved the level! 这个命令会详细列出该文件每行代码提交的哈希值, 提交的人和日期 Level 31: branch You want to work on a piece of code that has the potential to break things, create the branch test_code. 你想实验一些代码, 但害怕有问题. 创建一个新分支test_code 123456789[root@pipeline-cloud-test02 git_hug]# git branch test_code[root@pipeline-cloud-test02 git_hug]# git branch* master test_code[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! 12git branch xxx # 创建一个分支git branch # 列出所有分支 Level 32: checkout Create and switch to a new branch called my_branch. You will need to create a branch like you did in the previous level. 创建一个新分支my_branch并切换到这个新分支 1234567891011[root@pipeline-cloud-test02 git_hug]# git branch my_branch[root@pipeline-cloud-test02 git_hug]# git checkout my_branchSwitched to branch &apos;my_branch&apos;[root@pipeline-cloud-test02 git_hug]# git branch master* my_branch[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! 补充: 1234git checkout xxxBranch # 检出一个分支git checkout xxxFile # 之前讲过如何撤销进入暂存区的修改, 这个也可以git checkout -b xxx # == git branch xxx + git checkout xxxgit checkout - # 用于在最近的两个分支之间切换 Level 33: checkout_tag You need to fix a bug in the version 1.2 of your app. Checkout the tag v1.2. 你需要切换到tag v1.2上去修复一些Bug 123456789101112131415161718192021222324252627[root@pipeline-cloud-test02 git_hug]# git branch* master[root@pipeline-cloud-test02 git_hug]# git tagv1.0v1.2v1.5[root@pipeline-cloud-test02 git_hug]# git checkout v1.2Note: checking out &apos;v1.2&apos;.You are in &apos;detached HEAD&apos; state. You can look around, make experimentalchanges and commit them, and you can discard any commits you make in thisstate without impacting any branches by performing another checkout.If you want to create a new branch to retain commits you create, you maydo so (now or later) by using -b with the checkout command again. Example: git checkout -b new_branch_nameHEAD is now at 060320d... Some more changes[root@pipeline-cloud-test02 git_hug]# git branch* (detached from v1.2) master[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! 注意: 从前面已经看到git checkout后面可以跟分之, 标签, 文件. 但是作用不通. Level 34: checkout_tag_over_branch You need to fix a bug in the version 1.2 of your app. Checkout the tag v1.2 (Note: There is also a branch named v1.2). 切换到tag v1.2上去修复一个bug(注意有一个分支也叫v1.2) 123456789101112131415161718[root@pipeline-cloud-test02 git_hug]# git checkout tags/v1.2Note: checking out &apos;tags/v1.2&apos;.You are in &apos;detached HEAD&apos; state. You can look around, make experimentalchanges and commit them, and you can discard any commits you make in thisstate without impacting any branches by performing another checkout.If you want to create a new branch to retain commits you create, you maydo so (now or later) by using -b with the checkout command again. Example: git checkout -b new_branch_nameHEAD is now at fc5edda... Some more changes[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! 这样区分就好了. Level 35: branch_at You forgot to branch at the previous commit and made a commit on top of it. Create branch test_branch at the commit before the last. 你忘记在上次提交之前先创建一个分支, 那么创建一个test_branch在上次提交之前 12345678910[root@pipeline-cloud-test02 git_hug]# git log --oneline03a7f2c Updating file1 again3bcda78 Updating file13873d24 Adding file1[root@pipeline-cloud-test02 git_hug]# git branch test_branch 3bcda78[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! Level 36: delete_branch You have created too many branches for your project. There is an old branch in your repo called ‘delete_me’, you should delete it. 删掉delete_me分支 12345678910[root@pipeline-cloud-test02 git_hug]# git branch delete_me* master[root@pipeline-cloud-test02 git_hug]# git branch -d delete_meDeleted branch delete_me (was b60afe2).[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! Level 37: push_branch You’ve made some changes to a local branch and want to share it, but aren’t yet ready to merge it with the ‘master’ branch. Push only ‘test_branch’ to the remote repository. 你本地分支做了些修改想分享它, 但是不准备合并到master上. 所以把他推送到test_branch分支远程仓库上. 12345678910111213141516171819[root@pipeline-cloud-test02 git_hug]# git remoteorigin[root@pipeline-cloud-test02 git_hug]# git branch* master other_branch test_branch[root@pipeline-cloud-test02 git_hug]# git push origin test_branchCounting objects: 7, done.Delta compression using up to 4 threads.Compressing objects: 100% (6/6), done.Writing objects: 100% (6/6), 572 bytes | 0 bytes/s, done.Total 6 (delta 3), reused 0 (delta 0)To /tmp/d20171222-302-aokpd0/.git * [new branch] test_branch -&gt; test_branch[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! Level 38: merge We have a file in the branch ‘feature’; Let’s merge it to the master branch. feature分支上有个文件, 把它合并到master上 12345678910111213141516171819202122[root@pipeline-cloud-test02 git_hug]# git log master --onelinee12277f added file1[root@pipeline-cloud-test02 git_hug]# git log feature --onelinecc8ea5a added file2e12277f added file1[root@pipeline-cloud-test02 git_hug]# git branch feature* master[root@pipeline-cloud-test02 git_hug]# git merge featureUpdating e12277f..cc8ea5aFast-forward file2 | 0 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 file2[root@pipeline-cloud-test02 git_hug]# git log master --onelinecc8ea5a added file2e12277f added file1[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! 分支测试完成后就可以合并到主干, 主干上也会记录对应的日志. Level 39: fetch Looks like a new branch was pushed into our remote repository. Get the changes without merging them with the local repository 有一个新的分支推送到远程仓库了, 得到它但是不要合并到你的本地仓库 123456789101112131415161718192021[root@pipeline-cloud-test02 git_hug]# git branch* master[root@pipeline-cloud-test02 git_hug]# git branch -r origin/master[root@pipeline-cloud-test02 git_hug]# git fetchremote: Counting objects: 3, done.remote: Compressing objects: 100% (2/2), done.remote: Total 2 (delta 0), reused 0 (delta 0)Unpacking objects: 100% (2/2), done.From /tmp/d20171222-797-1165a5c/ * [new branch] new_branch -&gt; origin/new_branch[root@pipeline-cloud-test02 git_hug]# git branch* master[root@pipeline-cloud-test02 git_hug]# git branch -r origin/master origin/new_branch[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! 为什么不用git pull呢, 因为它不仅会拉取远程代码, 还会合并到本地代码, 相当于git fetch+git merge 在 git fetch 之后用 git branch -r 查看时会发现新分支的名称 Level 40: rebase We are using a git rebase workflow and the feature branch is ready to go into master. Let’s rebase the feature branch onto our master branch. 我们使用了git rebase工作流, 并准备把feature分支合并到master. 现在rebase feature分支到master分之上吧. 1234567891011121314151617181920212223242526[root@pipeline-cloud-test02 git_hug]# git branch feature* master[root@pipeline-cloud-test02 git_hug]# git checkout featureSwitched to branch &apos;feature&apos;[root@pipeline-cloud-test02 git_hug]# git log --onelineed0fdcf add featurea78bcab init commit[root@pipeline-cloud-test02 git_hug]# git log master --oneline98205e9 add contenta78bcab init commit[root@pipeline-cloud-test02 git_hug]# git rebase masterFirst, rewinding head to replay your work on top of it...Applying: add feature[root@pipeline-cloud-test02 git_hug]# git log master --oneline98205e9 add contenta78bcab init commit[root@pipeline-cloud-test02 git_hug]# git log --oneline967920c add feature98205e9 add contenta78bcab init commit[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! 参考: Git Rebase原理以及黄金准则详解 Level 41: rebase_onto You have created your branch from wrong_branch and already made some commits, and you realise that you needed to create your branch from master. Rebase your commits onto master branch so that you don’t have wrong_branch commits. 你错误的从wrong_branch上创建了一个分支, 并且有了一些提交, 但是你意识到你应该是从master上创建分支, 现在rebase你的提交到master分支上去, 并且不保留wrong_branch的提交. 1234567891011121314151617181920212223242526272829303132333435[root@pipeline-cloud-test02 git_hug]# git branch master* readme-update wrong_branch[root@pipeline-cloud-test02 git_hug]# git log master --oneline3d8ad8d Create authors file[root@pipeline-cloud-test02 git_hug]# git log wrong_branch --onelinea2c5c42 Wrong changes3d8ad8d Create authors file[root@pipeline-cloud-test02 git_hug]# git log --oneline9c959f1 Add `Install` header in readme082624b Add `About` header in readme73dbf72 Add app name in readmea2c5c42 Wrong changes3d8ad8d Create authors file[root@pipeline-cloud-test02 git_hug]# git rebase --onto master wrong_branch readme-updateFirst, rewinding head to replay your work on top of it...Applying: Add app name in readmeApplying: Add `About` header in readmeApplying: Add `Install` header in readme[root@pipeline-cloud-test02 git_hug]# git log master --oneline3d8ad8d Create authors file[root@pipeline-cloud-test02 git_hug]# git log wrong_branch --onelinea2c5c42 Wrong changes3d8ad8d Create authors file[root@pipeline-cloud-test02 git_hug]# git log --onelineb113760 Add `Install` header in readme809fc39 Add `About` header in readme7adf9cd Add app name in readme3d8ad8d Create authors file[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! wrong_branch和master分支的日志都没有变, 注意看readme-update的日志变化. 补充: 摘一段官网的内容: First let’s assume your topic is based on branch next. For example, a feature developed in topic depends on some functionality which is found in next. 12345o---o---o---o---o master \\ o---o---o---o---o next \\ o---o---o topic We want to make topic forked from branch master; for example, because the functionality on which topic depends was merged into the more stable master branch. We want our tree to look like this: 12345o---o---o---o---o master | \\ | o&apos;--o&apos;--o&apos; topic \\ o---o---o---o---o next We can get this using the following command:git rebase –onto master next topic Level 42: repack Optimise how your repository is packaged ensuring that redundant packs are removed. 优化仓库的打包, 确保删除掉重复多余的包 123456789101112131415[root@pipeline-cloud-test02 git_hug]# ls .git/objects/4d 80 e6 info pack[root@pipeline-cloud-test02 git_hug]# git repack -dCounting objects: 3, done.Writing objects: 100% (3/3), done.Total 3 (delta 0), reused 0 (delta 0)[root@pipeline-cloud-test02 git_hug]# ls .git/objects/info pack[root@pipeline-cloud-test02 git_hug]# ls .git/objects/pack/pack-830ef7487354a6468143f53dd19ee16c25fc2837.idx pack-830ef7487354a6468143f53dd19ee16c25fc2837.pack[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! Level 43: cherry-pick Your new feature isn’t worth the time and you’re going to delete it. But it has one commit that fills in READMEfile, and you want this commit to be on the master as well. 你的新功能废了你要删除它! 但是其中有一次README的提交还是有用的, 你把这次提交合并到主干上. 123456789101112131415161718192021222324252627282930313233[root@pipeline-cloud-test02 git_hug]# git branch* master new-feature[root@pipeline-cloud-test02 git_hug]# git log new-feature --onelineea2a47c some small fixes4a1961b Fixed featureca32a6d Filled in README.md with proper input58a8c8e Added a stub for the featureea3dbcc Initial commit[root@pipeline-cloud-test02 git_hug]# git log --oneline6edea63 Added fancy branded output232d266 Renamed project.js -&gt; herdcore-math.jsb30c6a9 Added a hardcore math moduleea3dbcc Initial commit[root@pipeline-cloud-test02 git_hug]# git cherrycherry cherry-pick[root@pipeline-cloud-test02 git_hug]# git cherrycherry cherry-pick[root@pipeline-cloud-test02 git_hug]# git cherry-pick ca32a6d[master 25034ea] Filled in README.md with proper input Author: Andrey &lt;aslushnikov@gmail.com&gt; 1 file changed, 1 insertion(+), 2 deletions(-)[root@pipeline-cloud-test02 git_hug]# git log --oneline25034ea Filled in README.md with proper input6edea63 Added fancy branded output232d266 Renamed project.js -&gt; herdcore-math.jsb30c6a9 Added a hardcore math moduleea3dbcc Initial commit[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! git cherry-pick hash-code: 摘樱桃. 可以选取一个分支上某次提交的哈希值来合并到主线上. Level 44: grep Your project’s deadline approaches, you should evaluate how many TODOs are left in your code. 找下项目里还有多少待办事项(TODOs) 1234567891011[root@pipeline-cloud-test02 git_hug]# git grep TODOapp.rb:# TODO Make site url variable.app.rb:# TODO Make API version variable.app.rb:# TODO Redirecting queries could be useful.config.rb: # TODO Move password to a configuration file.[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************How many items are there in your todolist? 4Congratulations, you have solved the level! 12git grep xxx # 全项目查找git grep xxx file-name # 指定文件查找 Level 45: rename_commit Correct the typo in the message of your first (non-root) commit. 修正第一次提交的拼写错误 123456789101112131415161718[root@pipeline-cloud-test02 git_hug]# git log --oneline5da15a1 Second commitd1c06bd First coommit2b08caa Initial commit[root@pipeline-cloud-test02 git_hug]# git rebase -i 2b08caa[detached HEAD ac4092f] First commit 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 file1Successfully rebased and updated refs/heads/master.[root@pipeline-cloud-test02 git_hug]# git log --oneline208de39 Second commitac4092f First commit2b08caa Initial commit[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! 补充: git rebase -i hash-code: 后面加了-i参数后就不再是分支合并的意思了. -i表示交互, 后面跟上提交的哈希码, 可以修改历史的提交. 执行该命令后, 会启动文本编辑器, 并在其中显示每次提交的记录信息(从前到后显示). 每行记录前面会有一个标识, 表示进行什么修改操作: Commands: p, pick: 表示执行此次提交 r, reword: 表示执行此次提交，但要修改备注内容 e, edit: 表示可以修改此次提交，比如再追加文件或修改文件 s, squash : 表示把此次提交的内容合并到上次提交中，备注内容也合并到上次提交中 f, fixup : 和 “squash” 类似，但会丢弃掉此次备注内容 x, exec : 执行命令行下的命令 drop : 删除此次提交 本关用的就是reword. Level 46: squash You have committed several times but would like all those changes to be one commit. 把多次提交合并成一次 12345678910111213141516171819[root@pipeline-cloud-test02 git_hug]# git log --onelinea68644a Updating README (squash this commit into Adding README)712a031 Updating README (squash this commit into Adding README)da9df2a Updating README (squash this commit into Adding README)e799cb3 Adding README04a03cf Initial Commit[root@pipeline-cloud-test02 git_hug]# git rebase -i 04a03cf[detached HEAD 63c098c] Adding README 1 file changed, 3 insertions(+) create mode 100644 READMESuccessfully rebased and updated refs/heads/master.[root@pipeline-cloud-test02 git_hug]# git log --oneline63c098c Adding README04a03cf Initial Commit[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! 本关用的就是squash. Level 47: merge_squash Merge all commits from the long-feature-branch as a single commit. 把long-feature-branch上的多次提交合并到主干上的一次提交 123456789101112131415161718192021222324252627[root@pipeline-cloud-test02 git_hug]# git branch long-feature-branch* master[root@pipeline-cloud-test02 git_hug]# git log master --onelineb79bd86 Second commit24289dd First commit[root@pipeline-cloud-test02 git_hug]# git log long-feature-branch --oneline0a1ccc1 Time36fb820 Takescef0b75 Developing new features24289dd First commit[root@pipeline-cloud-test02 git_hug]# git merge long-feature-branch --squashSquash commit -- not updating HEADAutomatic merge went well; stopped before committing as requested[root@pipeline-cloud-test02 git_hug]# git commit -m &quot;merge from long-fxxxx-branch&quot;[master 18b8931] merge from long-fxxxx-branch 1 file changed, 3 insertions(+) create mode 100644 file3[root@pipeline-cloud-test02 git_hug]# git log master --oneline18b8931 merge from long-fxxxx-branchb79bd86 Second commit24289dd First commit[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! 通常分支上会有很多个提交, 在合并到主干上时也会给主干带过去多次提交记录, 这样不便于追踪. 所以使用git merge xxbranch_name --squash命令把分支的多次提交搞成一次. 注意使用完合并之后, 需要在commit一下, 并写上注释信息. (不加--squash时会自动commit). Level 48: reorder You have committed several times but in the wrong order. Please reorder your commits. 调整提交顺序 1234567891011121314151617[root@pipeline-cloud-test02 git_hug]# git log --onelineef9e60e Second commit54c5e74 Third commitb424f5a First commitb435e3c Initial Setup[root@pipeline-cloud-test02 git_hug]# git rebase -i b435e3cSuccessfully rebased and updated refs/heads/master.[root@pipeline-cloud-test02 git_hug]# git log --oneline94b9586 Third commit866d552 Second commitb424f5a First commitb435e3c Initial Setup[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! Level 49: bisect A bug was introduced somewhere along the way. You know that running “ruby prog.rb 5” should output 15. You can also run “make test”. What are the first 7 chars of the hash of the commit that introduced the bug. 在开发过程中引入了一个 bug。已知运行 “ruby prog.rb 5” 应该输入 15，你也可以运行 “make test” 进行测试。你需要确定引入 bug 的那次提交的哈希值的前7位。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[root@pipeline-cloud-test02 git_hug]# git log --oneline12628f4 Another Commit9795761 Another Commit028763b Another Commit888386c Another Commitbb736dd Another Commit18ed2ac Another Commit5db7a7c Another Commit7c03a99 Another Commit9f54462 Another Commit5d1eb75 Another Commitfdbfc0d Another Commita530e7e Another Commitccddb96 Another Commit2e1735d Another Commitffb097e Another Commite060c0d Another Commit49774ea Another Commit8c992af Another Commit80a9b3d Another Commitf608824 First commit[root@pipeline-cloud-test02 git_hug]# git bisect start[root@pipeline-cloud-test02 git_hug]# git bisect good f608824[root@pipeline-cloud-test02 git_hug]# git bisect bad 12628f4Bisecting: 9 revisions left to test after this (roughly 3 steps)[fdbfc0d403e5ac0b2659cbfa2cbb061fcca0dc2a] Another Commit[root@pipeline-cloud-test02 git_hug]# make testruby prog.rb 5 | ruby test.rb[root@pipeline-cloud-test02 git_hug]# git bisect goodBisecting: 4 revisions left to test after this (roughly 2 steps)[18ed2ac1522a014412d4303ce7c8db39becab076] Another Commit[root@pipeline-cloud-test02 git_hug]# make testruby prog.rb 5 | ruby test.rbmake: *** [test] Error 1[root@pipeline-cloud-test02 git_hug]# git bisect badBisecting: 2 revisions left to test after this (roughly 1 step)[9f54462abbb991b167532929b34118113aa6c52e] Another Commit[root@pipeline-cloud-test02 git_hug]# make testruby prog.rb 5 | ruby test.rb[root@pipeline-cloud-test02 git_hug]# git bisect goodBisecting: 0 revisions left to test after this (roughly 1 step)[5db7a7cb90e745e2c9dbdd84810ccc7d91d92e72] Another Commit[root@pipeline-cloud-test02 git_hug]# make testruby prog.rb 5 | ruby test.rb[root@pipeline-cloud-test02 git_hug]# git bisect good18ed2ac1522a014412d4303ce7c8db39becab076 is the first bad commitcommit 18ed2ac1522a014412d4303ce7c8db39becab076Author: Robert Bittle &lt;guywithnose@gmail.com&gt;Date: Mon Apr 23 06:52:10 2012 -0400 Another Commit:100644 100644 917e70054c8f4a4a79a8e805c0e1601b455ad236 7562257b8e6446686ffc43a2386c50c254365020 M prog.rb[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************What are the first 7 characters of the hash of the commit that introduced the bug? 18ed2ac1522a014412d4303ce7c8db39becab076Congratulations, you have solved the level! 对我没用.. 略 Level 50: stage_lines You’ve made changes within a single file that belong to two different features, but neither of the changes are yet staged. Stage only the changes belonging to the first feature. 一个文件里有两个功能, 现在只想把其中一个功能提交到暂存区. 12345678910111213141516171819202122232425262728[root@pipeline-cloud-test02 git_hug]# git status -s M feature.rb[root@pipeline-cloud-test02 git_hug]# git diff feature.rbdiff --git a/feature.rb b/feature.rbindex 1a271e9..4a80dda 100644--- a/feature.rb+++ b/feature.rb@@ -1 +1,3 @@ this is the class of my feature+This change belongs to the first feature+This change belongs to the second feature[root@pipeline-cloud-test02 git_hug]# git add feature.rb --edit[root@pipeline-cloud-test02 git_hug]# git status -sMM feature.rb[root@pipeline-cloud-test02 git_hug]# git diff feature.rbdiff --git a/feature.rb b/feature.rbindex 3bccd0e..4a80dda 100644--- a/feature.rb+++ b/feature.rb@@ -1,2 +1,3 @@ this is the class of my feature This change belongs to the first feature+This change belongs to the second feature[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! git add file_name --edit会在添加文件时打开编辑器, 并显示文件diff结果. 可以根据编辑的结果进行add操作. Level 51: find_old_branch You have been working on a branch but got distracted by a major issue and forgot the name of it. Switch back to that branch. 你忘了刚才时在哪个分支上工作了, 找到它. 切换过去 123456789101112131415[root@pipeline-cloud-test02 git_hug]# git reflog894a16d HEAD@{0}: commit: commit another todo6876e5b HEAD@{1}: checkout: moving from solve_world_hunger to kill_the_batman324336a HEAD@{2}: commit: commit todo6876e5b HEAD@{3}: checkout: moving from blowup_sun_for_ransom to solve_world_hunger6876e5b HEAD@{4}: checkout: moving from kill_the_batman to blowup_sun_for_ransom6876e5b HEAD@{5}: checkout: moving from cure_common_cold to kill_the_batman6876e5b HEAD@{6}: commit (initial): initial commit[root@pipeline-cloud-test02 git_hug]# git checkout solve_world_hungerSwitched to branch &apos;solve_world_hunger&apos;[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! git reflog不光记录了Commit信息, 还记录了分支切换的信息. Level 52: revert You have committed several times but want to undo the middle commit. All commits have been pushed, so you can’t change existing history. 你提交了很多次, 而且已经推送到服务器上了,所以你不能改变已经存在的历史. 你想取消中间的某次提交. 123456789101112131415161718[root@pipeline-cloud-test02 git_hug]# git log --oneline498b9be Second commit5a42119 Bad commit4d925f6 First commit[root@pipeline-cloud-test02 git_hug]# git revert 5a42119 --no-edit[master c608236] Revert &quot;Bad commit&quot; 1 file changed, 0 insertions(+), 0 deletions(-) delete mode 100644 file3[root@pipeline-cloud-test02 git_hug]# git log --onelinec608236 Revert &quot;Bad commit&quot;498b9be Second commit5a42119 Bad commit4d925f6 First commit[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! 既然不能改变历史, 那就用git revert增加一次取消的逆处理. 12git revert hash-code # 调用文本编辑器让你编写注释git revert hash-code --no-edit # 自动生成注释 Level 53: restore You decided to delete your latest commit by running git reset --hard HEAD^. (Not a smart thing to do.) You then change your mind, and want that commit back. Restore the deleted commit. 你用git reset --hard HEAD^删掉了最后一次提交(傻), 然后你改变主意了, 想回退恢复那条被删除的提交. 12345678910111213141516171819[root@pipeline-cloud-test02 git_hug]# git log --onelined56c44f First commit9fd667b Initial commit[root@pipeline-cloud-test02 git_hug]# git reflogd56c44f HEAD@{0}: reset: moving to HEAD^dfabe57 HEAD@{1}: commit: Restore this commitd56c44f HEAD@{2}: commit: First commit9fd667b HEAD@{3}: commit (initial): Initial commit[root@pipeline-cloud-test02 git_hug]# git reset --hard dfabe57HEAD is now at dfabe57 Restore this commit[root@pipeline-cloud-test02 git_hug]# git log --onelinedfabe57 Restore this commitd56c44f First commit9fd667b Initial commit[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! Level 54: conflict You need to merge mybranch into the current branch (master). But there may be some incorrect changes in mybranch which may cause conflicts. Solve any merge-conflicts you come across and finish the merge. 你要合并mybranch分支到当前分支master, 可能有冲突, 解决! 123456789101112131415161718192021222324[root@pipeline-cloud-test02 git_hug]# git merge mybranchAuto-merging poem.txtCONFLICT (content): Merge conflict in poem.txtAutomatic merge failed; fix conflicts and then commit the result.[root@pipeline-cloud-test02 git_hug]# vim poem.txt[root@pipeline-cloud-test02 git_hug]# git status# On branch master# You have unmerged paths.# (fix conflicts and run &quot;git commit&quot;)## Unmerged paths:# (use &quot;git add &lt;file&gt;...&quot; to mark resolution)## both modified: poem.txt#no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)[root@pipeline-cloud-test02 git_hug]# git add poem.txt[root@pipeline-cloud-test02 git_hug]# git commit -m &quot;merge mybranch to master&quot;[master 7c3ba83] merge mybranch to master[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! Level 55: submodule You want to include the files from the following repo: https://github.com/jackmaney/githug-include-meinto a the folder ./githug-include-me. Do this without cloning the repo or copying the files from the repo into this repo. 你想把 https://github.com/jackmaney/githug-include-me 这个仓库的代码引入到自己项目的 ./githug-include-me 目录，这个方法不需要克隆第三方仓库，也不需要把第三方仓库的文件复制到你的项目中。 12345678910[root@pipeline-cloud-test02 git_hug]# git submodule add https://github.com/jackmaney/githug-include-meCloning into &apos;githug-include-me&apos;...remote: Counting objects: 9, done.remote: Total 9 (delta 0), reused 0 (delta 0), pack-reused 9Unpacking objects: 100% (9/9), done.[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! Level 56: contribute This is the final level, the goal is to contribute to this repository by making a pull request on GitHub. Please note that this level is designed to encourage you to add a valid contribution to Githug, not testing your ability to create a pull request. Contributions that are likely to be accepted are levels, bug fixes and improved documentation. 最后一关, 是让你去GitHub贡献一个request. 其实到这已经全部通关!","link":"/2017/12/25/通关Githug(二)/"},{"title":"问题排查:在eclipse下配置dbcp","text":"配置使用jndi的方式配置tomcat数据源. 我的步骤如下: 将Oracle的驱动拷贝到tomcat中的lib目录下 配置web.xml文件(这一步尝试了一下不写也暂未出现异常. 原因未知…) 1234567&lt;resource-ref&gt; &lt;description&gt;dbcp_drp&lt;/description&gt; &lt;!--数据源名称, 要和context.xml中的数据源名称一致--&gt; &lt;res-ref-name&gt;jdbc/drp&lt;/res-ref-name&gt; &lt;res-type&gt;javax.sql.DataSource&lt;/res-type&gt; &lt;res-auth&gt;Container&lt;/res-auth&gt; &lt;/resource-ref&gt; 配置context.xml文件 修改了该文件的内容, tomcat就会自动装载该应用.为了满足每个应用的单独配置, 所以我是在我的一个项目中的META-INFO中配置的 配置内容如下: 12345678910111213&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;Context&gt; &lt;Resource name=\"jdbc/drp\" type=\"javax.sql.DataSource\" password=\"drp\" driverClassName=\"oracle.jdbc.driver.OracleDriver\" maxIdle=\"2\" maxWait=\"5000\" username=\"drp\" url=\"jdbc:oracle:thin:@localhost:1521:orcl\" maxActive=\"4\"/&gt; &lt;/Context&gt; 可以使用Tomcat管理页面配置, 但是个人感觉太麻烦… 在jsp中的Java代码测试 12345Connection conn = null; Context ctx = new InitialContext(); //通过JNDI查找DataSource DataSource ds = (DataSource)ctx.lookup(\"java:comp/env/jdbc/drp\"); conn = ds.getConnection(); 按照科学, 这样就得到连接啦… 但是在eclipse下启动Tomcat时. 却出现了几个问题.. 花了几小时去找原因. 遇到问题问题一123[SetPropertiesRule]{Server/Service/Engine/Host/Context} Setting property ‘source’ to ‘org.eclipse.jst.jee.server:aa’ did not find a matching property………………………… 这导致tomcat根本没法启动 大致阅读, 发现有可能是server.xml配置中的source属性的问题 于是我进入tomcat_home/conf/server.xml查看, 确实自动加了一段代码标签 然后我尝试删除这段代码, 重新启动tomcat. 发现能启动了, 但却出现另一个严重问题: 问题二123Cannot create JDBC driver of class ‘’ for connect URL ‘null’……………… 为什么JDBC的DriverClass和url都为空, 明明在meta-info中都配置好了啊.为什么没取到? 还是jndi数据源读取出现错误? 于是我怀疑他读到的是tomcat_home/conf/context.xml这个, 而不是我应用中的context.xml 于是将tomcat_home/conf/下的context.xml删除.发现还是不行 显然, 这种解决方式很愚蠢. 于是, 我去寻找问题的原因. 最后找到原因如下: 在eclipse下, 启动tomcat时, tomcat的配置文件conf/server.xml中会自动生成一个关于该web工程的配置项信息.类似: 1docBase=\"\" path=\"\" reloadable=\"\" source=\"\"/&gt; 而默认情况下, server.xml的Context元素不支持名称为source的属性, 所以产生了警告. 解决办法点开eclipse下配置好的tomcat, 在打开的页面找到server option选项 选中Publish modual contexts to separat XML files选项即可, 就是将context部分放到一个单独的文件中. 此时, 再修改meta-info中的context.xml中的配置 tomcat_home/conf下的server.xml中就不会再出现和什么source属性了. 警告也消失了!","link":"/2015/10/06/问题排查在eclipse下配置dbcp/"},{"title":"通关Githug(一)","text":"[TOC] 一. 序言 第一眼看到githug, 以为是把github拼错. 原来… 是个游戏! 关于Git的学习, 大四写过一篇: git小玩 . 虽然很多IDE都有集成git插件, 在了解git命令和使用场景后再去使用IDE才会更加明白 甚至我认为很多操作使用git bash不比IDE效率慢 二. 介绍当然, githug不是什么RPG, FPS之类的游戏. 而是一个围绕git知识, 每一关都会帮你自动搭建好实验场景, 告诉你任务背景, 来完成这些关卡. 其实这个跟github提供的在线练习很像, 但是我觉得githug更加好玩! git命令在线练习 git分支在线练习 github地址: https://github.com/Gazler/githug 花了一天时间去通关. 当中也有过不去的然后去翻译和查通关攻略了 三. 安装使用1. 安装Githug可以使用在Linux, Windows, OS X下. 我以 CentOS 7 为例: 在安装Githug之前首先确认有Ruby环境. 查看是否安装: ruby --version 这里我选择自动安装的方式: sudo yum install ruby 接着通过gem安装githug: gem install githug 安装完成. 2. 使用Githug只有四个游戏命令, 很简单: githug play: 开始玩, 会验证是否完成当前关卡, 完成则进入下一关(可缩写成githug), 没完成则会提示你并提示任务要求 githug hint: 给你一点当前关卡的提示 githug reset: 重置当前关卡 githug levels: 列出所有关卡 在第一次输入githug开始游戏时, 会提示No githug directory found, do you wish to create one? 意思要初始化一个游戏目录. 输入y即可创建. 然后会出现一个git_hug目录, 以后所有关卡都会在该目录下生成文件和git对象. 每完成一关都会重新初始化游戏场景. 3. 关卡列表为了方便查看, 所以列出所有关卡. 这里摘抄一个wiki. 关卡名称 学习内容 Git 命令 第1关 init 初始化仓库 git init 第2关 config 设置用户名和电子邮箱地址 git config 第3关 add 把文件添加到暂存区 git add 第4关 commit 提交 git commit 第5关 clone 克隆远程仓库 git clone 第6关 clone_to_folder 克隆远程仓库，并指定本地目录名 git clone 第7关 ignore 配置不被 Git 管理的文件 vim .gitignore 第8关 include 配置不被 Git 管理的文件 vim .gitignore 第9关 status 查看仓库状态 git status 第10关 number_of_files_committed 查看仓库状态 git status 第11关 rm 删除文件 git rm 第12关 rm_cached 从暂存区中移除文件，系 git add 的逆操作 git rm –cached 第13关 stash 保存而不提交 git stash 第14关 rename 文件改名 git mv 第15关 restructure 整理目录结构 第16关 log 查询日志 git log 第17关 tag 打标签 git tag 第18关 push_tags 把标签推送到远程仓库 git push –tags 第19关 commit_amend 修改最后一次提交 git commit –amend 第20关 commit_in_future 指定提交的日期 git commit –date 第21关 reset 从暂存区中移除文件，系 git add 的逆操作 git reset 第22关 reset_soft 撤销提交，系 git commit 的逆操作 git reset –soft 第23关 checkout_file 撤销对一个文件的修改 git checkout 第24关 remote 查询远程仓库 git remote 第25关 remote_url 查询远程仓库的 URL git remote -v 第26关 pull 从远程仓库拉取更新 git pull 第27关 remote_add 添加远程仓库 git remote 第28关 push 把提交推送到远程仓库 git push 第29关 diff 查看文件被修改的细节 git diff 第30关 blame 查询每一行代码被谁编辑过 git blame 第31关 branch 创建分支 git branch 第32关 checkout 切换分支 git checkout 第33关 checkout_tag 切换到标签 git checkout 第34关 checkout_tag_over_branch 切换到标签 git checkout 第35关 branch_at 在指定的提交处创建分支 git branch 第36关 delete_branch 删除分支 git branch -d 第37关 push_branch 推送分支到远程仓库 git push 第38关 merge 合并分支 git merge 第39关 fetch 从远程仓库抓取数据 git fetch 第40关 rebase 变基合并 git rebase 第41关 repack 重新打包 git repack 第42关 cherry-pick 合并分支上指定的提交 git cherry-pick 第43关 grep 搜索文本 git grep 第44关 rename_commit 修改历史提交的说明 git rebase -i 第45关 squash 把多次提交合并成一次提交 git rebase -i 第46关 merge_squash 合并分支时把多次提交合并成一次提交 git merge –squash 第47关 reorder 调整提交顺序 git rebase -i 第48关 bisect 用二分法定位 bug git bisect 第49关 stage_lines 添加文件的部分行到暂存区 git add –edit 第50关 file_old_branch 查看 Git 上的操作历史 git reflog 第51关 revert 取消已推送到远程仓库的提交 git revert 第52关 restore 恢复被删除的提交 git reset –hard 第53关 conflict 解决冲突 第54关 submodule 把第三方库当作子模块 git submodule 第55关 contribute 捐献 四. 开始游戏Level 1: init A new directory, git_hug, has been created; initialize an empty repository in it. 在git_hug目录下初始化一个仓库 123456[root@pipeline-cloud-test02 git_hug]# git initInitialized empty Git repository in /home/githug/git_hug/.git/[root@pipeline-cloud-test02 git_hug]# ls -a. .. .git .gitignore .profile.yml[root@pipeline-cloud-test02 git_hug]# githugCongratulations, you have solved the level! 空目录和有文件的目录都可以初始化, 可以看到隐藏文件.git, 如同.svn一样. 该目录会被git所管理. Level 2: config Set up your git name and email, this is important so that your commits can be identified. 配置用户名和邮箱是为了提交代码时的团队标识 1234567891011[root@pipeline-cloud-test02 git_hug]# git config --add user.name thank[root@pipeline-cloud-test02 git_hug]# git config --add user.email coderthank@163.com[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************What is your name? thankWhat is your email? coderthank@163.comYour config has the following name: thankYour config has the following email: coderthank@163.comCongratulations, you have solved the level! 补充: 123456git config --add [--global/--local] user.name xxx # 增加name配置信息git config --get [--global/--local] user.name xxx # 查看name配置信息# --global/--local参数用来表示是全局配置还是本地配置# email的话换成user.emailgit config –list # 查看git全局配置 当然, 有配置就会有配置的记录文件. 你可以到~/.gitconfig去编辑配置信息 Level 3: add There is a file in your folder called ‘README’, you should add it to your staging area. 添加README文件到暂存区 123456[root@pipeline-cloud-test02 git_hug]# git add README[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! 补充: 首先补充一个图: 关于这三个工作区域: History 也可以叫repository(git仓库): 最终确定的文件能保存到仓库, 成为一个新的版本, 并且对他人可见 Staging area (暂存区,Cache, Index): 暂存已经修改的文件 Working directory(工作区): 也就是你实际看见的工作目录 Level 4: commit The ‘README’ file has been added to your staging area, now commit it. 把暂存区的README文件提交 123456789[root@pipeline-cloud-test02 git_hug]# git commit -m &quot;add README file&quot;[master (root-commit) a3317a2] add README file 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 README[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! Level 5: clone Clone the repository at https://github.com/Gazler/cloneme. 12345678910[root@pipeline-cloud-test02 git_hug]# git clone https://github.com/Gazler/clonemeCloning into &apos;cloneme&apos;...remote: Counting objects: 7, done.remote: Total 7 (delta 0), reused 0 (delta 0), pack-reused 7Unpacking objects: 100% (7/7), done.[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! 回一下第1关, 可以总结下创建git仓库大致有两种: git init初始化和git clone克隆远程git项目 Level 6: clone to folder Clone the repository at https://github.com/Gazler/cloneme to ‘my_cloned_repo’. 还是克隆远程git项目, 只不过换个名字 12345678910[root@pipeline-cloud-test02 git_hug]# git clone https://github.com/Gazler/cloneme my_cloned_repoCloning into &apos;my_cloned_repo&apos;...remote: Counting objects: 7, done.remote: Total 7 (delta 0), reused 0 (delta 0), pack-reused 7Unpacking objects: 100% (7/7), done.[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! Level 7: ignore The text editor ‘vim’ creates files ending in ‘.swp’ (swap files) for all files that are currently open. We don’t want them creeping into the repository. Make this repository ignore ‘.swp’ files. 有些零食文件什么的不需要git来管理. 忽略他们 123456789101112[root@pipeline-cloud-test02 git_hug]# ls -a. .. .git .gitignore .profile.yml README.swp[root@pipeline-cloud-test02 git_hug]# vim .gitignore[root@pipeline-cloud-test02 git_hug]# more .gitignore.profile.yml.gitignore*.swp[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! .gitignore 在仓库的根目录下, 用于配置可忽略文件的规则 Level 8: include Notice a few files with the ‘.a’ extension. We want git to ignore all but the ‘lib.a’ file. 在忽略的里面排除掉不忽略的 12345678910111213[root@pipeline-cloud-test02 git_hug]# ls -a. .. first.a .git .gitignore lib.a .profile.yml second.a[root@pipeline-cloud-test02 git_hug]# vim .gitignore[root@pipeline-cloud-test02 git_hug]# more .gitignore.profile.yml.gitignore*.a!lib.a # 加&quot;!&quot;就是不忽略[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! Level 9: status There are some files in this repository, one of the files is untracked, which file is it? 也就是去找没有登记的文件, Untrakced File 12345678910111213[root@pipeline-cloud-test02 git_hug]# git status -sA GuardfileA READMEA config.rbA deploy.rbA setup.rb?? database.yml[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************What is the full file name of the untracked file? database.ymlCongratulations, you have solved the level! 通过git status查看到的文件有这三种状态: untracked: 未被登记的 modified: 修改过的 staged: 暂存区的 Level 10: number_of_files_committed There are some files in this repository, how many of the files will be committed? 找出有几个要被提交的文件, 什么状态的文件能被提交? 只有暂存区的了 123456789101112[root@pipeline-cloud-test02 git_hug]# git status -sA rubyfile1.rbM rubyfile4.rb M rubyfile5.rb?? rubyfile6.rb?? rubyfile7.rb[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************How many changes are going to be committed? 2Congratulations, you have solved the level! 补充: 如果你用git status, 你会明显看到有两个文件处于Changes to be commited. 如果加-s查看简要列表, 你需要明白前面的标志位是什么意思. 首先第一个标志位: 是Staging area中的变化, 第二个标志位: 是Working directory中发生的更改. 标志位上的字母A, M, D, ?就不用说了 Level 11: rm A file has been removed from the working tree, however the file was not removed from the repository. Find out what this file was and remove it. 有个文件只在工作空间被删了, 然而没有从仓库里删掉. 找到它并删掉rm_cached 123456789[root@pipeline-cloud-test02 git_hug]# git status -s D deleteme.rb[root@pipeline-cloud-test02 git_hug]# git rm deleteme.rbrm &apos;deleteme.rb&apos;[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! 在删除git文件时, 不要用rm的系统级别删除, 这样git会失去对这个文件的追踪. 而应该用git rm删除. Level 12: rm_cached A file has accidentally been added to your staging area, find out which file and remove it from the staging area. NOTE Do not remove the file from the file system, only from git. 有个文件被错误的添加到暂存区, 找到他, 并从暂存区删除, 注意不是从文件系统删除. 1234567891011[root@pipeline-cloud-test02 git_hug]# git status -sA deleteme.rb[root@pipeline-cloud-test02 git_hug]# git rm --cached deleteme.rbrm &apos;deleteme.rb&apos;[root@pipeline-cloud-test02 git_hug]# git status -s?? deleteme.rb[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! Level 13: stash You’ve made some changes and want to work on them later. You should save them, but don’t commit them. 你已经在工作空间做了一些改动, 可能出现别的任务要你改动, 你需要保存之前的改动稍后处理, 但并不是提交. 1234567891011[root@pipeline-cloud-test02 git_hug]# git status -s M lyrics.txt[root@pipeline-cloud-test02 git_hug]# git stashSaved working directory and index state WIP on master: 0206059 Add some lyricsHEAD is now at 0206059 Add some lyrics[root@pipeline-cloud-test02 git_hug]# git status -s[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! 补充: 储藏(Stashing): 经常有这样的事情发生，当你正在进行项目中某一部分的工作，里面的东西处于一个比较杂乱的状态，而你想转到其他分支上进行一些工作。问题是，你不想提交进行了一半的工作，否则以后你无法回到这个工作点。解决这个问题的办法就是git stash命令。 “储藏”可以获取你工作目录的中间状态——也就是你修改过的被追踪的文件和暂存的变更——并将它保存到一个未完结变更的堆栈中，随时可以重新应用。 使用储藏命令后, 它会帮你把工作环境回到 最后一次提交的状态, 也就是一个完全干净的工作环境. 12git stash list # 查看暂存工作区的列表git stash pop # 弹出这个现场 Level 14: rename We have a file called ‘oldfile.txt’. We want to rename it to ‘newfile.txt’ and stage this change. 将这个文件改名, 并希望在暂存区也生效. 12345678[root@pipeline-cloud-test02 git_hug]# lsoldfile.txt[root@pipeline-cloud-test02 git_hug]# git mv oldfile.txt newfile.txt[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! 如同git rm一样, git mv也会把改动的结果记录到暂存区. Level 15: restructure You added some files to your repository, but now realize that your project needs to be restructured. Make a new folder named ‘src’ and using Git move all of the .html files into this folder. 你需要重构下仓库里的文件了, 新建一个src目录, 并把html文件放进去(用Git能够追踪的方式) 1234567891011[root@pipeline-cloud-test02 git_hug]# mkdir src[root@pipeline-cloud-test02 git_hug]# git mv *.html /ssbin/ srv/ sys/[root@pipeline-cloud-test02 git_hug]# git mv *.html ./src/[root@pipeline-cloud-test02 git_hug]# ls src/about.html contact.html index.html[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! Level 16: log You will be asked for the hash of most recent commit. You will need to investigate the logs of the repository for this. 查下最近一次commit的哈希值 12345678[root@pipeline-cloud-test02 git_hug]# git log --oneline7b8d2cd THIS IS THE COMMIT YOU ARE LOOKING FOR![root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************What is the hash of the most recent commit? 7b8d2cdCongratulations, you have solved the level! 每个git commit都会留下一条日志. Level 17: tag We have a git repo and we want to tag the current commit with ‘new_tag’. 给当前提交打一个新标签 12345678[root@pipeline-cloud-test02 git_hug]# git tag new_tag[root@pipeline-cloud-test02 git_hug]# git tagnew_tag[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! 1234git tag xxx # 给最近一次提交打标签git tag xxx hashCodeXxx # 给某次提交打标签git tag # 列出所有标签git tag -d xxx # 删除某个标签 Level 18: push tags There are tags in the repository that aren’t pushed into remote repository. Push them now. 推送本地仓库的标签到远程仓库 123456[root@pipeline-cloud-test02 git_hug]# git push --tags[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! Level 19: commit_amend The ‘README’ file has been committed, but it looks like the file ‘forgotten_file.rb’ was missing from the commit. Add the file and amend your previous commit to include it. README文件已经被提交, 但是另外一个文件忘记了, 添加这个文件到上一次提交中 12345678910111213[root@pipeline-cloud-test02 git_hug]# git status -s?? forgotten_file.rb[root@pipeline-cloud-test02 git_hug]# git add forgotten_file.rb[root@pipeline-cloud-test02 git_hug]# git commit --amend -C HEAD[master c90accd] Initial commit 2 files changed, 0 insertions(+), 0 deletions(-) create mode 100644 README create mode 100644 forgotten_file.rb[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! 一般是有文件漏提交了才会用--amend. 会把你漏的东西加到上一次提交里. 123git commit --amendgit commit --amend -m &quot;xxx&quot; # 并用新的注释覆盖上一次的git commit --amend -C HEAD # 还是用上一次的注释 Level 20: commit_in_future Commit your changes with the future date (e.g. tomorrow). 提交时候改变下日期 1234567891011[root@pipeline-cloud-test02 git_hug]# dateFri Dec 22 14:07:17 CST 2017[root@pipeline-cloud-test02 git_hug]# git commit --date=&quot;Fri Dec 22 14:07:17 CST 2018&quot; -m &quot;midify commit date&quot;[master (root-commit) 83bca42] midify commit date 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 README[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! 没什么鸟用吧? Level 21: reset There are two files to be committed. The goal was to add each file as a separate commit, however both were added by accident. Unstage the file ‘to_commit_second.rb’ using the reset command (don’t commit anything). 有两个文件要被提交, 但是你想分别提交, 把to_commit_second.rb从暂存区中拿出来 123456[root@pipeline-cloud-test02 git_hug]# git reset to_commit_second.rb[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! 这一关是让你从暂存区中取回文件到工作区, 目的是不想让这些文件被提交. 回忆第12关, 也是从暂存区中取. 所以你用git rm --cached to_commit_second.rb也能完成该任务. 区别在于, git rm适合于取回新增的文件, git reset适合取回修改的文件. 当然git reset还有更强的恢复功能, 后面会遇到. Level 22: reset_soft You committed too soon. Now you want to undo the last commit, while keeping the index. 取消最后一次提交, 并保持暂存区不变 1234567891011[root@pipeline-cloud-test02 git_hug]# git log --oneline02dd7f7 Premature commit40820f0 Initial commit[root@pipeline-cloud-test02 git_hug]# git reset --soft HEAD^[root@pipeline-cloud-test02 git_hug]# git log --oneline40820f0 Initial commit[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! 在git reset命令中, --soft HEAD^就表示撤销的事最近一次提交. 并且暂存区不受影响. 回忆下19关, 是将某个遗漏的文件添加到上一次提交中去. 用的是git commit --amend -C HEAD 那它就等同于先执行git reset --soft HEAD^再执行git commit的合体. Level 23: checkout_file A file has been modified, but you don’t want to keep the modification. Checkout the ‘config.rb’ file from the last commit. 一个文件已经被修改了, 但你并不想保留这份修改. 把文件撤销到最后一次提交的状态. 123456789[root@pipeline-cloud-test02 git_hug]# git status -s M config.rb[root@pipeline-cloud-test02 git_hug]# git checkout config.rb[root@pipeline-cloud-test02 git_hug]# git status -s[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! Level 24: remote This project has a remote repository. Identify it. 找到这个项目的远程仓库名 12345678[root@pipeline-cloud-test02 git_hug]# git remotemy_remote_repo[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************What is the name of the remote repository? my_remote_repoCongratulations, you have solved the level! Level 25: remote_url The remote repositories have a url associated to them. Please enter the url of remote_location. 找到这个项目的远程仓库的URL 1234567891011[root@pipeline-cloud-test02 git_hug]# git remote -vmy_remote_repo https://github.com/Gazler/githug (fetch)my_remote_repo https://github.com/Gazler/githug (push)remote_location https://github.com/githug/not_a_repo (fetch)remote_location https://github.com/githug/not_a_repo (push)[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************What is the url of the remote repository? remote_location https://github.com/githug/not_a_repoCongratulations, you have solved the level! Level 26: pull You need to pull changes from your origin repository. 从远程仓库origin拉取更新 1234567891011[root@pipeline-cloud-test02 git_hug]# git pull origin masterremote: Counting objects: 3, done.remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 3Unpacking objects: 100% (3/3), done.From https://github.com/pull-this/thing-to-pull * branch master -&gt; FETCH_HEAD[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! Level 27: remote_add Add a remote repository called ‘origin’ with the url https://github.com/githug/githug 添加一个远程仓库, 仓库名origin, 地址https://github.com/githug/githug 12345678910111213[root@pipeline-cloud-test02 git_hug]# git remote[root@pipeline-cloud-test02 git_hug]# git remote -v[root@pipeline-cloud-test02 git_hug]# git remote add origin https://github.com/githug/githug[root@pipeline-cloud-test02 git_hug]# git remote -vorigin https://github.com/githug/githug (fetch)origin https://github.com/githug/githug (push)[root@pipeline-cloud-test02 git_hug]# git remoteorigin[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************Congratulations, you have solved the level! clone的项目git会自动保存地址, 以上这种方式适合于手工添加远程仓库 Level 28: push Your local master branch has diverged from the remote origin/master branch. Rebase your commit onto origin/master and push it to remote. 你本地的master分支是从远程仓库(origin/master)上创建的, rebase你的提交到远程仓库(origin/master). 123456789101112131415161718192021222324[root@pipeline-cloud-test02 git_hug]# git log --onelinefd962e8 Third commit98f106a Second commit3186c7d First commit[root@pipeline-cloud-test02 git_hug]# git log origin/master --oneline6c41ca7 Fourth commit[root@pipeline-cloud-test02 git_hug]# git rebaseFirst, rewinding head to replay your work on top of it...Applying: First commitApplying: Second commitApplying: Third commit[root@pipeline-cloud-test02 git_hug]# git push origin masterCounting objects: 7, done.Delta compression using up to 4 threads.Compressing objects: 100% (6/6), done.Writing objects: 100% (6/6), 591 bytes | 0 bytes/s, done.Total 6 (delta 2), reused 0 (delta 0)To /tmp/d20171222-60085-1tu401p/.git 6c41ca7..1154eda master -&gt; master[root@pipeline-cloud-test02 git_hug]# git log origin/master --oneline1154eda Third commitf2580d0 Second commit944547b First commit6c41ca7 Fourth commit 补充: 关于推送到远程: 1234# 推送到远程git push remote-name branch-namegit push -u remote-name branch-name # 推送的同时记住仓库名和分支git push # 记住后这样推送 在多人协作的项目中, 大家都需要推送自己的更改到远程仓库. 但是推送也是有时间顺序的. 如果在你推送之前有人已经推送了, 那么你会收到&quot;non-fast forward&quot;的提示. 所以需要先获取远程的最新代码到本地, 有如下两种方式: git pull: 把远程仓库的最新代码合并到本地，然后再提交。这时本地的提交和远程的提交按时间顺序混合排列。 git rebase: 把本地仓库的更新排到远程仓库更新之后，那这时候本地仓库的所有提交都排在远程仓库的最后一次提交之后。rebase翻译过来叫变基, 后面还会有关于它的应用. Level 29: diff There have been modifications to the ‘app.rb’ file since your last commit. Find out whick line has changed. 最后一次提交之后你又修改了app.rb文件, 找到哪行被修改了 123456789101112131415161718192021[root@pipeline-cloud-test02 git_hug]# git diff app.rbdiff --git a/app.rb b/app.rbindex 4f703ca..3bfa839 100644--- a/app.rb+++ b/app.rb@@ -23,7 +23,7 @@ get &apos;/yet_another&apos; do erb :success end get &apos;/another_page&apos; do- @message = get_response(&apos;data.json&apos;)+ @message = get_response(&apos;server.json&apos;) erb :another end[root@pipeline-cloud-test02 git_hug]# vim app.rb[root@pipeline-cloud-test02 git_hug]# githug********************************************************************************* Githug *********************************************************************************What is the number of the line which has changed? 26Congratulations, you have solved the level! 补充: git diff 是查看第二个flag(也就是Staging area和Working directory) 具体变化信息的命令 被修改后的文件是modified状态: 123git diff –staged # 可以查看第一个flag(也就是Staging arae和History 之间的变化), 产生相同的效果git diff HEAD # 可以看History 和 Working 之间的变化git diff –stat # 后面加stat可以简化变化信息 未完待续: 通关Githug(二))","link":"/2017/12/25/通关Githug(一)/"}],"tags":[{"name":"工作流","slug":"工作流","link":"/tags/工作流/"},{"name":"activiti","slug":"activiti","link":"/tags/activiti/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"运维","slug":"运维","link":"/tags/运维/"},{"name":"shell","slug":"shell","link":"/tags/shell/"},{"name":"工具","slug":"工具","link":"/tags/工具/"},{"name":"IDE","slug":"IDE","link":"/tags/IDE/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"镜像仓库","slug":"镜像仓库","link":"/tags/镜像仓库/"},{"name":"Elasticsearch","slug":"Elasticsearch","link":"/tags/Elasticsearch/"},{"name":"ELK","slug":"ELK","link":"/tags/ELK/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"github","slug":"github","link":"/tags/github/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"数据库","slug":"数据库","link":"/tags/数据库/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"爬虫","slug":"爬虫","link":"/tags/爬虫/"},{"name":"微服务","slug":"微服务","link":"/tags/微服务/"},{"name":"Spring Boot","slug":"Spring-Boot","link":"/tags/Spring-Boot/"},{"name":"源码分析","slug":"源码分析","link":"/tags/源码分析/"},{"name":"Spring全家桶","slug":"Spring全家桶","link":"/tags/Spring全家桶/"},{"name":"Spring Data","slug":"Spring-Data","link":"/tags/Spring-Data/"},{"name":"Spring","slug":"Spring","link":"/tags/Spring/"},{"name":"freemarker","slug":"freemarker","link":"/tags/freemarker/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"微信开发","slug":"微信开发","link":"/tags/微信开发/"},{"name":"内网穿透","slug":"内网穿透","link":"/tags/内网穿透/"},{"name":"算法","slug":"算法","link":"/tags/算法/"},{"name":"智能机器人","slug":"智能机器人","link":"/tags/智能机器人/"},{"name":"随笔","slug":"随笔","link":"/tags/随笔/"},{"name":"音乐","slug":"音乐","link":"/tags/音乐/"},{"name":"扒谱","slug":"扒谱","link":"/tags/扒谱/"},{"name":"蓝桥杯","slug":"蓝桥杯","link":"/tags/蓝桥杯/"},{"name":"设计模式","slug":"设计模式","link":"/tags/设计模式/"}],"categories":[{"name":"activiti","slug":"activiti","link":"/categories/activiti/"},{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"工具","slug":"工具","link":"/categories/工具/"},{"name":"Docker","slug":"Docker","link":"/categories/Docker/"},{"name":"Elasticsearch","slug":"Elasticsearch","link":"/categories/Elasticsearch/"},{"name":"版本管理","slug":"版本管理","link":"/categories/版本管理/"},{"name":"独立博客","slug":"独立博客","link":"/categories/独立博客/"},{"name":"问题排查","slug":"问题排查","link":"/categories/问题排查/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"微服务专题","slug":"微服务专题","link":"/categories/微服务专题/"},{"name":"Spring Boot系列","slug":"Spring-Boot系列","link":"/categories/Spring-Boot系列/"},{"name":"Spring Data系列","slug":"Spring-Data系列","link":"/categories/Spring-Data系列/"},{"name":"工作","slug":"工作","link":"/categories/工作/"},{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"算法","slug":"算法","link":"/categories/算法/"},{"name":"微信开发","slug":"微信开发","link":"/categories/微信开发/"},{"name":"随笔","slug":"随笔","link":"/categories/随笔/"},{"name":"音乐","slug":"音乐","link":"/categories/音乐/"},{"name":"设计模式","slug":"设计模式","link":"/categories/设计模式/"}]}